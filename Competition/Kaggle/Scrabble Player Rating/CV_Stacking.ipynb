{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "veQ768ckNTch",
        "d7HcAVTsNXcV"
      ],
      "authorship_tag": "ABX9TyPjTN1wpff80Fsh3lxzjFkq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namwootree/Portfolio/blob/main/Competition/Kaggle/Scrabble%20Player%20Rating/CV_Stacking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting"
      ],
      "metadata": {
        "id": "l8MIv1ehMxTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install"
      ],
      "metadata": {
        "id": "RcBljuN8MyyR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaCKpl05MKsd"
      },
      "outputs": [],
      "source": [
        "!pip install textstat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders"
      ],
      "metadata": {
        "id": "uC_MnXK5M1A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "pEbD8rqHM2H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "RUKlMRRDM4X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "MvUCQAjBM9Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "import textstat\n",
        "\n",
        "from sklearn.model_selection import cross_validate, KFold, RepeatedKFold\n",
        "import category_encoders as ce\n",
        "\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMRegressor, early_stopping, Dataset\n",
        "from catboost import CatBoostRegressor\n",
        "from catboost import Pool, cv\n",
        "\n",
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "import optuna\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "3Fm-ejlAM-3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "pcvJMKf3NFcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "kBLoZ8JdNGvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Drive Mount"
      ],
      "metadata": {
        "id": "bd7JR3iaNInH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fqcRcT2ZNH66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip File"
      ],
      "metadata": {
        "id": "jAxRT3APNMI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq '/content/drive/MyDrive/머신러닝 엔지니어링/Kaggle/Scrabble Player Rating/data/scrabble-player-rating.zip'"
      ],
      "metadata": {
        "id": "aroV_9yrNNWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Train / Test / Games / Tunrs Data"
      ],
      "metadata": {
        "id": "KdDX3S27NPln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Turns Data"
      ],
      "metadata": {
        "id": "veQ768ckNTch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_turn_features(df):\n",
        "\n",
        "    # 타일 관련 변수 전처리\n",
        "    df[\"rack_len\"] = df[\"rack\"].str.len() # rack (남은 타일)의 개수\n",
        "    df[\"rack_len_less_than_7\"] = df[\"rack_len\"].apply(lambda x : x <7) # rack의 개수가 6개 이하 인 경우\n",
        "    df[\"move_len\"] = df[\"move\"].str.len() # 배치한 타일 개수\n",
        "    df[\"move\"].fillna(\"None\",inplace=True) # 결측치 처리\n",
        "    # Dale–Chall readability formula 기준 어려운 단어\n",
        "    df[\"difficult_word\"] = df[\"move\"].apply(textstat.difficult_words)\n",
        "    \n",
        "    # 해당 턴에 한 플레이 변수 전처리\n",
        "    df[\"turn_type\"].fillna(\"None\",inplace=True) # 결측치 처리\n",
        "    turn_type_unique = df[\"turn_type\"].unique()\n",
        "    df = pd.get_dummies(df, columns=[\"turn_type\"]) # # 'turn_type' 변수 더미화\n",
        "\n",
        "    # 더미화한 변수\n",
        "    dummy_features = [f\"turn_type_{value}\" for value in turn_type_unique]\n",
        "    \n",
        "    # 타일 위치 관련 변수 전처리\n",
        "    df['y'] = df[\"location\"].str.extract('(\\d+)')[0].values # 숫자만 (Y축 정보) 추출\n",
        "    df['y'].fillna(\"0\",inplace=True) # 결측치 처리\n",
        "    df[\"y\"] = df[\"y\"].astype(int) # 데이터 타입 변경\n",
        "\n",
        "    # X축 -> 숫자\n",
        "    char_map = {\n",
        "        'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8,\n",
        "        'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15,\n",
        "                }\n",
        "    \n",
        "    df[\"x\"] = df[\"location\"].str.extract('([A-Z])')[0].values # 글자만 (X축 정보) 추출\n",
        "    df[\"x\"].replace(char_map, inplace=True) # 숫자로 변경\n",
        "    df['x'].fillna(\"0\",inplace=True) # 결측치 처리\n",
        "    df[\"x\"] = df[\"x\"].astype(int) # 데이터 타입 변경\n",
        "    \n",
        "    # 데이터의 첫 글자가 숫자인지 아닌지 여부\n",
        "    df[\"direction_of_play\"] = df[\"location\"].apply(lambda x: 1 if str(x)[0].isdigit() else 0)\n",
        "    \n",
        "    # 타일의 위치 정보를 나타내지 않은 것의 개수 (. , (time) 등등)\n",
        "    df[\"curr_board_pieces_used\"] = df[\"move\"].apply(lambda x: str(x).count(\".\") + sum(int(c.islower()) for c in str(x)))\n",
        "    \n",
        "    # 통계값을 활용한 전처리\n",
        "\n",
        "    # 평균값을 구할 변수\n",
        "    avg_features = [\"points\", \"move_len\", \"difficult_word\", \"curr_board_pieces_used\", \"direction_of_play\"]\n",
        "    # 합계를 구할 변수\n",
        "    sum_features = [\"difficult_word\", \"rack_len_less_than_7\"]+dummy_features\n",
        "    # 최대값을 구할 변수\n",
        "    max_features = [\"move_len\", \"points\"]\n",
        "    agg_func = {feature:'sum' for feature in dummy_features}\n",
        "    agg_func.update({\n",
        "        \"points\":[\"mean\", \"max\"],\n",
        "        \"move_len\":[\"mean\", \"max\"],\n",
        "        \"difficult_word\":[\"mean\", \"sum\"],\n",
        "        \"curr_board_pieces_used\": \"mean\",\n",
        "        \"direction_of_play\": \"mean\",\n",
        "        \"rack_len_less_than_7\" : \"sum\"\n",
        "    }\n",
        "    )\n",
        "\n",
        "    turns_grouped = df.groupby([\"game_id\", \"nickname\"], as_index=False).agg(agg_func)\n",
        "    \n",
        "    turns_grouped.columns = [\"_\".join(a) if a[0] not in [\"game_id\", \"nickname\"] else a[0] for a in turns_grouped.columns.to_flat_index()]\n",
        "    \n",
        "    print('DONE : create_turn_features')\n",
        "\n",
        "    return turns_grouped"
      ],
      "metadata": {
        "id": "G7XTHpprNSyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Train / Test Data"
      ],
      "metadata": {
        "id": "d7HcAVTsNXcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(bot_names =[\"BetterBot\", \"STEEBot\", \"HastyBot\", \"MasterBot\"], cat_features=[]):\n",
        "    \n",
        "    # 파일 경로\n",
        "    ROOT_DIR = '/content/'\n",
        "    \n",
        "    # 데이터 불러오기\n",
        "    train = pd.read_csv(os.path.join(ROOT_DIR, \"train.csv\"))\n",
        "    test = pd.read_csv(os.path.join(ROOT_DIR, \"test.csv\"))\n",
        "    turns = pd.read_csv(os.path.join(ROOT_DIR, \"turns.csv\"))\n",
        "    games = pd.read_csv(os.path.join(ROOT_DIR, \"games.csv\"))\n",
        "    \n",
        "    # Train & Test 데이터 셋 결합\n",
        "    df = pd.concat([train, test])\n",
        "    \n",
        "    # 전처리\n",
        "    \n",
        "    # turns 데이터 전처리\n",
        "    turns_fe_df = create_turn_features(turns)\n",
        "\n",
        "    # 전처리된 turns 데이터 결합\n",
        "    df = df.merge(turns_fe_df, how=\"left\", on=[\"game_id\", \"nickname\"])\n",
        "    \n",
        "    # Bot 관련된 데이터 플레임 생성\n",
        "    bot_df = df[[\"game_id\", \"nickname\", \"score\", \"rating\"]].copy()\n",
        "    bot_df['bot_name'] = bot_df['nickname'].apply(lambda x: x if x in bot_names else np.nan)\n",
        "    bot_df = bot_df[[\"game_id\", \"score\", \"rating\", \"bot_name\"]].dropna(subset=[\"bot_name\"])\n",
        "    bot_df.columns = [\"game_id\", \"bot_score\", \"bot_rating\", \"bot_name\"]\n",
        "    \n",
        "    # Human 관련된 데이터 플레임 생성 및 Bot 관련된 데이터 & games 데이터 결합\n",
        "    df = df[~df['nickname'].isin(bot_names)] # Human 관련된 데이터 플레임 생성\n",
        "    df = df.merge(bot_df, on=\"game_id\") # Bot 관련된 데이터 결합\n",
        "    df = df.merge(games, on=\"game_id\") # games 관련된 데이터 결합\n",
        "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"]) # 데이터 타임 수정\n",
        "    \n",
        "    # 랭킹 1500등 계정 전처리\n",
        "    users= df[df[\"rating\"]==1500][\"nickname\"]\n",
        "    anamolous = df[df[\"nickname\"].isin(users)].groupby(\"nickname\").agg({'nickname':'count',\n",
        "                                                         'rating' : lambda x : np.sum(x == 1500)\n",
        "                                                         })\n",
        "    \n",
        "    # 이상치 제거 : 너무 잘하는 사람\n",
        "\n",
        "    # 랭킹 1500등에 달성할 확률 \n",
        "    anamolous[\"ratio\"] = anamolous[\"rating\"] / anamolous[\"nickname\"]\n",
        "\n",
        "    # 너무 잘하는 유저\n",
        "    anamolous_users = anamolous[(anamolous[\"ratio\"] >=1.0) & (anamolous[\"nickname\"]>1)].index\n",
        "\n",
        "    # 이상치 제거\n",
        "    df = df[~df[\"nickname\"].isin(anamolous_users)]\n",
        "    \n",
        "    # 범주형 데이터 지정\n",
        "    for name in cat_features:\n",
        "        df[name] = df[name].astype(\"category\")\n",
        "\n",
        "        # 카테고리에 'None'값 추가\n",
        "        # if \"None\" not in df[name].cat.categories:\n",
        "        #     df[name].cat.add_categories(\"None\", inplace=True)\n",
        "\n",
        "    # Split Train / Test\n",
        "    train = df[df[\"game_id\"].isin(train[\"game_id\"])].set_index(\"game_id\")\n",
        "    test = df[df[\"game_id\"].isin(test[\"game_id\"])].set_index(\"game_id\")\n",
        "\n",
        "    print('\\nDONE : load_data\\n')\n",
        "\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "Huj4bvljNcRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features = [\"nickname\",\"bot_name\", \n",
        "                \"time_control_name\", \"first\",\n",
        "                \"game_end_reason\", \"winner\",\n",
        "                \"lexicon\", \"rating_mode\"]"
      ],
      "metadata": {
        "id": "K52-fF9FNiMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = load_data(cat_features = cat_features)"
      ],
      "metadata": {
        "id": "zz7dhQLINjgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "lsM5YXfUNlIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "id": "EMDiE6qINoR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "Gh1nvvxpNo9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create_cumm_player_features_overall"
      ],
      "metadata": {
        "id": "fWB_KWRNNq05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cumm_player_features_overall(df):\n",
        "\n",
        "    df = df[[\"nickname\", \"created_at\",\"score\",\"winner\", \"game_duration_seconds\"]]\n",
        "    \n",
        "    df= df.sort_values(by=\"created_at\")\n",
        "\n",
        "    #Initialize our new variables with 0's\n",
        "    df[\"cumm_avg_player_score\"] = np.zeros(len(df))\n",
        "    df[\"cumm_player_wins\"] = np.zeros(len(df))\n",
        "    df[\"cumm_avg_player_win_ratio\"] = np.zeros(len(df))\n",
        "    df[\"cumm_avg_game_duration_seconds\"] = np.zeros(len(df))\n",
        "\n",
        "    # 닉네임 별\n",
        "    for nickname in tqdm(df[\"nickname\"].unique()):\n",
        "\n",
        "        # 게임이 플레이 되기 전에는 현재의 데이터를 알 수 없기에 이전 데이터를 활용함\n",
        "        \n",
        "        # 플레이어 평균 점수\n",
        "        df.loc[df[\"nickname\"]==nickname, \"cumm_avg_player_score\"]= np.append(0, df[df[\"nickname\"]==nickname][\"score\"].expanding(min_periods=1).mean().values[:-1])\n",
        "        \n",
        "        # 플레이어 우승 횟수 (우승 : 1, 무승부 : 0, 패배 : -1)\n",
        "        df.loc[df[\"nickname\"]==nickname, \"cumm_player_wins\"]= np.append(0, df[df[\"nickname\"]==nickname][\"winner\"].expanding(min_periods=1).sum().values[:-1])\n",
        "        \n",
        "        # 플레이어 우승 비율 (우승 횟수 / 경기 수)\n",
        "        df.loc[df[\"nickname\"]==nickname, \"cumm_avg_player_win_ratio\"]= \\\n",
        "        df[df[\"nickname\"]==nickname][\"cumm_player_wins\"] / np.append(0, df[df[\"nickname\"]==nickname][\"winner\"].expanding(min_periods=1).count().values[:-1])\n",
        "        \n",
        "        # 플레이어 평균 경기 시간\n",
        "        df.loc[df[\"nickname\"]==nickname, \"cumm_avg_game_duration_seconds\"]= \\\n",
        "        np.append(0, df[df[\"nickname\"]==nickname][\"game_duration_seconds\"].expanding(min_periods=2).mean().values[:-1])\n",
        "        \n",
        "    # 결측치 처리\n",
        "    df[[\"cumm_avg_player_score\", \"cumm_player_wins\", \"cumm_avg_player_win_ratio\", \"cumm_avg_game_duration_seconds\"]]\\\n",
        "    = df[[\"cumm_avg_player_score\", \"cumm_player_wins\", \"cumm_avg_player_win_ratio\", \"cumm_avg_game_duration_seconds\"]].fillna(0)\n",
        "    \n",
        "    df = df.sort_index()\n",
        "    \n",
        "    print('DONE : create_cumm_player_features_overall')\n",
        "\n",
        "    return df[[\"cumm_avg_player_score\", \"cumm_player_wins\", \"cumm_avg_player_win_ratio\", \"cumm_avg_game_duration_seconds\"]]"
      ],
      "metadata": {
        "id": "CP60zMiZNp-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create_cumm_player_features_bot"
      ],
      "metadata": {
        "id": "4fhkuinBNuLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cumm_player_features_bot(df):\n",
        "\n",
        "    df= df[[\"nickname\", \"created_at\",\"score\",\"winner\",\"bot_name\", \"game_duration_seconds\"]]\n",
        "    \n",
        "    df= df.sort_values(by=\"created_at\")\n",
        "\n",
        "    for bot_name in df[\"bot_name\"].unique():\n",
        "        df[\"cumm_avg_player_score_\"+str(bot_name)] = np.zeros(len(df))\n",
        "        df[\"cumm_player_wins_\"+str(bot_name)] = np.zeros(len(df))\n",
        "        df[\"cumm_avg_player_win_ratio_\"+str(bot_name)] = np.zeros(len(df))\n",
        "        df[\"cumm_avg_game_duration_seconds_\"+str(bot_name)] = np.zeros(len(df))\n",
        "\n",
        "    for nickname in tqdm(df[\"nickname\"].unique()):\n",
        "        \n",
        "        # 상대 Bot 별\n",
        "        for bot_name in df[\"bot_name\"].unique():\n",
        "            \n",
        "            # 게임이 플레이 되기 전에 현재 데이터 값을 알 수 없기에 이전 데이터를 활용함\n",
        "\n",
        "            # 플레이어 평균 점수\n",
        "            df.loc[(df[\"nickname\"]==nickname) & (df[\"bot_name\"]==bot_name), \"cumm_avg_player_score_\"+str(bot_name)]= \\\n",
        "            np.append(0, df[(df[\"nickname\"]==nickname) & (df[\"bot_name\"]==bot_name)][\"score\"].expanding(min_periods=1).mean().values[:-1])\n",
        "            \n",
        "            # 플레이어 우승 횟수 (우승 : 1, 무승부 : 0, 패배 : -1)\n",
        "            df.loc[(df[\"nickname\"]==nickname) & (df[\"bot_name\"]==bot_name), \"cumm_player_wins_\"+str(bot_name)]= \\\n",
        "            np.append(0, df[(df[\"nickname\"]==nickname) & (df[\"bot_name\"]==bot_name)][\"winner\"].expanding(min_periods=1).sum().values[:-1])\n",
        "            \n",
        "            # 플레이어 평균 우승 비율\n",
        "            df.loc[(df[\"nickname\"]==nickname) & (df[\"bot_name\"]==bot_name), \"cumm_avg_player_win_ratio_\"+str(bot_name)]= \\\n",
        "            df[(df[\"nickname\"]==nickname) & (df[\"bot_name\"]==bot_name)][\"cumm_avg_player_win_ratio_\"+str(bot_name)] / np.append(0, df[(df[\"nickname\"]==nickname) & (df[\"bot_name\"]==bot_name)][\"winner\"].expanding(min_periods=1).count().values[:-1])\n",
        "            \n",
        "            # 플레이어 평균 경기 시간\n",
        "            df.loc[(df[\"nickname\"]==nickname) & (df[\"bot_name\"]==bot_name), \"cumm_avg_game_duration_seconds_\"+str(bot_name)]= \\\n",
        "            np.append(0, df[(df[\"nickname\"]==nickname) & (df[\"bot_name\"]==bot_name)][\"game_duration_seconds\"].expanding(min_periods=1).mean().values[:-1])\n",
        "            \n",
        "    #fill in any missing values with 0\n",
        "    for bot_name in df[\"bot_name\"].unique():\n",
        "        df[[\"cumm_avg_player_score_\"+str(bot_name), \"cumm_player_wins_\"+str(bot_name), \"cumm_avg_player_win_ratio_\"+str(bot_name), \"cumm_avg_game_duration_seconds_\"+str(bot_name)]] = \\\n",
        "        df[[\"cumm_avg_player_score_\"+str(bot_name), \"cumm_player_wins_\"+str(bot_name), \"cumm_avg_player_win_ratio_\"+str(bot_name), \"cumm_avg_game_duration_seconds_\"+str(bot_name)]].fillna(0)\n",
        "    \n",
        "    # resort the data by the the index (i.e. game number)\n",
        "    df = df.sort_index()\n",
        "\n",
        "    print('DONE : create_cumm_player_features_bot')\n",
        "    \n",
        "    # 아래의 변수가 없는 변수들만 사용\n",
        "    return df[df.columns.difference([\"nickname\", \"created_at\",\"score\",\"winner\",\"bot_name\", \"game_duration_seconds\"])]"
      ],
      "metadata": {
        "id": "U9g5GZ84NvlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create_cumm_player_features_lexicon"
      ],
      "metadata": {
        "id": "ymABQa0WNxfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cumm_player_features_lexicon(df):\n",
        "\n",
        "    df= df[[\"nickname\", \"created_at\",\"score\",\"winner\",\"lexicon\",  \"game_duration_seconds\"]]\n",
        "\n",
        "    df= df.sort_values(by=\"created_at\")\n",
        "\n",
        "    for lexicon in df[\"lexicon\"].unique():\n",
        "        df[\"cumm_avg_player_score_\"+str(lexicon)] = np.zeros(len(df))\n",
        "        df[\"cumm_player_wins_\"+str(lexicon)] = np.zeros(len(df))\n",
        "        df[\"cumm_avg_player_win_ratio_\"+str(lexicon)] = np.zeros(len(df))\n",
        "        df[\"cumm_avg_game_duration_seconds_\"+str(lexicon)] = np.zeros(len(df))\n",
        "\n",
        "    for nickname in tqdm(df[\"nickname\"].unique()):\n",
        "\n",
        "        # 사용한 어휘 사전 별\n",
        "        for lexicon in df[\"lexicon\"].unique():\n",
        "\n",
        "            # 게임이 플레이 되기 전에 현재 데이터 값을 알 수 없기에 이전 데이터를 활용함\n",
        "            \n",
        "            # 플레이어 평균 점수\n",
        "            df.loc[(df[\"nickname\"]==nickname) & (df[\"lexicon\"]==lexicon), \"cumm_avg_player_score_\"+str(lexicon)]= \\\n",
        "            np.append(0, df[(df[\"nickname\"]==nickname) & (df[\"lexicon\"]==lexicon)][\"score\"].expanding(min_periods=1).mean().values[:-1])\n",
        "            \n",
        "            # 플레이어 우승 횟수 (우승 : 1, 무승부 : 0, 패배 : -1)\n",
        "            df.loc[(df[\"nickname\"]==nickname) & (df[\"lexicon\"]==lexicon), \"cumm_player_wins_\"+str(lexicon)]= \\\n",
        "            np.append(0, df[(df[\"nickname\"]==nickname) & (df[\"lexicon\"]==lexicon)][\"winner\"].expanding(min_periods=1).sum().values[:-1])\n",
        "            \n",
        "            # 플레이어 평균 우승 비율\n",
        "            df.loc[(df[\"nickname\"]==nickname) & (df[\"lexicon\"]==lexicon), \"cumm_avg_player_win_ratio_\"+str(lexicon)]= \\\n",
        "            df[(df[\"nickname\"]==nickname) & (df[\"lexicon\"]==lexicon)][\"cumm_avg_player_win_ratio_\"+str(lexicon)] / np.append(0, df[(df[\"nickname\"]==nickname) & (df[\"lexicon\"]==lexicon)][\"winner\"].expanding(min_periods=1).count().values[:-1])\n",
        "            \n",
        "            # 플레이어 평균 경기 시간\n",
        "            df.loc[(df[\"nickname\"]==nickname) & (df[\"lexicon\"]==lexicon), \"cumm_avg_game_duration_seconds_\"+str(lexicon)]= \\\n",
        "            np.append(0, df[(df[\"nickname\"]==nickname) & (df[\"lexicon\"]==lexicon)][\"game_duration_seconds\"].expanding(min_periods=1).mean().values[:-1])\n",
        "\n",
        "    # 결측치 처리\n",
        "    for lexicon in df[\"lexicon\"].unique():\n",
        "        df[[\"cumm_avg_player_score_\"+str(lexicon), \"cumm_player_wins_\"+str(lexicon), \"cumm_avg_player_win_ratio_\"+str(lexicon), \"cumm_avg_game_duration_seconds_\"+str(lexicon)]] = \\\n",
        "        df[[\"cumm_avg_player_score_\"+str(lexicon), \"cumm_player_wins_\"+str(lexicon), \"cumm_avg_player_win_ratio_\"+str(lexicon), \"cumm_avg_game_duration_seconds_\"+str(lexicon)]].fillna(0)\n",
        "    \n",
        "    df = df.sort_index()\n",
        "\n",
        "    print('DONE : create_cumm_player_features_lexicon')\n",
        "\n",
        "    # 아래의 변수가 없는 변수들만 사용\n",
        "    return df[df.columns.difference([\"nickname\", \"created_at\",\"score\",\"winner\",\"lexicon\", \"game_duration_seconds\"])]"
      ],
      "metadata": {
        "id": "YS-v7vw1Nyhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create_cumm_player_game_features"
      ],
      "metadata": {
        "id": "7dYPDUuRN0zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cumm_player_game_features(df):\n",
        "    \n",
        "    df = df[[\"nickname\", \"created_at\", \"bot_name\", \"rating_mode\", \"lexicon\", \"game_end_reason\"]]\n",
        "    \n",
        "    # One-Hot Encoder\n",
        "    encoder = ce.OneHotEncoder(cols=[\"bot_name\", \"rating_mode\", \"lexicon\", \"game_end_reason\"], use_cat_names=True)\n",
        "    df = df.join(encoder.fit_transform(df[[\"bot_name\", \"rating_mode\", \"lexicon\", \"game_end_reason\"]]))\n",
        "    \n",
        "    df= df.sort_values(by=\"created_at\")\n",
        "    \n",
        "    # One-Hot Encoder한 변수 생성 (갯수 합계로 채울 예정)\n",
        "    for feature_name in encoder.get_feature_names():\n",
        "        df[\"cumm_\"+str(feature_name)+\"_counts\"] = np.zeros(len(df))\n",
        "\n",
        "    # 닉네임 별\n",
        "    for nickname in tqdm(df[\"nickname\"].unique()):\n",
        "\n",
        "        # 원-핫 인코딩한 변수 별\n",
        "        for feature_name in encoder.get_feature_names():\n",
        "\n",
        "            # 게임이 플레이 되기 전에 현재 데이터 값을 알 수 없기에 이전 데이터를 활용함\n",
        "\n",
        "            # 원-핫 인코딩한 변수별 합계 구함\n",
        "            df.loc[df[\"nickname\"]==nickname, \"cumm_\"+str(feature_name)+\"_counts\"]= \\\n",
        "            np.append(0, df[df[\"nickname\"]==nickname][feature_name].expanding(min_periods=1).sum().values[:-1])\n",
        "\n",
        "    # 결측치 제거\n",
        "    for feature_name in encoder.get_feature_names():\n",
        "        df[\"cumm_\"+str(feature_name)+\"_counts\"] = df[\"cumm_\"+str(feature_name)+\"_counts\"].fillna(0)\n",
        "        \n",
        "    df = df.sort_index()\n",
        "    \n",
        "    print('DONE : create_cumm_player_game_features')\n",
        "\n",
        "    return df[df.columns.difference([\"nickname\", \"created_at\", \"bot_name\", \"rating_mode\", \"lexicon\", \"game_end_reason\"]+encoder.get_feature_names())]"
      ],
      "metadata": {
        "id": "gQe2VytON1LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create_cumm_bot_features"
      ],
      "metadata": {
        "id": "wo550wJ2N4if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cumm_bot_features(df):\n",
        "\n",
        "    df= df[[\"nickname\", \"created_at\",\"bot_name\", \"bot_score\", \"bot_rating\"]]\n",
        "\n",
        "    df= df.sort_values(by=\"created_at\")\n",
        "\n",
        "    for bot_name in df[\"bot_name\"].unique():\n",
        "        df[\"cumm_avg_bot_score_\"+str(bot_name)] = np.zeros(len(df))\n",
        "        df[\"cumm_avg_bot_rating_\"+str(bot_name)] = np.zeros(len(df))\n",
        "\n",
        "\n",
        "    for nickname in tqdm(df[\"nickname\"].unique()):\n",
        "        for bot_name in df[\"bot_name\"].unique():\n",
        "\n",
        "            # 게임이 플레이 되기 전에 현재 데이터 값을 알 수 없기에 이전 데이터를 활용함\n",
        "            # 그러나 'bot rating'의 경우 게임 플레이하기 전에도 알 수 있기에 적용에서 제외됨\n",
        "\n",
        "            # Bot의 평균 점수\n",
        "            df.loc[(df[\"nickname\"]==nickname) & (df[\"bot_name\"]==bot_name), \"cumm_avg_bot_score_\"+str(bot_name)]= \\\n",
        "            np.append(0, df[(df[\"nickname\"]==nickname) & (df[\"bot_name\"]==bot_name)][\"bot_score\"].expanding(min_periods=1).mean().values[:-1])\n",
        "            \n",
        "            # Bot의 평균 랭킹\n",
        "            df.loc[(df[\"nickname\"]==nickname) & (df[\"bot_name\"]==bot_name), \"cumm_avg_bot_rating_\"+str(bot_name)]= \\\n",
        "            df[(df[\"nickname\"]==nickname) & (df[\"bot_name\"]==bot_name)][\"bot_rating\"].expanding(min_periods=1).mean().values\n",
        "\n",
        "    # 결측치 제거       \n",
        "    for bot_name in df[\"bot_name\"].unique():\n",
        "        df[[\"cumm_avg_bot_score_\"+str(bot_name), \"cumm_avg_bot_rating_\"+str(bot_name)]] = \\\n",
        "        df[[\"cumm_avg_bot_score_\"+str(bot_name), \"cumm_avg_bot_rating_\"+str(bot_name)]].fillna(0)\n",
        "    \n",
        "    df = df.sort_index()\n",
        "    \n",
        "    print('DONE : create_cumm_bot_features')\n",
        "    \n",
        "    return df[df.columns.difference([\"nickname\", \"created_at\",\"bot_name\", \"bot_score\", \"bot_rating\"])]"
      ],
      "metadata": {
        "id": "OCG8GHyfN44i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create_cumm_turns_features"
      ],
      "metadata": {
        "id": "opYIk8jBN8gX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cumm_turns_features(df):\n",
        "\n",
        "    turn_features = ['turn_type_Play_sum',\n",
        "       'turn_type_End_sum', 'turn_type_Exchange_sum', 'turn_type_Pass_sum',\n",
        "       'turn_type_Timeout_sum', 'turn_type_Challenge_sum',\n",
        "       'turn_type_Six-Zero Rule_sum', 'turn_type_None_sum', 'points_mean',\n",
        "       'points_max', 'move_len_mean', 'move_len_max', 'difficult_word_mean',\n",
        "       'difficult_word_sum', 'curr_board_pieces_used_mean',\n",
        "       'direction_of_play_mean', 'rack_len_less_than_7_sum']\n",
        "    \n",
        "    df = df[[\"nickname\", \"created_at\"]+turn_features]\n",
        "    \n",
        "    df= df.sort_values(by=\"created_at\")\n",
        "    \n",
        "    for nickname in tqdm(df[\"nickname\"].unique()):\n",
        "\n",
        "        # turns 관련 변수 별\n",
        "        for feature_name in turn_features:\n",
        "\n",
        "            # 게임이 플레이 되기 전에 현재 데이터 값을 알 수 없기에 이전 데이터를 활용함\n",
        "\n",
        "            # 유저별 urns 관련 변수들의 평균값\n",
        "            df.loc[df[\"nickname\"]==nickname, \"cumm_\"+str(feature_name)+\"_average\"]= \\\n",
        "            np.append(0, df[df[\"nickname\"]==nickname][feature_name].expanding(min_periods=1).mean().values[:-1])\n",
        "\n",
        "    # 결측치 제거\n",
        "    for feature_name in turn_features:\n",
        "        df[\"cumm_\"+str(feature_name)+\"_average\"] = df[\"cumm_\"+str(feature_name)+\"_average\"].fillna(0)\n",
        "    \n",
        "    df = df.sort_index()\n",
        "\n",
        "    print('DONE : create_cumm_turns_features')\n",
        "\n",
        "    return df[df.columns.difference([\"nickname\", \"created_at\"]+turn_features)]"
      ],
      "metadata": {
        "id": "YKLglVwYN8NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finalize Features"
      ],
      "metadata": {
        "id": "T0lr_gSVN_Kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 전처리 기법 사용\n",
        "def create_features(df, df_test=None):\n",
        "    X_raw = df.copy()\n",
        "    y = df['rating'].copy()\n",
        "    \n",
        "    # test 데이터 존재하면\n",
        "    if df_test is not None:\n",
        "        X_test = df_test.copy()\n",
        "\n",
        "        # train 데이터 & test 데이터 결합\n",
        "        X_raw = pd.concat([X_raw, X_test])\n",
        "        \n",
        "    # 모든 전처리 기법 적용\n",
        "    X = create_cumm_player_features_lexicon(X_raw)\n",
        "    X = X.join(create_cumm_player_game_features(X_raw))\n",
        "    X = X.join(create_cumm_player_features_overall(X_raw))\n",
        "    X = X.join(create_cumm_bot_features(X_raw))\n",
        "    X = X.join(create_cumm_turns_features(X_raw))\n",
        "\n",
        "    \n",
        "    # Reform splits (데이터 무작위로 섞임)\n",
        "\n",
        "    # test 데이터 존재하면\n",
        "    if df_test is not None:\n",
        "\n",
        "        # test 데이터 구별\n",
        "        X_test = X.loc[df_test.index, :]\n",
        "\n",
        "        # train 데이터 구별\n",
        "        X.drop(df_test.index, inplace=True)\n",
        "    \n",
        "    if df_test is not None:\n",
        "        return X, X_test\n",
        "\n",
        "    else:\n",
        "        return X"
      ],
      "metadata": {
        "id": "utu6YQl-OAzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, X_test = create_features(train, test)\n",
        "y = train['rating'].copy()"
      ],
      "metadata": {
        "id": "e2sqLyBlODA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "PsmtuCApOGpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_stacking_data = '/content/drive/MyDrive/머신러닝 엔지니어링/Kaggle/Scrabble Player Rating/for_stacking_data/'"
      ],
      "metadata": {
        "id": "d4if2K-_Qucd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LGBM"
      ],
      "metadata": {
        "id": "m5GEjDtCOHt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optuna"
      ],
      "metadata": {
        "id": "aJLssg4FOKS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial, X, y):\n",
        "    \n",
        "    # 적용(수색)할 최적의 하이퍼 파라미터 범위 지정\n",
        "    param = {\n",
        "        \"objective\": \"regression\",\n",
        "        \"verbosity\": -1,\n",
        "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-10, 10.0),\n",
        "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-10, 10.0),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 2, 1024),\n",
        "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n",
        "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 0, 60),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 500),\n",
        "    }\n",
        "    \n",
        "    # LGBM reg 모델 사용 / 교차 검증 사용\n",
        "    lgbcv = lgb.cv(param,\n",
        "                   lgb.Dataset(X, label=y),\n",
        "                   folds= KFold(n_splits=5, shuffle=True),\n",
        "                   verbose_eval=False,                   \n",
        "                   early_stopping_rounds=200,                   \n",
        "                   num_boost_round=50000\n",
        "                  )\n",
        "    \n",
        "    cv_score = lgbcv['l2-mean'][-1] # MSE\n",
        "    \n",
        "    # Return metric of interest\n",
        "    return cv_score"
      ],
      "metadata": {
        "id": "jIrZ3iTfOjAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(lambda trial: objective(trial, X, y),\n",
        "               timeout=1800,\n",
        "               n_trials=150,\n",
        "               n_jobs=1,\n",
        "               show_progress_bar=True) "
      ],
      "metadata": {
        "id": "rG-sy481On67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(study.best_params)"
      ],
      "metadata": {
        "id": "U9_sq8v3OqGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(study.best_value**0.5)"
      ],
      "metadata": {
        "id": "2XOUpgc6OrV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CV Stacking"
      ],
      "metadata": {
        "id": "xfbIwKctOMKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stacking_base_datasets(X, y, df_test, n_splits, n_repeats):\n",
        "\n",
        "  num_model = 0\n",
        "\n",
        "  train_fold_pred = np.zeros((X.shape[0] ,1 ))\n",
        "  test_pred = np.zeros((df_test.shape[0], n_splits))\n",
        "\n",
        "  skf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
        "\n",
        "  num_fold = 0\n",
        "\n",
        "  for fold_idx, (train_index, valid_index) in enumerate(tqdm(skf.split(X, y))):\n",
        "      X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "      y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "      lgb_train = lgb.Dataset(X_train, y_train)\n",
        "      lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
        "      lgb_params = {\n",
        "          'objective': 'regression',\n",
        "          'verbose': 5000,\n",
        "          'n_estimators': 50000,\n",
        "          **study.best_params\n",
        "      }\n",
        "\n",
        "      print(f'\\nNumber fold : {num_fold}\\n')\n",
        "      model = lgb.train(lgb_params,\n",
        "                        lgb_train,\n",
        "                        valid_sets=lgb_eval,\n",
        "                        verbose_eval=False,\n",
        "                        callbacks=[lgb.early_stopping(100)])\n",
        "      \n",
        "      y_pred = model.predict(X_valid)\n",
        "\n",
        "      MSE = mean_squared_error(y_pred, y_valid)\n",
        "      print(f'\\n{num_fold} 번 Model Vaild MSE : {MSE}\\n')\n",
        "\n",
        "      train_fold_pred[valid_index, :] = y_pred.reshape(-1,1)\n",
        "      test_pred[:, num_model] = model.predict(df_test)\n",
        "\n",
        "      num_model += 1\n",
        "\n",
        "  test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1) \n",
        "\n",
        "  return train_fold_pred , test_pred_mean"
      ],
      "metadata": {
        "id": "lH_1EsP6OvW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacking_train, stacking_test = get_stacking_base_datasets(X, y, X_test, 5, 3)"
      ],
      "metadata": {
        "id": "NBZtLXR4OxdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LGBM_Stacking_train = pd.DataFrame(stacking_train)\n",
        "LGBM_Stacking_train['target'] = y\n",
        "\n",
        "LGBM_Stacking_test = pd.DataFrame(stacking_test)"
      ],
      "metadata": {
        "id": "DCUN9BSJO6SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LGBM_Stacking_train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "PB2YuPXFPAYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LGBM_Stacking_train.to_csv(path_stacking_data+'LGBM_Stacking_train.csv')\n",
        "LGBM_Stacking_test.to_csv(path_stacking_data+'LGBM_Stacking_test.csv')"
      ],
      "metadata": {
        "id": "YQ5bw_pWQWT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CatBoost"
      ],
      "metadata": {
        "id": "ZWlkCY_kOIm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optuna"
      ],
      "metadata": {
        "id": "SInqpe80OU2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial, X, y):\n",
        "\n",
        "    cv_dataset = Pool(data=X,\n",
        "                   label=y)\n",
        "    \n",
        "    # 적용(수색)할 최적의 하이퍼 파라미터 범위 지정\n",
        "    param = {\n",
        "    'loss_function':'RMSE',\n",
        "    'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
        "    \"max_depth\":trial.suggest_int(\"max_depth\", 4, 16),\n",
        "    'random_strength' :trial.suggest_int('random_strength', 0, 100),\n",
        "    \"colsample_bylevel\":trial.suggest_float(\"colsample_bylevel\", 0.4, 1.0),\n",
        "    \"l2_leaf_reg\":trial.suggest_float(\"l2_leaf_reg\",1e-8,3e-5),\n",
        "    \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 500),\n",
        "    \"max_bin\": trial.suggest_int(\"max_bin\", 100, 500)\n",
        "    }\n",
        "\n",
        "    scores = cv(cv_dataset,\n",
        "                param,\n",
        "                folds= KFold(n_splits=5, shuffle=True),\n",
        "                verbose_eval=False,                   \n",
        "                early_stopping_rounds=200, \n",
        "                num_boost_round=50000)\n",
        "    \n",
        "    cv_score = list(scores['test-RMSE-mean'])[-1]\n",
        "\n",
        "    # Return metric of interest\n",
        "    return cv_score"
      ],
      "metadata": {
        "id": "SahTf1YTPKTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(lambda trial: objective(trial, X, y),\n",
        "               timeout=1800,\n",
        "               n_trials=100,\n",
        "               n_jobs=1,\n",
        "               show_progress_bar=True) "
      ],
      "metadata": {
        "id": "wuL3LNk-PL-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(study.best_params)"
      ],
      "metadata": {
        "id": "T5qyLUAbPOBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(study.best_value**0.5)"
      ],
      "metadata": {
        "id": "_KY2ldkGPO_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CV Stacking"
      ],
      "metadata": {
        "id": "7y6uB-O9OeW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stacking_base_datasets(X, y, df_test, n_splits, n_repeats):\n",
        "\n",
        "  num_model = 0\n",
        "\n",
        "  train_fold_pred = np.zeros((X.shape[0] ,1 ))\n",
        "  test_pred = np.zeros((df_test.shape[0], n_splits))\n",
        "\n",
        "  skf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
        "\n",
        "  num_fold = 0\n",
        "\n",
        "  for fold_idx, (train_index, valid_index) in enumerate(tqdm(skf.split(X, y))):\n",
        "      X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "      y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "      cat_params = {\n",
        "          'objective': 'regression',\n",
        "          'verbose': 100,\n",
        "          'n_estimators': 50000,\n",
        "          **study.best_params\n",
        "      }\n",
        "\n",
        "      print(f'\\nNumber fold : {num_fold}\\n')\n",
        "\n",
        "      model = CatBoostRegressor(**cat_params)\n",
        "\n",
        "      model.fit(X_train, y_train,\n",
        "                eval_set=(X_valid, y_valid),\n",
        "                early_stopping_rounds=100,\n",
        "                use_best_model=True,\n",
        "                verbose=100)\n",
        "      \n",
        "      y_pred = model.predict(X_valid)\n",
        "\n",
        "      MSE = mean_squared_error(y_pred, y_valid)\n",
        "      print(f'\\n{num_fold} 번 Model Vaild MSE : {MSE}\\n')\n",
        "\n",
        "      train_fold_pred[valid_index, :] = y_pred.reshape(-1,1)\n",
        "      test_pred[:, num_model] = model.predict(df_test)\n",
        "\n",
        "      num_model += 1\n",
        "\n",
        "  test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1) \n",
        "\n",
        "  return train_fold_pred , test_pred_mean"
      ],
      "metadata": {
        "id": "APswaUL5OHY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacking_train, stacking_test = get_stacking_base_datasets(X, y, X_test, 5, 3)"
      ],
      "metadata": {
        "id": "6i_lwcU8Ppae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CAT_Stacking_train = pd.DataFrame(stacking_train)\n",
        "CAT_Stacking_train['target'] = y\n",
        "\n",
        "CAT_Stacking_test = pd.DataFrame(stacking_test)"
      ],
      "metadata": {
        "id": "kvrKek5CP54G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CAT_Stacking_train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "C-DD8l5KP-9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CAT_Stacking_train.to_csv(path_stacking_data+'CAT_Stacking_train.csv')\n",
        "CAT_Stacking_test.to_csv(path_stacking_data+'CAT_Stacking_test.csv')"
      ],
      "metadata": {
        "id": "uxLcIStJQ3oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "e6CrcvTJcRXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optuna"
      ],
      "metadata": {
        "id": "w69fwwXwcVxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import cv\n",
        "def objective(trial, X, y):\n",
        "\n",
        "    dtrain_matrix = xgb.DMatrix(X, label=y)\n",
        "    \n",
        "    # 적용(수색)할 최적의 하이퍼 파라미터 범위 지정\n",
        "    param = {\n",
        "        'n_estimators': 5000,\n",
        "        'max_depth': trial.suggest_int('max_depth', 8, 20),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 500),\n",
        "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
        "        'subsample': trial.suggest_loguniform('subsample', 0.01, 1.0),\n",
        "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.01, 1.0),\n",
        "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n",
        "    }\n",
        "\n",
        "    xgbcv = xgb.cv(\n",
        "                  params = param,\n",
        "                  dtrain = dtrain_matrix,\n",
        "                  num_boost_round=50000, \n",
        "                  folds= KFold(n_splits=5, shuffle=True),\n",
        "                  verbose_eval=False, \n",
        "                  metrics = 'rmse',\n",
        "                  early_stopping_rounds = 200,\n",
        "                  )\n",
        "    \n",
        "    cv_score = list(xgbcv['test-rmse-mean'])[-1]\n",
        "    \n",
        "    # Return metric of interest\n",
        "    return cv_score"
      ],
      "metadata": {
        "id": "LetTC_aEcW7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(lambda trial: objective(trial, X, y),\n",
        "               timeout=1800,\n",
        "               n_trials=100,\n",
        "               n_jobs=1,\n",
        "               show_progress_bar=True) "
      ],
      "metadata": {
        "id": "-fvlLcNIcc_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(study.best_params)"
      ],
      "metadata": {
        "id": "XYDqphnbcfZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(study.best_value**0.5)"
      ],
      "metadata": {
        "id": "QM496PQLcgm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CV Stacking"
      ],
      "metadata": {
        "id": "Ph6PZo3McXIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stacking_base_datasets(X, y, df_test, n_splits, n_repeats):\n",
        "\n",
        "  num_model = 0\n",
        "\n",
        "  train_fold_pred = np.zeros((X.shape[0] ,1 ))\n",
        "  test_pred = np.zeros((df_test.shape[0], n_splits))\n",
        "\n",
        "  skf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
        "\n",
        "  num_fold = 0\n",
        "\n",
        "  for fold_idx, (train_index, valid_index) in enumerate(tqdm(skf.split(X, y))):\n",
        "      X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "      y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "      xgb_params = {\n",
        "        'n_estimators': 50000,\n",
        "        'verbose':1000,\n",
        "        **study.best_params\n",
        "    }\n",
        "\n",
        "      print(f'\\nNumber fold : {num_fold}\\n')\n",
        "\n",
        "      model = XGBRegressor(**xgb_params)\n",
        "\n",
        "      model.fit(X_train, y_train,\n",
        "                eval_set=[(X_valid, y_valid)],\n",
        "                early_stopping_rounds=100,\n",
        "                use_best_model=True,\n",
        "                verbose=100)\n",
        "      \n",
        "      y_pred = model.predict(X_valid)\n",
        "\n",
        "      MSE = mean_squared_error(y_pred, y_valid)\n",
        "      print(f'\\n{num_fold} 번 Model Vaild MSE : {MSE}\\n')\n",
        "\n",
        "      train_fold_pred[valid_index, :] = y_pred.reshape(-1,1)\n",
        "      test_pred[:, num_model] = model.predict(df_test)\n",
        "\n",
        "      num_model += 1\n",
        "\n",
        "  test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1) \n",
        "\n",
        "  return train_fold_pred , test_pred_mean"
      ],
      "metadata": {
        "id": "FA_PUj4zcZnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacking_train, stacking_test = get_stacking_base_datasets(X, y, X_test, 5, 3)"
      ],
      "metadata": {
        "id": "5HucmLqhcwrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_Stacking_train = pd.DataFrame(stacking_train)\n",
        "XGB_Stacking_train['target'] = y\n",
        "\n",
        "XGB_Stacking_test = pd.DataFrame(stacking_test)"
      ],
      "metadata": {
        "id": "oMxviQJxczJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_Stacking_train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "KZ3F4NJMc13a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_Stacking_train.to_csv(path_stacking_data+'CAT_Stacking_train.csv')\n",
        "XGB_Stacking_test.to_csv(path_stacking_data+'CAT_Stacking_test.csv')"
      ],
      "metadata": {
        "id": "3zDHtwQVc7MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meta Model"
      ],
      "metadata": {
        "id": "OIlsDQgxQIp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_train = {\n",
        "    'LGBM':LGBM_Stacking_train['0'],\n",
        "    'CAT':CAT_Stacking_train['0'],\n",
        "    'XGB':XGB_Stacking_train['0'],\n",
        "    'target':LGBM_Stacking_train['target']\n",
        "}\n",
        "\n",
        "train = pd.DataFrame(dict_train)"
      ],
      "metadata": {
        "id": "wWwuT1tXQKR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_test = {\n",
        "    'LGBM':LGBM_Stacking_test['0'],\n",
        "    'CAT':CAT_Stacking_test['0'],\n",
        "    'XGB':XGB_Stacking_test['0']\n",
        "}\n",
        "\n",
        "test = pd.DataFrame(dict_test)"
      ],
      "metadata": {
        "id": "Qc_zhDEORgEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train.drop(columns='target')\n",
        "y = train['target']"
      ],
      "metadata": {
        "id": "gfEeQWk6RzjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svr = LinearSVR(max_iter= 1000000, verbose=1)\n",
        "\n",
        "svr.fit(X, y)"
      ],
      "metadata": {
        "id": "oBFDNu4IRtcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['rating'] = svr.predict(test)\n",
        "submission = test['rating']\n",
        "\n",
        "save_path = '/content/drive/MyDrive/머신러닝 엔지니어링/Kaggle/Scrabble Player Rating/'\n",
        "submission.to_csv(save_path+\"stacking_LGBM_CAT.csv\")"
      ],
      "metadata": {
        "id": "0y9AE_KjRvoG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}