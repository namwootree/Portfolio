{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting"
      ],
      "metadata": {
        "id": "DTm2_h2nMJIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU"
      ],
      "metadata": {
        "id": "uyzaPsgmMLvN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLeasiXyL4L_",
        "outputId": "4c0de4ba-263f-4bf8-c436-f1a5b6309779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 18 05:35:50 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0    45W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library"
      ],
      "metadata": {
        "id": "fVyLQEWSMSx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import *\n",
        "\n",
        "import gc\n",
        "import glob\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "dENsByyfMT9T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "hrTsgEqpMXs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_to_parquet(csv_path, save_name):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.to_parquet(f'./{save_name}.parquet')\n",
        "    del df\n",
        "    gc.collect()\n",
        "    print(save_name, 'Done.')"
      ],
      "metadata": {
        "id": "wayiic_OMYzD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb7bGlv-Ma_T",
        "outputId": "63e5fb57-b5d7-47e5-8281-38f6a804227c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_submission_path = '/content/drive/MyDrive/머신러닝 엔지니어링/데이콘/제주도 도로 교통량 예측/data/sample_submission.csv'\n",
        "df_train_path = '/content/drive/MyDrive/머신러닝 엔지니어링/데이콘/제주도 도로 교통량 예측/data/train.csv'\n",
        "df_test_path = '/content/drive/MyDrive/머신러닝 엔지니어링/데이콘/제주도 도로 교통량 예측/data/test.csv'"
      ],
      "metadata": {
        "id": "Nbig_G6FMeD_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_to_parquet(df_train_path, 'train')\n",
        "csv_to_parquet(df_test_path, 'test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1VnnTaqMi-J",
        "outputId": "f0b1735d-fea7-4473-d5e6-b38b0d603b76"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Done.\n",
            "test Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_parquet('/content/train.parquet')\n",
        "test = pd.read_parquet('/content/test.parquet')"
      ],
      "metadata": {
        "id": "JNJj8IODMmD3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "buWG4OtGM7HH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LabelEncoder"
      ],
      "metadata": {
        "id": "3CNlwoKLM_37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str_col = ['day_of_week',\n",
        "           'base_hour',\n",
        "           'lane_count',\n",
        "           'maximum_speed_limit',\n",
        "           'start_latitude',\n",
        "           'start_longitude',\n",
        "           'end_latitude',\n",
        "           'end_longitude',\n",
        "           'road_rating',\n",
        "           'weight_restricted',\n",
        "           'start_turn_restricted',\n",
        "           'end_turn_restricted',\n",
        "           'start_node_name', \n",
        "           'end_node_name', \n",
        "           'road_type',\n",
        "           'road_name', \n",
        "           'connect_code', \n",
        "           'multi_linked']"
      ],
      "metadata": {
        "id": "YWQj_OBHM8aA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(str_col):\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    le=le.fit(train[i])\n",
        "    train[i]=le.transform(train[i])\n",
        "    \n",
        "    for label in np.unique(test[i]):\n",
        "        if label not in le.classes_: \n",
        "            le.classes_ = np.append(le.classes_, label)\n",
        "    test[i]=le.transform(test[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqj4-vweM-AW",
        "outputId": "32a7380a-b729-459c-e9a7-4936a43914c7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [00:14<00:00,  1.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split X / y "
      ],
      "metadata": {
        "id": "hdlMmTAWNDJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train['target'] \n",
        "\n",
        "X_train = train.drop(['id', 'target', 'vehicle_restricted', 'height_restricted'], axis=1)\n",
        "\n",
        "test = test.drop(['id', 'vehicle_restricted', 'height_restricted'], axis=1)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxPhPbUANGU-",
        "outputId": "eda29fc2-e6ab-4564-be8a-99384c61b796"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4701217, 19)\n",
            "(4701217,)\n",
            "(291241, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## array"
      ],
      "metadata": {
        "id": "LUCv2FiGNLvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "test = np.array(test)"
      ],
      "metadata": {
        "id": "wIjFI1yDNNlz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ml-GQNuNPBd",
        "outputId": "cbf21fd3-1b21-45e5-b381-676189af9899"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4701217, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## External Data"
      ],
      "metadata": {
        "id": "6aCk1LtyNds2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 제주시 월별 평년 값 중 최고기온 (NOAA 1991년 ~ 2022년)"
      ],
      "metadata": {
        "id": "KNRGloi3Njvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_temp = np.array([8, 9, 12, 17, 21, 24, 29, 29, 25, 21, 16, 10]) "
      ],
      "metadata": {
        "id": "VT8-TMi7NiQc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_temp_max = np.max(max_temp)\n",
        "max_temp = max_temp / max_temp_max "
      ],
      "metadata": {
        "id": "cxuFoWqQNyB0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 휴일 (네이버 2021년 ~ 2022년)"
      ],
      "metadata": {
        "id": "aQ41QN34N5MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "holiday = np.array([\n",
        "                    20210920, \n",
        "                    20210921, \n",
        "                    20210922, \n",
        "                    20211003, \n",
        "                    20211004, \n",
        "                    20211009, \n",
        "                    20211011, \n",
        "                    20211225,\n",
        "                    20220101, \n",
        "                    20220131, \n",
        "                    20220201, \n",
        "                    20220202, \n",
        "                    20220301, \n",
        "                    20220309, \n",
        "                    20220505, \n",
        "                    20220508,\n",
        "                    20220601, \n",
        "                    20220606, \n",
        "                    20220815\n",
        "                    ]) "
      ],
      "metadata": {
        "id": "f7vsoAXgN2GG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 최고 기온 및 휴일 데이터 삽입"
      ],
      "metadata": {
        "id": "_FOzhXAOOeca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "\n",
        "for k, d in tqdm(enumerate(X_train)):\n",
        "\n",
        "    month_i = np.int((d[0]%10000)/100) - 1\n",
        "\n",
        "    d = np.concatenate([\n",
        "        d[1:], [np.minimum(3,np.min(np.abs(holiday - d[0]))), max_temp[month_i]] #input data에 휴무일과의 차이(0일, 1일, 2일, 3일 이상), 최고기온에 대한 월별 평년값 추가 \n",
        "        ])\n",
        "    \n",
        "    x_train.append([d, y_train[k]])\n",
        "\n",
        "x_test = []\n",
        "\n",
        "for d in tqdm(test):\n",
        "\n",
        "    month_i = np.int((d[0]%10000)/100) - 1\n",
        "    \n",
        "    d = np.concatenate([\n",
        "        d[1:], [np.minimum(3, np.min(np.abs(holiday - d[0]))), max_temp[month_i]]\n",
        "        ])\n",
        "    \n",
        "    x_test.append(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHPI8Wx5OH5Z",
        "outputId": "9324e617-ee31-4dad-b1d5-b94b01e5c990"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \"\"\"\n",
            "4701217it [01:44, 44880.17it/s]\n",
            "  0%|          | 0/291241 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "100%|██████████| 291241/291241 [00:05<00:00, 49184.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJpShnb2OXEZ",
        "outputId": "558a3315-eebb-4d0e-9d82-fb816c4a29b1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "o9hsWxYsOnfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def known_data_model(input_layer, start_neurons):\n",
        "\n",
        "    #각각의 input값에 대한 embedding\n",
        "    input_dims = [7, 24, 3, 3, 61, 2, 2, 6, 4, 2, 487, 586, 586, 2, 487, 586, 586, 2, 4]\n",
        "\n",
        "    for i in range(20):\n",
        "\n",
        "        if i==0:\n",
        "            input_embedding = layers.Embedding(input_dim=input_dims[i], output_dim=start_neurons)(input_layer[:, i]) \n",
        "\n",
        "        elif i >= 19:\n",
        "            input_embedding = layers.concatenate([input_embedding, layers.Dense(start_neurons)(input_layer[:, i:i+1])])\n",
        "\n",
        "        else :\n",
        "            input_embedding = layers.concatenate([input_embedding, layers.Embedding(input_dim=input_dims[i], output_dim=start_neurons)(input_layer[:, i])])\n",
        "    \n",
        "    \n",
        "    \n",
        "    print(input_embedding.get_shape().as_list())\n",
        "    \n",
        "    all_layer = input_embedding\n",
        "\n",
        "    for layer_num in range(5):\n",
        "\n",
        "        all_layer_d = layers.Dropout(0.2)(all_layer)\n",
        "        all_layer_d_gate = layers.Dense(all_layer_d.get_shape().as_list()[-1])(all_layer_d)\n",
        "        all_layer_ = all_layer * tf.math.sigmoid(all_layer_d_gate) #weighted sigmoid gate unit\n",
        "        all_layer_c = layers.concatenate([all_layer, all_layer_]) \n",
        "        all_layer += layers.Dense(20*start_neurons, activation='relu')(all_layer_c)\n",
        "    \n",
        "    output1 = tf.squeeze(layers.Dense(1)(all_layer), axis=-1)\n",
        "    output2 = tf.squeeze(layers.Dense(1)(all_layer), axis=-1)\n",
        "    output3 = tf.squeeze(layers.Dense(1)(all_layer), axis=-1)\n",
        "    output4 = tf.squeeze(layers.Dense(1)(all_layer), axis=-1)\n",
        "    output5 = tf.squeeze(layers.Dense(1)(all_layer), axis=-1)\n",
        "    output6 = tf.squeeze(layers.Dense(1)(all_layer), axis=-1)\n",
        "    output7 = tf.squeeze(layers.Dense(1)(all_layer), axis=-1)\n",
        "    output8 = tf.squeeze(layers.Dense(1)(all_layer), axis=-1)\n",
        "    output9 = tf.squeeze(layers.Dense(1)(all_layer), axis=-1)\n",
        "    output10 = tf.squeeze(layers.Dense(1)(all_layer), axis=-1)\n",
        "\n",
        "    output = (output1 + output2 + output3 + output4 + output5 + output6 + output7 + output8 + output9 + output10) / 10 #average output\n",
        "\n",
        "    return output "
      ],
      "metadata": {
        "id": "oo2da4iAOpIa"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.MirroredStrategy() # multi GPU parallelization strategy"
      ],
      "metadata": {
        "id": "nHxD8IJkO9dT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "PFBnGdYISPth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir models"
      ],
      "metadata": {
        "id": "kdvpagG-TpSA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fold include day (need test w/o day), include output_dim, 256, layer 6, w/ all BatchNorm (need test w/o BatchNorm), \n",
        "\n",
        "\n",
        "# Ensemble codes in public discussion were used.\n",
        "def trainGenerator():\n",
        "    for data in train_data_:\n",
        "        target = data[1]\n",
        "        feature = data[0]\n",
        "        yield (feature, target)\n",
        "\n",
        "def valGenerator():\n",
        "    for data in val_data:\n",
        "        target = data[1]\n",
        "        feature = data[0]\n",
        "        yield (feature, target)\n",
        "\n",
        "kfold_list = [2, 3, 4, 5, 6, 10, 20]\n",
        "\n",
        "for kfold in kfold_list:\n",
        "\n",
        "    kf = KFold(n_splits=kfold, random_state=42, shuffle=True)\n",
        "\n",
        "    for fold, (train, val) in enumerate(kf.split(x_train)):\n",
        "\n",
        "\n",
        "        val_data = np.array(x_train)[val]\n",
        "        train_data_ = np.array(x_train)[train]\n",
        "\n",
        "\n",
        "    \n",
        "        tr_ds = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([20]), tf.TensorShape([])))\n",
        "        tr_ds = tr_ds.cache()\n",
        "        tr_ds = tr_ds.shuffle(100000).padded_batch(4096)\n",
        "        tr_ds = tr_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "        val_ds = tf.data.Dataset.from_generator(valGenerator, (tf.float32, tf.float32), (tf.TensorShape([20]), tf.TensorShape([])))\n",
        "        val_ds = val_ds.cache()\n",
        "        val_ds = val_ds.batch(4096).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    \n",
        "        with strategy.scope():\n",
        "\n",
        "            input_layer = Input((20))\n",
        "  \n",
        "            outputs = known_data_model(input_layer, 32)\n",
        "            model = Model(input_layer, outputs)\n",
        "\n",
        "            adam = tf.keras.optimizers.Adam()\n",
        "\n",
        "            model.compile(optimizer=adam,\n",
        "                  loss=tf.keras.losses.MeanAbsoluteError())\n",
        "            callbacks = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss', factor=.1, patience=2, verbose=0, mode='min', min_delta=1e-4, cooldown=0, min_lr=0\n",
        "            )\n",
        "\n",
        "            sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "            f'/content/models/ehfehf-{fold}-road_all_org_fold_{kfold}.h5', monitor='val_loss', verbose=0, save_best_only=True,\n",
        "            save_weights_only=True, mode='min', save_freq='epoch'\n",
        "            )\n",
        "\n",
        "            es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, mode='min')\n",
        "\n",
        "            model.fit(tr_ds, epochs=15000, verbose=1, validation_data=val_ds, callbacks=[callbacks, sv, es])\n",
        "\n",
        "            del model \n",
        "\n",
        "            gc.collect()\n",
        "\n",
        "print('Training complete.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoS2sk62RMKv",
        "outputId": "09fe1fae-7587-4508-a427-8a6ec19128ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, 640]\n",
            "Epoch 1/15000\n",
            "    574/Unknown - 284s 465ms/step - loss: 4.7092"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "TLznom7zSTCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "\n",
        "for kfold in kfold_list:\n",
        "\n",
        "    for n_fold in range(kfold):\n",
        "      \n",
        "        input_layer = Input((20))\n",
        "        outputs = known_data_model(input_layer, 32)\n",
        "        model = Model(input_layer, outputs)\n",
        "        model.load_weights(f'./models/ehfehf-{n_fold}-road_all_org_fold_{kfold}.h5')\n",
        "        val_pred = model.predict(np.array(x_test))\n",
        "        pred.append(val_pred)"
      ],
      "metadata": {
        "id": "UyLgGSkpRPol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_sum = sum(pred)    "
      ],
      "metadata": {
        "id": "tpKjo6-LSFGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_sum /= len(pred)\n",
        "pred_sum"
      ],
      "metadata": {
        "id": "mMHIdMTvSGJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred = np.round(pred_sum) \n",
        "val_pred"
      ],
      "metadata": {
        "id": "6cP7VrokSIPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission"
      ],
      "metadata": {
        "id": "LqsxJtFQSLr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission = pd.read_csv('./sample_submission.csv')\n",
        "sample_submission['target'] = val_pred\n",
        "sample_submission.to_csv(\"./submit.csv\", index = False)"
      ],
      "metadata": {
        "id": "T4e2GKOFSMxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}