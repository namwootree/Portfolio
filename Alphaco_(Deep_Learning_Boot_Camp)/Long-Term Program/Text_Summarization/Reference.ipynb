{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namwootree/Portfolio/blob/main/Alphaco_(Deep_Learning_Boot_Camp)/Long-Term%20Program/Text_Summarization/Reference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a482DHPilRc"
      },
      "source": [
        "# **Dacon Gas**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Abstract**"
      ],
      "metadata": {
        "id": "ma28F02dDTwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "본 문서는 한국가스공사에서 주최 및 주관하고, 데이콘 주식회사에서 운영한 가스・에너지분야 문서요약 모델개발 대회에 참여한 이야기연구소 주식회사의 모델 복원을 위한 코드입니다. PyTorch를 백엔드로 하는 HuggingFace Trainer와 Pretrain된 KoBART를 Finetuning 하였으며, 특별히 [기존 코드](https://github.com/cawandmilk/gas)에서 변경된 사항은 아래와 같습니다.\n",
        "\n",
        "* 자체 서버와는 달리 코랩 환경에서는 리소스 사용량에 제한이 있으므로, 훈련 및 추론 시간을 줄이기 위해 전체 데이터의 10%만 덜어서 사용하였습니다.\n",
        "\n",
        "* 비공개 테스트 데이터 세트는 대회 종료 이후 공개되지 않을 예정이므로, 검증용 데이터 세트의 일부를 덜어내어 실제 추론간 사용하였습니다.\n",
        "\n",
        "* 비공개 테스트 데이터 세트의 정답을 알고있으며 [채점 프로그램](https://dacon.io/competitions/official/235673/talkboard/401911?page=1&dtype=recent) 또한 공개되어 있으므로, 생성문에 대한 Rouge Score를 계산하는 코드를 추가하였습니다."
      ],
      "metadata": {
        "id": "K9hsHF5lDVyE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFg2-ygCis25"
      },
      "source": [
        "## **Default Setting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTsBecUZjJGS"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC8ED87DjHsN"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heaQyIAQis0K",
        "outputId": "c548c169-b1e5-423f-8ffd-1f46ce5bc853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VERSION]\n",
            "torch: 1.10.0+cu111\n",
            "transformers: 4.15.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "import datetime\n",
        "import easydict\n",
        "import itertools\n",
        "import json\n",
        "import matplotlib\n",
        "import pathlib\n",
        "import pprint\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from operator import itemgetter\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict\n",
        "\n",
        "## Version.\n",
        "print(\"[VERSION]\")\n",
        "print(f\"torch: {torch.__version__}\")\n",
        "print(f\"transformers: {transformers.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLt2ml07j6PO",
        "outputId": "ec81d24e-c048-4073-fa0b-cc8781d39f97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ],
      "source": [
        "!python -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDVsO2EwtDKe",
        "outputId": "ad5502fb-e592-4bc0-b054-ac9e649dd4ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         167G   43G  125G  26% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm              13G     0   13G   0% /dev/shm\n",
            "/dev/root       2.0G  1.2G  817M  59% /sbin/docker-init\n",
            "tmpfs            13G   28K   13G   1% /var/colab\n",
            "/dev/sda1       174G   46G  128G  27% /opt/bin/.nvidia\n",
            "tmpfs            13G     0   13G   0% /proc/acpi\n",
            "tmpfs            13G     0   13G   0% /proc/scsi\n",
            "tmpfs            13G     0   13G   0% /sys/firmware\n"
          ]
        }
      ],
      "source": [
        "!df -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BCEa4JBhHeT",
        "outputId": "893d12a9-0aeb-4319-e074-07d28aec2bef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec 27 04:18:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            25G        1.1G         19G        1.2M        4.4G         24G\n",
            "Swap:            0B          0B          0B\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi; free -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_sc7CCyjvpg",
        "outputId": "a6670d75-dcf5-48a6-f37d-e3b38184acbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   'answer_path': 'answer.tsv',\n",
            "    'beam_size': 5,\n",
            "    'ckpt': 'ckpt',\n",
            "    'clean': False,\n",
            "    'data': 'data',\n",
            "    'gpu_id': 0,\n",
            "    'gradient_accumulation_steps': 16,\n",
            "    'inp_max_len': 1024,\n",
            "    'length_penalty': 0.8,\n",
            "    'logs': 'logs',\n",
            "    'lr': 5e-05,\n",
            "    'model_fpath': '/content/drive/MyDrive/ColabNotebooks/dacon-gas/kobart-model.pth',\n",
            "    'n_epochs': 10,\n",
            "    'no_repeat_ngram_size': 3,\n",
            "    'per_replica_batch_size': 8,\n",
            "    'prediction_path': 'prediction.tsv',\n",
            "    'pretrained_model_name': 'gogamza/kobart-base-v1',\n",
            "    'raw_train_path': './data/Training',\n",
            "    'raw_valid_path': './data/Validation',\n",
            "    'sample_submission_path': 'sample_submission.tsv',\n",
            "    'seed': 42,\n",
            "    'tar_max_len': 256,\n",
            "    'test': 'data/test.tsv',\n",
            "    'train': 'data/train.tsv',\n",
            "    'valid': 'data/valid.tsv',\n",
            "    'var_len': False,\n",
            "    'warmup_ratio': 0.2,\n",
            "    'weight_decay': 0.01}\n"
          ]
        }
      ],
      "source": [
        "def print_elements(a: dict) -> None:\n",
        "    pprint.PrettyPrinter(indent=4).pprint(a)\n",
        "\n",
        "config = easydict.EasyDict({\n",
        "    \"raw_train_path\": \"./data/Training\",\n",
        "    \"raw_valid_path\": \"./data/Validation\",\n",
        "    \"clean\": False,\n",
        "    \"data\": \"data\", ## data path\n",
        "    \"seed\": 42,\n",
        "    \"sample_submission_path\": \"sample_submission.tsv\",\n",
        "    \"answer_path\": \"answer.tsv\",\n",
        "    \"prediction_path\": \"prediction.tsv\",\n",
        "    \"pretrained_model_name\": \"gogamza/kobart-base-v1\",\n",
        "    \"train\": \"data/train.tsv\",\n",
        "    \"valid\": \"data/valid.tsv\",\n",
        "    \"test\": \"data/test.tsv\",\n",
        "    ## Training arguments.\n",
        "    \"ckpt\": \"ckpt\", ## path\n",
        "    \"logs\": \"logs\", ## path\n",
        "    \"per_replica_batch_size\": 8,\n",
        "    \"gradient_accumulation_steps\": 16,\n",
        "    \"lr\": 5e-5,\n",
        "    \"weight_decay\": 1e-2,\n",
        "    \"warmup_ratio\": 0.2,\n",
        "    \"n_epochs\": 10,\n",
        "    \"inp_max_len\": 1024,\n",
        "    \"tar_max_len\": 256,\n",
        "    \"model_fpath\": \"/content/drive/MyDrive/ColabNotebooks/dacon-gas/kobart-model.pth\",\n",
        "    ## Inference.\n",
        "    \"gpu_id\": 0,\n",
        "    \"beam_size\": 5,\n",
        "    \"length_penalty\": 0.8,\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"var_len\": False,\n",
        "})\n",
        "\n",
        "print_elements(vars(config)) ## sort_dicts=False -> > python 3.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkcFdFO9kO_1",
        "outputId": "1a0d9d4e-a081-458f-da78-42c1847dfb5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "except:\n",
        "    raise AssertionError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb5FjbNAinpR"
      },
      "source": [
        "## **Get Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[AI Hub](https://aihub.or.kr/aidata/8054)에서 다운로드 받은 데이터 세트를 그대로 압축하여 구글 드라이브에 올리고, 이를 Colab에서 마운트하여 사용하였습니다. Google Drive에 위치한 파일의 저장 경로만 적절히 수정 후 실행하시면 됩니다.\n",
        "\n",
        "실행 결과는 다음과 같아야 합니다.\n",
        "\n",
        "```bash\n",
        "./data\n",
        "├── [4.0K]  Training\n",
        "│   ├── [ 75M]  사설_train_original.zip\n",
        "│   ├── [ 16M]  법률_train_original.zip\n",
        "│   └── [267M]  신문기사_train_original.zip\n",
        "└── [4.0K]  Validation\n",
        "    ├── [1.6M]  법률_valid_original.zip\n",
        "    ├── [7.9M]  사설_valid_original.zip\n",
        "    └── [ 34M]  신문기사_valid_original.zip\n",
        "\n",
        "2 directories, 6 files\n",
        "```"
      ],
      "metadata": {
        "id": "xxS8oNtIJqY5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZJEyPR2i8D5",
        "outputId": "51e78fb5-9774-4f38-f103-45886cbf7720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data\n",
            "├── [4.0K]  Training\n",
            "│   ├── [ 75M]  사설_train_original.zip\n",
            "│   ├── [ 16M]  법률_train_original.zip\n",
            "│   └── [267M]  신문기사_train_original.zip\n",
            "└── [4.0K]  Validation\n",
            "    ├── [1.6M]  법률_valid_original.zip\n",
            "    ├── [7.9M]  사설_valid_original.zip\n",
            "    └── [ 34M]  신문기사_valid_original.zip\n",
            "\n",
            "2 directories, 6 files\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "## Copy zip file from google drive to colab session.\n",
        "cp \"/content/drive/MyDrive/ColabNotebooks/dacon-gas/data/문서요약 텍스트.zip\" ./\n",
        "\n",
        "## Unzip.\n",
        "mkdir ./data\n",
        "unzip -q \"/content/문서요약 텍스트.zip\" -d ./data\n",
        "\n",
        "## Show lists.\n",
        "tree -alh ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "필요한 신문기사 데이터 세트만 압축을 해제하여 사용합니다.\n",
        "\n",
        "마찬가지로, 실행 결과는 다음과 같아야 합니다.\n",
        "\n",
        "```bash\n",
        "./data\n",
        "├── [4.0K]  Training\n",
        "│   ├── [1.1G]  신문기사_train_original.json\n",
        "│   ├── [ 75M]  사설_train_original.zip\n",
        "│   ├── [ 16M]  법률_train_original.zip\n",
        "│   └── [267M]  신문기사_train_original.zip\n",
        "└── [4.0K]  Validation\n",
        "    ├── [140M]  신문기사_valid_original.json\n",
        "    ├── [1.6M]  법률_valid_original.zip\n",
        "    ├── [7.9M]  사설_valid_original.zip\n",
        "    └── [ 34M]  신문기사_valid_original.zip\n",
        "\n",
        "2 directories, 8 files\n",
        "```"
      ],
      "metadata": {
        "id": "Nrhnb4WiKtc1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1RfC7zNnR0U",
        "outputId": "5db398ac-524c-423c-882f-ce01e25b19d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data\n",
            "├── [4.0K]  Training\n",
            "│   ├── [1.1G]  신문기사_train_original.json\n",
            "│   ├── [ 75M]  사설_train_original.zip\n",
            "│   ├── [ 16M]  법률_train_original.zip\n",
            "│   └── [267M]  신문기사_train_original.zip\n",
            "└── [4.0K]  Validation\n",
            "    ├── [140M]  신문기사_valid_original.json\n",
            "    ├── [1.6M]  법률_valid_original.zip\n",
            "    ├── [7.9M]  사설_valid_original.zip\n",
            "    └── [ 34M]  신문기사_valid_original.zip\n",
            "\n",
            "2 directories, 8 files\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "## Extract only \"신문기사\" dataset.\n",
        "cd /content/data/Training \\\n",
        "    && unzip -q \"신문기사_train_original.zip\" \\\n",
        "    && mv ./train_original.json ./신문기사_train_original.json\n",
        "\n",
        "cd /content/data/Validation \\\n",
        "    && unzip -q \"신문기사_valid_original.zip\" \\\n",
        "    && mv ./valid_original.json ./신문기사_valid_original.json\n",
        "\n",
        "## Show lists.\n",
        "cd /content && tree -alh ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzbSdMA6iyNh"
      },
      "source": [
        "## **Preprocess**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8DEOynPvc4J"
      },
      "source": [
        "### **Load JSON**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JSON 포맷의 데이터 세트를 호출합니다."
      ],
      "metadata": {
        "id": "Fwa281eoK-Kw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyDqaBHtqtZb"
      },
      "outputs": [],
      "source": [
        "def read_samples(raw_path: str, sort_dicts: bool = True) -> List[Dict[str, str]]:\n",
        "    \n",
        "    def _read_json(fpath: pathlib.PosixPath) -> List[Dict[str, str]]:\n",
        "        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
        "            document = json.loads(f.read())[\"documents\"]\n",
        "        return document\n",
        "\n",
        "    # def _read_jsonl(fpath: pathlib.PosixPath) -> List[Dict[str, str]]:\n",
        "    #     with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
        "    #         documents = [json.loads(line) for line in f]\n",
        "    #     return documents\n",
        "\n",
        "    for sample in Path(raw_path).glob(\"*.json\"):\n",
        "        ## We only use \"신문기사\" dataset, not \"법률\" or \"사설\".\n",
        "        if not sample.name.startswith(\"신문기사\"):\n",
        "            continue\n",
        "\n",
        "        documents = _read_json(sample)\n",
        "\n",
        "        ## Sort dictionaries by \"id\".\n",
        "        if sort_dicts:\n",
        "            documents = sorted(\n",
        "                documents,\n",
        "                key=itemgetter(\"id\"),\n",
        "                reverse=False,\n",
        "            )\n",
        "\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvieBaGHr42V",
        "outputId": "669603a7-3d9a-47ff-d6d7-e841402a523b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   'abstractive': [   '당진 지역업체인 중앙개발과 GS건설의 하도급 분쟁이 심화되고 있는 가운데 안전 문제를 비롯하여 '\n",
            "                       '대기업과 하청업체 간의 하도급 실태가 심각한 사회적 문제로 대두되고 있다.'],\n",
            "    'annotator_id': 121,\n",
            "    'category': '사회',\n",
            "    'char_count': '1786',\n",
            "    'document_quality_scores': {   'accurate': 2,\n",
            "                                   'informative': 2,\n",
            "                                   'readable': 2,\n",
            "                                   'trustworthy': 1},\n",
            "    'extractive': [1, 15, 13],\n",
            "    'id': '250362727',\n",
            "    'media_name': '당진시대',\n",
            "    'media_sub_type': '지역지',\n",
            "    'media_type': 'online',\n",
            "    'publish_date': '2017-01-06 08:31:00',\n",
            "    'size': 'large',\n",
            "    'text': [   [   {   'highlight_indices': '',\n",
            "                        'index': 0,\n",
            "                        'sentence': '“GS 공사 참여…12억 원 손해” “하도급 문제 근본적 개선 필요하다”'},\n",
            "                    {   'highlight_indices': '52,53',\n",
            "                        'index': 1,\n",
            "                        'sentence': '당진 지역업체인 중앙개발이 최근 GS건설로부터 4건의 공사를 하도급 받아 '\n",
            "                                    '진행했지만 현재까지 총 12억 원 가량의 손해를 봐 도산 위기에 몰렸다고 '\n",
            "                                    '호소하고 있다.'}],\n",
            "                [   {   'highlight_indices': '59,60;66,68',\n",
            "                        'index': 2,\n",
            "                        'sentence': '중앙개발 이상학 대표에 따르면 GS EPS가 당진4호기 바이오매스 발전소 '\n",
            "                                    '건설공사을 추진하면서 민원 해결 및 지역업체 우선 사용 등의 이유로 관련 '\n",
            "                                    '공사에 참여하게 됐다.'},\n",
            "                    {   'highlight_indices': '0,3',\n",
            "                        'index': 3,\n",
            "                        'sentence': '그러나 공사를 진행하던 중 바닷물이 올라와 이를 수습하는 등 공사비가 당초 '\n",
            "                                    '77억 원에서 105억 원까지 상승했음에도 불구하고 원청인 GS건설 '\n",
            "                                    '측에서는 이에 합당한 공사비를 주지 않았다고 주장하고 있다.'},\n",
            "                    {   'highlight_indices': '0,1',\n",
            "                        'index': 4,\n",
            "                        'sentence': '이 대표는 “GS건설 측이 보험 문제로 서명이 필요하다면서, 대표이사인 '\n",
            "                                    '자신도 모르게 부사장에게 날인을 받았다”며 “GS 측에서는 이를 정산에 '\n",
            "                                    '날인한 것이라고 주장하고 있다”고 말했다.'}],\n",
            "                [   {   'highlight_indices': '4,5;41,43;75,76;77,79',\n",
            "                        'index': 5,\n",
            "                        'sentence': '이어 “이 공사에서 4억9000만 원의 손해를 보게 돼 원청에 항의하니, '\n",
            "                                    '다른 공사 참여로 손해비용을 보전할 수 있게 해주겠다고 해서 또 다른 '\n",
            "                                    '공사를 시작했다”고 말했다.'},\n",
            "                    {   'highlight_indices': '0,3;4,5;27,28;89,90;90,92',\n",
            "                        'index': 6,\n",
            "                        'sentence': '그러나 두 번째 공사인 당진4호기 복합화력발전소 및 배수로 건설 공사에서도 '\n",
            "                                    '1억9000만 원의 적자를 봤고, 같은 이유로 안양2호기 건설 공사에도 '\n",
            "                                    '참여했지만, 또다시 4억5800만 원의 적자를 떠안았다.'},\n",
            "                    {   'highlight_indices': '70,71',\n",
            "                        'index': 7,\n",
            "                        'sentence': '이후 행정중심복합도시 자동크린넷 3-2차 시설 공사에 참여하던 중 1억 '\n",
            "                                    '1000만 원의 적자가 발생하자 결국 공사를 포기하고 타절했다.'}],\n",
            "                [   {   'highlight_indices': '0,3;28,29',\n",
            "                        'index': 8,\n",
            "                        'sentence': '이렇게 4번의 공사에서 중앙개발이 손해 본 금액만 총 12억 원에 '\n",
            "                                    '달한다.'},\n",
            "                    {   'highlight_indices': '40,41',\n",
            "                        'index': 9,\n",
            "                        'sentence': '중앙개발은 지난 2일부터 GS EPS 앞에서 하루 3시간씩 집회를 열고 이 '\n",
            "                                    '문제를 알리고 있다.'},\n",
            "                    {   'highlight_indices': '9,11;39,42;54,56;63,65;100,102',\n",
            "                        'index': 10,\n",
            "                        'sentence': '이상학 대표는 “다른 사업에서 번 돈으로 대부분의 인건비·자재비 등을 '\n",
            "                                    '간신히 지급했지만, 손실이 너무 큰 데다, 다른 사업까지 영향을 미쳐 '\n",
            "                                    '줄도산 나게 생겼다”면서 “손실 금액이 너무 커 죽고 싶은 심정”이라고 '\n",
            "                                    '한탄했다.'}],\n",
            "                [   {   'highlight_indices': '55,58;81,83',\n",
            "                        'index': 11,\n",
            "                        'sentence': '일각에서는 “대기업을 유치함으로써 고용창출 등 경제적 효과를 강조하지만, '\n",
            "                                    '대기업의 횡포로 공사비를 제대로 받지 못해 지역업체가 도산한다면, 이게 '\n",
            "                                    '과연 지역경제를 활성화 하는 것이냐”고 비판했다.'}],\n",
            "                [   {   'highlight_indices': '',\n",
            "                        'index': 12,\n",
            "                        'sentence': '이에 대해 GS 측의 입장을 듣고자 연락을 취했지만 연락이 닿지 않았다.'},\n",
            "                    {   'highlight_indices': '',\n",
            "                        'index': 13,\n",
            "                        'sentence': '한편 대기업과 하청업체 간의 하도급 실태가 심각한 사회적 문제로 떠오르고 '\n",
            "                                    '있다.'},\n",
            "                    {   'highlight_indices': '52,54',\n",
            "                        'index': 14,\n",
            "                        'sentence': '대기업의 최저가 입찰 경쟁으로 하청업체에서는 최대한 공사비를 줄여 이윤을 '\n",
            "                                    '남겨야 하기 때문에 여러 가지 문제가 발생하고 있는 것이다.'},\n",
            "                    {   'highlight_indices': '0,2',\n",
            "                        'index': 15,\n",
            "                        'sentence': '가장 심각한 것은 안전 문제다.'},\n",
            "                    {   'highlight_indices': '',\n",
            "                        'index': 16,\n",
            "                        'sentence': '하청업체는 인건비를 줄이기 위해 최소한의 인력을 투입하는 한편, 공사기간 '\n",
            "                                    '단축에 대한 압박으로 노동자들에게 과중한 업무가 부여되기 때문이다.'}],\n",
            "                [   {   'highlight_indices': '29,31',\n",
            "                        'index': 17,\n",
            "                        'sentence': '지난해 11월 당진공장에서 발생한 현대제철 사망사고 역시 근본적인 원인이 '\n",
            "                                    '여기에 있다는 목소리가 일고 있다.'},\n",
            "                    {   'highlight_indices': '',\n",
            "                        'index': 18,\n",
            "                        'sentence': '당시 현대제철에서 근무하던 하청업체 비정규직 노동자 A씨(37)는 원료를 '\n",
            "                                    '옮기는 통로(슈트)를 점검하던 중 철광석 분배 설비와 슈트 사이에 몸이 '\n",
            "                                    '끼여 압사했다.'},\n",
            "                    {   'highlight_indices': '33,34',\n",
            "                        'index': 19,\n",
            "                        'sentence': '업계에서는 A씨가 혼자 근무하지 않고 2인1조로 근무했으면 이 같은 참사는 '\n",
            "                                    '벌어지지 않았을 것이라고 지적하고 있다.'},\n",
            "                    {   'highlight_indices': '0,2;80,82',\n",
            "                        'index': 20,\n",
            "                        'sentence': '또한 지난해 5월 서울 지하철 2호선 구의역에서 스크린도어를 수리하던 '\n",
            "                                    '용역업체 직원 B씨(20)씨가 열차와 스크린도어 사이에 끼어 숨졌던 사고 '\n",
            "                                    '역시 2인1조가 아닌 혼자 근무하면서 위험한 작업환경에 노출됐기 때문이라는 '\n",
            "                                    '분석이 이어지기도 했다.'},\n",
            "                    {   'highlight_indices': '0,3',\n",
            "                        'index': 21,\n",
            "                        'sentence': '따라서 하도급 문제를 근본적으로 개선해야 한다는 목소리가 지역사회에서도 '\n",
            "                                    '계속해서 높아지고 있는 실정이다.'}],\n",
            "                [   {   'highlight_indices': '',\n",
            "                        'index': 22,\n",
            "                        'sentence': '임아연 zelkova87@hanmail.net'}]],\n",
            "    'title': '대기업 횡포에 지역업체 운다'}\n",
            "CPU times: user 17.9 s, sys: 3.77 s, total: 21.7 s\n",
            "Wall time: 21.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "## Read corpus.\n",
        "tr_corpus = read_samples(config.raw_train_path)\n",
        "vl_corpus = read_samples(config.raw_valid_path)\n",
        "\n",
        "## Print samples.\n",
        "print_elements(tr_corpus[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eixki_9fu-6m"
      },
      "source": [
        "### **Extract Lines**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "불필요한 메타 데이터를 제거한 뒤, 필수적인 입력문(신문기사), 출력문(요약문) 및 id만 추출합니다. 이 과정에서 민감정보 제거 등 공통으로 적용되는 정제 작업과 키 `media_name` (예를 들어, 충청일보, 이데일리 등)에 종속적인 개별 정제 작업이 구현되어 있지만, 실제 성능(Test Rouge Score)이 개선되지 않아 최종적으로는 사용하지 않았습니다.\n",
        "\n",
        "결측치 및 이상치 제거도 본 과정에서 함께 진행됩니다."
      ],
      "metadata": {
        "id": "I8665LiWLMu-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHiJV8YEwQc6"
      },
      "outputs": [],
      "source": [
        "class CleanNewspaperArticleBase():\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "    ):\n",
        "        super(CleanNewspaperArticleBase, self).__init__()\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_reporter_info(article_original: List[str]) -> List[str]:\n",
        "        email_pattern = re.compile(r\"[a-zA-Z0-9+-_.]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\")\n",
        "        reporter_pattern = re.compile(r\"^\\/?([가-힣]+)\\s?(기자|팀)?\\s*\\.?$\")\n",
        "\n",
        "        texts = []\n",
        "        reporters = []\n",
        "        for sentence in article_original:\n",
        "            ## If we can find email pattern in one sentence:\n",
        "            email = email_pattern.findall(sentence)\n",
        "            if email != []:\n",
        "                continue\n",
        "            \n",
        "            ## We don't care about \"OOO 기자 OOO 기자\", not \"OOO 기자\"\n",
        "            reporter = reporter_pattern.findall(sentence)\n",
        "            if reporter != []:\n",
        "                reporters.extend([i[0] for i in reporter])\n",
        "                continue\n",
        "\n",
        "            ## If known reporter name is in sentence...\n",
        "            if any([i in sentence for i in reporters]):\n",
        "                continue\n",
        "            \n",
        "            texts.append(sentence)\n",
        "        \n",
        "        return texts\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_url(article_original: List[str]) -> List[str]:\n",
        "        ## (e.g. id=\"357606465\")\n",
        "        ## Ref: https://www.codegrepper.com/code-examples/python/regex+for+url+python\n",
        "        url_pattern = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = url_pattern.sub(\"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_phone_number(article_original: List[str]) -> List[str]:\n",
        "        phone_pattern = re.compile(r\"\"\"(\n",
        "            (\\d{2}|\\(\\d{2}\\)|\\d{3}|\\(\\d{3}\\))?      ## 2 or 3 words include \"(...)\" patterns -> optional\n",
        "            (|-|\\.)?                                ## sep word: \".\" or \"-\"\n",
        "            (\\d{3}|\\d{4})                           ## 3 or 4 numbers\n",
        "            (\\s|-|\\.)                               ## sep word: \".\" or \"-\"\n",
        "            (\\d{4})                                 ## 4 numbers\n",
        "        )\"\"\", re.VERBOSE | re.MULTILINE)  \n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = phone_pattern.sub(\"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_brack_sentence(article_original: List[str]) -> List[str]:\n",
        "        bracket_1_pattern = re.compile(r\"\\s?<.*>\", re.MULTILINE)        ## e.g. \"<OOO씨 제공>\"\n",
        "        bracket_2_pattern = re.compile(r\"\\s?\\(.*\\)\", re.MULTILINE)      ## e.g. \n",
        "        bracket_3_pattern = re.compile(r\"\\s?\\[.*\\]\", re.MULTILINE)      ## e.g. \n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = bracket_1_pattern.sub(\"\", texts)\n",
        "        texts = bracket_2_pattern.sub(\"\", texts)\n",
        "        texts = bracket_3_pattern.sub(\"\", texts)\n",
        "        \n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_supplementary_sentence(article_original: List[str]) -> List[str]:\n",
        "        ## e.g. \"/광주시양궁협회 제공\"\n",
        "        ## But, it may be an important sentence... (e.g. id=\"334957827\")\n",
        "        supplementary_sentence_pattern = re.compile(r\"^\\s?[▶|\\/][.]*\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = supplementary_sentence_pattern.sub(\"\", texts)\n",
        "        \n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def replace_start_with_hyphen(article_original: List[str]) -> List[str]:\n",
        "        ## e.g. \n",
        "        hyphen_pattern = re.compile(r\"^\\s?\\-\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = hyphen_pattern.sub(\"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def replace_universal_apostrophe(article_original: List[str]) -> List[str]:\n",
        "        ## “ (U+201C), ” (U+201D) -> \" (U+0022)\n",
        "        ## ‘ (U+2018), ’ (U+2019) -> ' (U+0027)\n",
        "        small_apostrophe_pattern = re.compile(r\"[‘|’]\", re.MULTILINE)\n",
        "        large_apostrophe_pattern = re.compile(r\"[“|”]\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = small_apostrophe_pattern.sub(\"'\", texts)\n",
        "        texts = large_apostrophe_pattern.sub(\"\\\"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def replace_repeated_apostrophe(article_original: List[str]) -> List[str]:\n",
        "        ## (e.g. id=\"330644133\")\n",
        "        small_apostrophe_pattern = re.compile(r\"'{2,}\", re.MULTILINE)\n",
        "        large_apostrophe_pattern = re.compile(r\"\\\"{2,}\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = small_apostrophe_pattern.sub(\"'\", texts)\n",
        "        texts = large_apostrophe_pattern.sub(\"\\\"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def apply(article_original: List[str], minimize: bool = True) -> List[str]:\n",
        "        texts = article_original\n",
        "        \n",
        "        ## Remove functions.\n",
        "        texts = CleanNewspaperArticleBase.remove_reporter_info(texts)\n",
        "        texts = CleanNewspaperArticleBase.remove_url(texts)\n",
        "        texts = CleanNewspaperArticleBase.remove_phone_number(texts)\n",
        "\n",
        "        if not minimize:\n",
        "            texts = CleanNewspaperArticleBase.remove_supplementary_sentence(texts)\n",
        "            texts = CleanNewspaperArticleBase.remove_brack_sentence(texts)\n",
        "\n",
        "            # Replace functions.\n",
        "            texts = CleanNewspaperArticleBase.replace_start_with_hyphen(texts)\n",
        "            texts = CleanNewspaperArticleBase.replace_universal_apostrophe(texts)\n",
        "            texts = CleanNewspaperArticleBase.replace_repeated_apostrophe(texts)\n",
        "\n",
        "        return texts\n",
        "\n",
        "\n",
        "class CleanNewspaperArticle():\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "    ):\n",
        "        super(CleanNewspaperArticle, self).__init__()\n",
        "        self.media_name_to_function = {\n",
        "            \"건설경제\": self._cnews,\n",
        "            \"광양신문\": self._gynet,\n",
        "            \"광주매일신문\": self._kjdaily,\n",
        "            \"광주일보\": self._kwangju,\n",
        "            \"국제신문\": self._kookje,\n",
        "            \"기호일보\": self._kihoilbo,\n",
        "            \"남도일보\": self._namdonews,\n",
        "            \"당진시대\": self._djtimes,\n",
        "            \"대구신문\": self._idaegu_co_kr,\n",
        "            \"대구일보\": self._idaegu_com,\n",
        "            \"대전일보\": self._daejonilbo,\n",
        "            \"동양일보\": self._dynews,\n",
        "            \"디지털타임스\": self._dt,\n",
        "            \"매일경제\": self._mk,\n",
        "            \"매일신문\": self._imaeil,\n",
        "            \"머니투데이\": self._mt,\n",
        "            \"무등일보\": self._honam,\n",
        "            \"부산일보\": self._busan,\n",
        "            \"새전북신문\": self._sjbnews,\n",
        "            \"서울경제\": self._sedaily,\n",
        "            \"서울신문\": self._seoul,\n",
        "            \"아시아경제\": self._asiae,\n",
        "            \"아주경제\": self._ajunews,\n",
        "            \"영남일보\": self._yeongnam,\n",
        "            \"울산매일\": self._iusm,\n",
        "            \"이데일리\": self._edaily,\n",
        "            \"인천일보\": self._incheonilbo,\n",
        "            \"전남일보\": self._jnilbo,\n",
        "            \"전라일보\": self._jeollailbo,\n",
        "            \"전북도민일보\": self._domin,\n",
        "            \"전북일보\": self._jjan,\n",
        "            \"제민일보\": self._jemin,\n",
        "            \"제주일보\": self._jejunews,\n",
        "            \"중도일보\": self._joongdo,\n",
        "            \"중부매일\": self._jbnews,\n",
        "            \"중부일보\": self._joongboo,\n",
        "            \"충북일보\": self._inews365,\n",
        "            \"충청일보\": self._ccdailynews,\n",
        "            \"충청투데이\": self._cctoday,\n",
        "            \"한국경제\": self._hankyung,\n",
        "            \"한라일보\": self._ihalla,\n",
        "            \"환경일보\": self._hkbs,\n",
        "        }\n",
        "\n",
        "\n",
        "    def _cnews(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 건설경제 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _gynet(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 광양신문 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _kjdaily(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 광주매일신문 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _kwangju(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 광주일보 \"\"\"\n",
        "        ## Remove standalone \"출처 :\" or \"출처:\"\n",
        "        reference_pattern = re.compile(r\"^(출처)\\s?\\:\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = reference_pattern.sub(\"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _kookje(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 국제신문 \"\"\"\n",
        "        sponser_pattern = re.compile(r\"제공\\s?$\", re.MULTILINE)\n",
        "        dotted_pattern = re.compile(r\"^\\.$\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = sponser_pattern.sub(\"\", texts)\n",
        "        texts = dotted_pattern.sub(\"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _kihoilbo(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 기호일보 \"\"\"\n",
        "        ## Media depended style:\n",
        "        ##   e.g. \"양주=OOO 기자 OOO@OOO.co.kr\"\n",
        "        ##   e.g. \"기호일보, KIHOILBO\"\n",
        "        reporter_pattern = re.compile(r\"^[가-힣]+\\=[가-힣]+\\s?(기자|팀)?$\", re.MULTILINE)\n",
        "        media_pattern = re.compile(r\"^\\s?기호일보, KIHOILBO\\s?$\", re.MULTILINE)\n",
        "        picture_reference_pattern = re.compile(r\"^\\s?(포토|사진)\\s?\\:[.]*\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = reporter_pattern.sub(\"\", texts)\n",
        "        texts = media_pattern.sub(\"\", texts)\n",
        "        texts = picture_reference_pattern.sub(\"\", texts)\n",
        "        \n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _namdonews(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 남도일보 \"\"\"\n",
        "        picture_reference_pattern = re.compile(r\"^\\s?사진\\s?\\=[.]*\", re.MULTILINE)\n",
        "        \n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = picture_reference_pattern.sub(\"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _djtimes(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 당진시대 \"\"\"\n",
        "        removed_special_token_pattern = re.compile(r\"^\\s?(▲|■|※)[.]*\", re.MULTILINE)\n",
        "        replaced_special_token_pattern = re.compile(r\"^\\s?[>]{2}\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = removed_special_token_pattern.sub(\"\", texts)\n",
        "        texts = replaced_special_token_pattern.sub(\"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _idaegu_co_kr(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 대구신문 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _idaegu_com(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 대구일보 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _daejonilbo(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 대전일보 \"\"\"\n",
        "        ## e.g. \"한국관광 100선에 선정된 궁남지 전경 사진=부여군 제공 [부여]\"\n",
        "        ## e.g. \"사진=OOO 의원 제공\"\n",
        "        ## It will not care s.t. have no \"제공\" in the end of reference.\n",
        "\n",
        "        # reference_pattern = re.compile(r\"(사진)\\s?\\=\\s?([가-힣]*\\s?)+(제공)\", re.MULTILINE)\n",
        "\n",
        "        # texts = \"\\n\".join(article_original)\n",
        "        # texts = reference_pattern.sub(\"\", texts)\n",
        "        # print(\"Done\")\n",
        "\n",
        "        # return texts.split(\"\\n\")\n",
        "\n",
        "        \"\"\" Skip -> cause error because of too many iteration steps... (too high complexity) \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _dynews(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 동양일보 \"\"\"\n",
        "        ## e.g. \"(동양일보 OOO 기자)\"\n",
        "        company_mark_pattern = re.compile(\"동양일보\")\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = company_mark_pattern.sub(\"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _dt(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 디지털타임스 \"\"\"\n",
        "        replaced_special_token_pattern = re.compile(r\"^\\s?◇\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = replaced_special_token_pattern.sub(\"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _mk(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 매일경제 \"\"\"\n",
        "        ## (e.g. id=\"354419753\")\n",
        "        advertisement_pattern = re.compile(r\"스탁론\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        ## If you can find advertisement documents...\n",
        "        if advertisement_pattern.findall(texts) != []:\n",
        "            ## Skip it.\n",
        "            return [\"\"]\n",
        "        \n",
        "        return article_original\n",
        "        \n",
        "\n",
        "    def _imaeil(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 매일신문 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _mt(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 머니투데이 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _honam(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 무등일보 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _busan(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 부산일보 \"\"\"\n",
        "        ## (e.g. id=\"350912775\")\n",
        "        ## Reporter's email will represented as \"OOO 기자 OOO@\" (ends with \"@\")\n",
        "        email_pattern = re.compile(r\"[\\S]*@$\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = email_pattern.sub(\"\", texts)\n",
        "\n",
        "        ## (e.g. id=\"359520459\")\n",
        "        ## Advertisements pattern.\n",
        "        advertisement_pattern_1 = re.compile(r\"^▶ 네이버에서 부산일보 구독하기 클릭!$\", re.MULTILINE)\n",
        "        advertisement_pattern_2 = re.compile(r\"^▶ 부산일보 구독하고 스타벅스 Get 하자!$\", re.MULTILINE)\n",
        "        advertisement_pattern_3 = re.compile(r\"^▶ 부산일보 홈 바로가기$\", re.MULTILINE)\n",
        "\n",
        "        texts = advertisement_pattern_1.sub(\"\", texts)\n",
        "        texts = advertisement_pattern_2.sub(\"\", texts)\n",
        "        texts = advertisement_pattern_3.sub(\"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _sjbnews(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 새전북신문 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _sedaily(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 서울경제 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _seoul(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 서울신문 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _asiae(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 아시아경제 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _ajunews(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 아주경제 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _yeongnam(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 영남일보 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _iusm(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 울산매일 \"\"\"\n",
        "        ## (e.g. id=\"340995643\")\n",
        "        tag_pattern = re.compile(r\"&lt;br[\\/]?&gt\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = tag_pattern.sub(\" \", texts)\n",
        "\n",
        "        ## (e.g. id=\"341038283\")\n",
        "        ref_pattern = re.compile(r\"^노컷뉴스$\", re.MULTILINE)\n",
        "        texts = ref_pattern.sub(\"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _edaily(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 이데일리 \"\"\"\n",
        "        ## (e.g. id=\"329454903\")\n",
        "        advertisement_patterns = [\n",
        "            r\"^네이버에서 이데일리 \\[구독하기▶\\]$\",\n",
        "            r\"^빡침해소! 청춘뉘우스~ \\[스냅타임▶\\]$\",\n",
        "            r\"^이데일리 채널 구독하면 \\[방탄소년단 실물영접 기회가▶\\]$\",\n",
        "            r\"^꿀잼가득 \\[영상보기▶\\] , 빡침해소!$\",\n",
        "            r\"^청춘뉘우스~ \\[스냅타임▶\\]$\"\n",
        "        ]\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        for pattern in advertisement_patterns:\n",
        "            texts = re.sub(pattern, \"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _incheonilbo(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 인천일보 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _jnilbo(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 전남일보 \"\"\"\n",
        "        repeated_word_patterns = [\n",
        "            r\"뉴시스\",\n",
        "            r\"편집에디터\",\n",
        "        ]\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        for pattern in repeated_word_patterns:\n",
        "            texts = re.sub(pattern, \"\", texts)\n",
        "\n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _jeollailbo(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 전라일보 \"\"\"\n",
        "        ## (e.g. id=\"329500477\")\n",
        "        email_pattern = re.compile(r\"[\\S]*@$\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = email_pattern.sub(\"\", texts)\n",
        "        \n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _domin(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 전북도민일보 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _jjan(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 전북일보 \"\"\"\n",
        "        repeated_word_patterns = re.compile(r\"^전북일보$\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = repeated_word_patterns.sub(\"\", texts)\n",
        "        \n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _jemin(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 제민일보 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _jejunews(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 제주일보 \"\"\"\n",
        "        repeated_word_patterns = re.compile(r\"^제주신보$\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = repeated_word_patterns.sub(\"\", texts)\n",
        "        \n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _joongdo(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 중도일보 \"\"\"\n",
        "        ## (e.g. id=\"350886753\")\n",
        "        email_pattern = re.compile(r\"[\\S]*@$\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = email_pattern.sub(\"\", texts)\n",
        "        \n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _jbnews(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 중부매일 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _joongboo(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 중부일보 \"\"\"\n",
        "        ## (e.g. id=\"330572743\")\n",
        "        repeated_word_patterns = re.compile(r\"^연합$\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = repeated_word_patterns.sub(\"\", texts)\n",
        "        \n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _inews365(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 충북일보 \"\"\"\n",
        "        ## (e.g. id=\"331218728\")\n",
        "        ## We don't care such like \"충북일보=충주]\" (id=\"333262510\")\n",
        "        repeated_word_patterns = re.compile(r\"^충북일보\\]\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = repeated_word_patterns.sub(\"\", texts)\n",
        "        \n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _ccdailynews(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 충청일보 \"\"\"\n",
        "        repeated_word_patterns = re.compile(r\"온라인충청일보\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = repeated_word_patterns.sub(\"\", texts)\n",
        "        \n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _cctoday(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 충청투데이 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _hankyung(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 한국경제 \"\"\"\n",
        "        ## (e.g. id=\"335987391\")\n",
        "        repeated_word_patterns = re.compile(r\"^\\]\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = repeated_word_patterns.sub(\"\", texts)\n",
        "        \n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def _ihalla(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 한라일보 \"\"\"\n",
        "        return article_original\n",
        "\n",
        "\n",
        "    def _hkbs(self, article_original: List[str]) -> List[str]:\n",
        "        \"\"\" 환경일보 \"\"\"\n",
        "        removed_special_token_pattern = re.compile(r\"^=\", re.MULTILINE)\n",
        "\n",
        "        texts = \"\\n\".join(article_original)\n",
        "        texts = removed_special_token_pattern.sub(\"\", texts)\n",
        "        \n",
        "        return texts.split(\"\\n\")\n",
        "\n",
        "\n",
        "    def __call__(self, article_original: List[str], media_name: str, minimize: bool = True) -> List[str]:\n",
        "        texts = article_original\n",
        "\n",
        "        ## We cannot be sure that the distribution of the training data domain set and\n",
        "        ## the validation & test data domain sets are the same.\n",
        "        ## We should be able to handle even the first seen \"media_name\". :(\n",
        "        if not minimize:\n",
        "            func = self.media_name_to_function.get(media_name)\n",
        "            if func != None:\n",
        "                texts = func(texts)\n",
        "\n",
        "        ## Apply common regex after customized cleaner.\n",
        "        texts = [i.strip() for i in texts if i.strip() != \"\"]\n",
        "        texts = CleanNewspaperArticleBase.apply(texts, minimize=minimize)\n",
        "        texts = [i.strip() for i in texts if i.strip() != \"\"]\n",
        "\n",
        "        return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2pnDbxVu-4T"
      },
      "outputs": [],
      "source": [
        "def extract_lines(documents: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
        "    ## We will save to jsonl files.\n",
        "    extracted_features = []\n",
        "    cleaner = CleanNewspaperArticle()\n",
        "\n",
        "    for document in tqdm(documents, total=len(documents)): \n",
        "        ## In train, valid dataset, we can get key \"text\".\n",
        "        if document.get(\"text\") != None:\n",
        "            id = document[\"id\"]\n",
        "\n",
        "            text = [line[\"sentence\"].replace(\"\\n\", \" \").strip() for line in itertools.chain(*document[\"text\"])]\n",
        "            if config.clean:\n",
        "                text = cleaner(text, document[\"media_name\"])\n",
        "            text = \" \".join(text)\n",
        "\n",
        "            summary = document[\"abstractive\"][0].replace(\"\\n\", \" \").strip()\n",
        "\n",
        "            ## If a document is total-advertisement, it can be skipped.\n",
        "            ## Like \"id\" == \"362852732\", no abstractive summaries exists.\n",
        "            if text == \"\" or summary == \"\":\n",
        "                continue\n",
        "\n",
        "            ## Wrong preprocessed article. (무등일보)\n",
        "            if id == \"338143341\":\n",
        "                continue\n",
        "\n",
        "            ## Gather all.\n",
        "            f = {\n",
        "                \"id\": id,\n",
        "                \"text\": text,\n",
        "                \"summary\": summary,\n",
        "            }\n",
        "\n",
        "        ## In test dataset, we can get key \"article_original\".\n",
        "        elif document.get(\"article_original\") != None:\n",
        "            id = document[\"id\"]\n",
        "\n",
        "            text = [line.replace(\"\\n\", \" \").strip() for line in document[\"article_original\"]]\n",
        "            ## assert document[\"media\"] in cleaner.media_name_to_function.keys()\n",
        "            if config.clean:\n",
        "                text = cleaner(text, document[\"media\"])\n",
        "            text = \" \".join(text)\n",
        "\n",
        "            if text == \"\":\n",
        "                raise AssertionError(f\"'text' must not be empty: id={id}\")\n",
        "\n",
        "            f = {\n",
        "                \"id\": id,\n",
        "                \"text\": text,\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            raise AssertionError(f\"Document must have 'text' or 'article_original' key: {document.keys()}\")\n",
        "\n",
        "        extracted_features.append(f)\n",
        "\n",
        "    return extracted_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJg0EzLLu-10",
        "outputId": "b6a4db81-1801-4490-8145-2e0e9061c7f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 243983/243983 [00:02<00:00, 120471.71it/s]\n",
            "100%|██████████| 30122/30122 [00:00<00:00, 128884.49it/s]\n"
          ]
        }
      ],
      "source": [
        "## Extract features.\n",
        "tr_documents = extract_lines(tr_corpus)\n",
        "vl_documents = extract_lines(vl_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqsrav5_iyTa",
        "outputId": "4bf2433e-db46-4af9-b4e4-e261d199698e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   'id': '250362727',\n",
            "    'summary': '당진 지역업체인 중앙개발과 GS건설의 하도급 분쟁이 심화되고 있는 가운데 안전 문제를 비롯하여 대기업과 하청업체 '\n",
            "               '간의 하도급 실태가 심각한 사회적 문제로 대두되고 있다.',\n",
            "    'text': '“GS 공사 참여…12억 원 손해” “하도급 문제 근본적 개선 필요하다” 당진 지역업체인 중앙개발이 최근 '\n",
            "            'GS건설로부터 4건의 공사를 하도급 받아 진행했지만 현재까지 총 12억 원 가량의 손해를 봐 도산 위기에 몰렸다고 '\n",
            "            '호소하고 있다. 중앙개발 이상학 대표에 따르면 GS EPS가 당진4호기 바이오매스 발전소 건설공사을 추진하면서 민원 '\n",
            "            '해결 및 지역업체 우선 사용 등의 이유로 관련 공사에 참여하게 됐다. 그러나 공사를 진행하던 중 바닷물이 올라와 이를 '\n",
            "            '수습하는 등 공사비가 당초 77억 원에서 105억 원까지 상승했음에도 불구하고 원청인 GS건설 측에서는 이에 합당한 '\n",
            "            '공사비를 주지 않았다고 주장하고 있다. 이 대표는 “GS건설 측이 보험 문제로 서명이 필요하다면서, 대표이사인 자신도 '\n",
            "            '모르게 부사장에게 날인을 받았다”며 “GS 측에서는 이를 정산에 날인한 것이라고 주장하고 있다”고 말했다. 이어 “이 '\n",
            "            '공사에서 4억9000만 원의 손해를 보게 돼 원청에 항의하니, 다른 공사 참여로 손해비용을 보전할 수 있게 해주겠다고 '\n",
            "            '해서 또 다른 공사를 시작했다”고 말했다. 그러나 두 번째 공사인 당진4호기 복합화력발전소 및 배수로 건설 공사에서도 '\n",
            "            '1억9000만 원의 적자를 봤고, 같은 이유로 안양2호기 건설 공사에도 참여했지만, 또다시 4억5800만 원의 적자를 '\n",
            "            '떠안았다. 이후 행정중심복합도시 자동크린넷 3-2차 시설 공사에 참여하던 중 1억 1000만 원의 적자가 발생하자 '\n",
            "            '결국 공사를 포기하고 타절했다. 이렇게 4번의 공사에서 중앙개발이 손해 본 금액만 총 12억 원에 달한다. 중앙개발은 '\n",
            "            '지난 2일부터 GS EPS 앞에서 하루 3시간씩 집회를 열고 이 문제를 알리고 있다. 이상학 대표는 “다른 사업에서 '\n",
            "            '번 돈으로 대부분의 인건비·자재비 등을 간신히 지급했지만, 손실이 너무 큰 데다, 다른 사업까지 영향을 미쳐 줄도산 '\n",
            "            '나게 생겼다”면서 “손실 금액이 너무 커 죽고 싶은 심정”이라고 한탄했다. 일각에서는 “대기업을 유치함으로써 고용창출 '\n",
            "            '등 경제적 효과를 강조하지만, 대기업의 횡포로 공사비를 제대로 받지 못해 지역업체가 도산한다면, 이게 과연 지역경제를 '\n",
            "            '활성화 하는 것이냐”고 비판했다. 이에 대해 GS 측의 입장을 듣고자 연락을 취했지만 연락이 닿지 않았다. 한편 '\n",
            "            '대기업과 하청업체 간의 하도급 실태가 심각한 사회적 문제로 떠오르고 있다. 대기업의 최저가 입찰 경쟁으로 '\n",
            "            '하청업체에서는 최대한 공사비를 줄여 이윤을 남겨야 하기 때문에 여러 가지 문제가 발생하고 있는 것이다. 가장 심각한 '\n",
            "            '것은 안전 문제다. 하청업체는 인건비를 줄이기 위해 최소한의 인력을 투입하는 한편, 공사기간 단축에 대한 압박으로 '\n",
            "            '노동자들에게 과중한 업무가 부여되기 때문이다. 지난해 11월 당진공장에서 발생한 현대제철 사망사고 역시 근본적인 '\n",
            "            '원인이 여기에 있다는 목소리가 일고 있다. 당시 현대제철에서 근무하던 하청업체 비정규직 노동자 A씨(37)는 원료를 '\n",
            "            '옮기는 통로(슈트)를 점검하던 중 철광석 분배 설비와 슈트 사이에 몸이 끼여 압사했다. 업계에서는 A씨가 혼자 '\n",
            "            '근무하지 않고 2인1조로 근무했으면 이 같은 참사는 벌어지지 않았을 것이라고 지적하고 있다. 또한 지난해 5월 서울 '\n",
            "            '지하철 2호선 구의역에서 스크린도어를 수리하던 용역업체 직원 B씨(20)씨가 열차와 스크린도어 사이에 끼어 숨졌던 '\n",
            "            '사고 역시 2인1조가 아닌 혼자 근무하면서 위험한 작업환경에 노출됐기 때문이라는 분석이 이어지기도 했다. 따라서 '\n",
            "            '하도급 문제를 근본적으로 개선해야 한다는 목소리가 지역사회에서도 계속해서 높아지고 있는 실정이다. 임아연 '\n",
            "            'zelkova87@hanmail.net'}\n"
          ]
        }
      ],
      "source": [
        "## Print samples.\n",
        "print_elements(tr_documents[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH02c40QneIm"
      },
      "source": [
        "### **Sample 10% of Total Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab에서 훈련을 진행하기에는 데이터 세트의 크기가 지나치게 크므로, 서두에서 언급한 것과 같이 10%만을 덜어서 사용합니다."
      ],
      "metadata": {
        "id": "i6OIwN1PM3Tl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-6Q_86BneE-"
      },
      "outputs": [],
      "source": [
        "## Get 10%.\n",
        "_, small_tr_documents = train_test_split(tr_documents, test_size=0.1, shuffle=True, random_state=config.seed)\n",
        "_, small_vl_documents = train_test_split(vl_documents, test_size=0.1, shuffle=True, random_state=config.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjydNY90neB6",
        "outputId": "996f1844-0f9e-44a7-9a83-82710acd33ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24398, 3013)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "len(small_tr_documents), len(small_vl_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljl4t6kFKeNw"
      },
      "source": [
        "### **Generate Test Dataset and Make Answersheet**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이콘 제출 형식과 유사하게 `sample_submission.tsv`를 만들고, 실제 정답이 들어있는 `answer.tsv` 또한 생성합니다."
      ],
      "metadata": {
        "id": "-cKaOXEY8012"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwAOYjOwL0Tn",
        "outputId": "115a42c4-e5a9-47da-c93e-466204757d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   'id': '330061292',\n",
            "    'summary': \"한국카카오은행(카카오뱅크)이 7일 발표한 내용에 따르면 지난해 12월 3일 출시한 '모임통장 서비스' 이용자가 \"\n",
            "               '출시 하루 만에 1만5000좌 그리고 한달 만에 100만명을 넘어섰고, 출시 이후 한달 간 하루 평균 신규 계좌 '\n",
            "               '개설수는 약 1만건에 달했고 신규 이용자는 하루 평균 3만명 이상을 기록하고 있다고 한다.',\n",
            "    'text': \"출시 한달 만에 한국카카오은행(카카오뱅크)은 '모임통장 서비스' 이용자가 출시 한달 만에 100만명을 넘어섰다고 7일 \"\n",
            "            '밝혔다. 6일 자정 기준 모임통장 이용자수는 100만300명이고, 모임통장 계좌는 33만2000좌가 개설됐다. '\n",
            "            '모임통장 서비스는 지난해 12월 3일 출시돼 하루 만에 1만5000좌의 모임통장 계좌가 개설되는 등 빠른 속도로 '\n",
            "            '가입자가 확대됐다. 출시 이후 한달 간 하루 평균 신규 계좌 개설수는 약 1만건에 달했고 신규 이용자는 하루 평균 '\n",
            "            '3만명 이상을 기록하고 있다. 모임통장 서비스는 동호회, 동아리 등 모임의 회비를 투명하고 편리하게 관리할 수 있어 '\n",
            "            '인기를 끌고 있다. 모임주는 모임통장에서 카카오톡의 친구 초대, 단체 카톡방 초대 기능을 통해 모임원을 간편하게 '\n",
            "            '초대할 수 있고 모임 멤버들은 회비 현황을 실시간으로 확인할 수 있다. 카카오뱅크의 자체 이용자 분석결과 계좌당 평균 '\n",
            "            '3.01명 이상이 회비 현황을 공유하고 있는 것으로 나타났다. 모임통장 계좌 개설 목적을 보면 친목과 생활비가 각각 '\n",
            "            '30%와 22%로 많았고 여행을 위한 목적도 20%에 달했다. 커플 통장으로 이용 중인 계좌는 16%였다. 모임통장 '\n",
            "            '이용자의 연령별 분포는 10대에서 50대까지 다양했다. 이 중 30대가 45%로 가장 높았고, 20대(29%), '\n",
            "            '40대(18%) 순으로 나타났다. 50대 이상 비율은 7%였다. 카카오뱅크의 계좌개설 가능 연령은 만 17세, '\n",
            "            '모임통장의 모임원으로 초대받을 수 있는 연령은 만 14세부터다. 카카오뱅크 관계자는 \"기존 은행 서비스에 커뮤니티와 '\n",
            "            '공유 기능을 결합했다\"며 \"고객 반응과 요청 사항을 수렴해 서비스를 업그레이드 해나가겠다\"고 말했다.'}\n"
          ]
        }
      ],
      "source": [
        "## Split validation dataset as valid & test. (not train dataset)\n",
        "small_vl_documents, small_ts_documents = train_test_split(small_vl_documents, test_size=0.1, shuffle=True, random_state=config.seed)\n",
        "\n",
        "## Sort it.\n",
        "small_ts_documents = sorted(small_ts_documents, key=itemgetter(\"id\"), reverse=False)\n",
        "\n",
        "## Make sample_submission.csv.\n",
        "sample_submission = pd.DataFrame(small_ts_documents).drop([\"text\"], axis=1)\n",
        "sample_submission.loc[:, \"summary\"] = sample_submission.loc[:, \"summary\"].apply(lambda _: \"\")\n",
        "sample_submission.to_csv(Path.cwd() / Path(config.sample_submission_path), index=False, sep=\"\\t\", encoding=\"utf-8\")\n",
        "\n",
        "## Make real answer.\n",
        "answer = pd.DataFrame(small_ts_documents).drop([\"text\"], axis=1)\n",
        "answer.to_csv(Path.cwd() / Path(config.answer_path), index=False, sep=\"\\t\", encoding=\"utf-8\")\n",
        "\n",
        "## Print samples.\n",
        "print_elements(small_ts_documents[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbsRpYhhL0Qt",
        "outputId": "21b94901-1855-4ace-e9ae-1d83356cd6b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "302\n"
          ]
        }
      ],
      "source": [
        "print(len(small_ts_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70yE7HaAJray"
      },
      "source": [
        "### **Export to TSV**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장 내에 콤마(,)가 없음을 보장하지 못하므로, `csv` 포맷 대신 `tsv` 포맷으로 주어진 훈련, 검증, 테스트 데이터 세트를 코랩 가상환경 내부에 저장합니다.\n",
        "\n",
        "wc 명령어를 사용하면, 아래와 같은 결과를 보여야 합니다.\n",
        "```bash\n",
        "$ wc -l ./data/*.tsv\n",
        "     303 ./data/test.tsv\n",
        "   24399 ./data/train.tsv\n",
        "    2712 ./data/valid.tsv\n",
        "   27414 total\n",
        "```"
      ],
      "metadata": {
        "id": "3DYApKbz9Bfo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yATIeSm7JrW_"
      },
      "outputs": [],
      "source": [
        "def save_lines(mode: str, documents: List[Dict[str, str]]) -> None:\n",
        "    assert mode in [\"train\", \"valid\", \"test\"]\n",
        "\n",
        "    ## Save as: ./data/train.tsv, ./data/valid.tsv, ./data/test.tsv.\n",
        "    fpath = Path.cwd() / Path(config.data, f\"{mode}.tsv\")\n",
        "    pd.DataFrame(documents).to_csv(fpath, index=False, sep=\"\\t\", encoding=\"utf-8\")\n",
        "\n",
        "    print(f\"File {fpath} saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFw-MhRGJrUM",
        "outputId": "be0c4827-0c26-4b8a-9653-949981ef4bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File /content/data/train.tsv saved.\n",
            "File /content/data/valid.tsv saved.\n",
            "File /content/data/test.tsv saved.\n",
            "CPU times: user 1.08 s, sys: 65.7 ms, total: 1.15 s\n",
            "Wall time: 1.14 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "## Save it.\n",
        "save_lines(\"train\", small_tr_documents) ## not tr_documents\n",
        "save_lines(\"valid\", small_vl_documents) ## not vl_documents\n",
        "save_lines(\"test\", small_ts_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x636527jJrRR",
        "outputId": "5ba0b155-3152-4dd2-aec5-4f8f128cb6a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     303 ./data/test.tsv\n",
            "   24399 ./data/train.tsv\n",
            "    2712 ./data/valid.tsv\n",
            "   27414 total\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "## Calculate lines.\n",
        "wc -l ./data/*.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nnt3x1oiwed"
      },
      "source": [
        "## **EDA**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "본 EDA의 주요 목적은 다음과 같습니다.\n",
        "\n",
        "* 훈련 및 추론용 하이퍼파라미터의 설정 근거 마련\n",
        "* 모델링 과정에서의 주요 인사이트 획득\n",
        "* 텍스트 정제 코드 구현\n",
        "\n",
        "텍스트 정제 과정은 아래 내용만으로는 확인하기 어렵고, 저희 팀의 경우 `media_name` 별로 데이터 세트를 분할하여 각각을 VSCode로 확인하는 과정을 수행했습니다."
      ],
      "metadata": {
        "id": "YT0zZaeC9gDo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV748KI2Tbi4"
      },
      "source": [
        "### **Get Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdyR3t_LTdoZ"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.PreTrainedTokenizerFast.from_pretrained(config.pretrained_model_name)\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD1byViUTdlu"
      },
      "source": [
        "### **Encode and Calculate Lines**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전체 훈련용 데이터 세트에 대해 토크나이징을 진행한 뒤, 입출력간 상관관계를 확인하는 절차를 수행함으로써 `max_inp_len` 및 `max_tar_len`의 적절한 값을 설정합니다. 본 과정은 데이콘 대회 규칙 및 AI Hub의 구축 가이드라인에 명시되어 있는, `전체 문장의 10% 내외로 요약문을 구축하였음`을 검증하는 과정이기도 합니다.\n",
        "\n",
        "이미지를 확인해 보았을 때, 데이콘의 주장과는 다르게 도메인이 직선 y=0.1x 에 적합(fitting)하지는 않는 것으로 보이며, `max_inp_len`=1024, `max_tar_len`=256으로 설정하는데에 무리가 없다고 판단할 수 있습니다."
      ],
      "metadata": {
        "id": "2Oi8hch9-ENs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFSMAk_PTMxQ",
        "outputId": "071e58fb-1992-4905-8c84-c6df6e113fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 243978/243978 [04:14<00:00, 958.22it/s]\n"
          ]
        }
      ],
      "source": [
        "texts = [document[\"text\"] for document in tr_documents]\n",
        "summaries = [document[\"summary\"] for document in tr_documents]\n",
        "\n",
        "len_text = []\n",
        "len_summary = []\n",
        "\n",
        "for text, summary in tqdm(zip(texts, summaries), total=len(texts)):\n",
        "    len_text.append(len(tokenizer.encode(text)))\n",
        "    len_summary.append(len(tokenizer.encode(summary)))\n",
        "\n",
        "len_text = np.array(len_text)\n",
        "len_summary = np.array(len_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "Fmb_shvsiwbm",
        "outputId": "16f9bc6e-f8e1-416c-d4a5-5076965c40ba"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Show relations.\n",
        "plt.rcParams[\"font.family\"] = \"serif\"\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 4), facecolor=\"white\", dpi=72)\n",
        "\n",
        "x = len_text\n",
        "y = len_summary\n",
        "\n",
        "x_bins = np.linspace(min(x), max(x), 140)\n",
        "y_bins = np.linspace(min(y), max(y), 50)\n",
        "\n",
        "plt.hist2d(x, y, bins=[x_bins, y_bins], cmap=\"Blues\", norm=matplotlib.colors.LogNorm())\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label(\"Counts\", fontsize=14)\n",
        "\n",
        "## y = 0.1 * x\n",
        "foo = np.linspace(0, max(x), 1000)\n",
        "bar = foo * 0.1\n",
        "plt.plot(foo, bar, lw=4, c=\"firebrick\", alpha=.3)\n",
        "\n",
        "plt.xlim([0, 1400])\n",
        "plt.ylim([0, 330])\n",
        "plt.xlabel(\"Length of Text\", fontsize=14)\n",
        "plt.ylabel(\"Length of Summary\", fontsize=14)\n",
        "\n",
        "ax.tick_params(axis=\"both\", direction=\"in\")\n",
        "\n",
        "plt.grid(True, lw=0.15)\n",
        "\n",
        "plt.tight_layout()\n",
        "# plt.savefig(\"2.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8_pjLqTXYyh"
      },
      "source": [
        "## **Make Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZNx0fSZZ0eh"
      },
      "source": [
        "### **Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu6zZ5fbZ7ty"
      },
      "outputs": [],
      "source": [
        "# del tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR7aD7HXZ0Zo",
        "outputId": "440301a1-7cb4-4097-ac82-468a33000fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   '_additional_special_tokens': [],\n",
            "    '_bos_token': '<s>',\n",
            "    '_cls_token': None,\n",
            "    '_decode_use_source_tokenizer': False,\n",
            "    '_eos_token': '</s>',\n",
            "    '_mask_token': '<mask>',\n",
            "    '_pad_token': '<pad>',\n",
            "    '_pad_token_type_id': 0,\n",
            "    '_sep_token': None,\n",
            "    '_tokenizer': <tokenizers.Tokenizer object at 0x5607a794d680>,\n",
            "    '_unk_token': '<unk>',\n",
            "    'deprecation_warnings': {},\n",
            "    'init_inputs': (),\n",
            "    'init_kwargs': {   'name_or_path': 'gogamza/kobart-base-v1',\n",
            "                       'special_tokens_map_file': '/root/.cache/huggingface/transformers/3e6abf40f4fadbea9e7b539c182868d979838d8f7e6cdcdf2ed52ddcf01420c0.15447ae63ad4a2eba8bc7a5146360711dc32b315b4f1488b4806debf35315e9a'},\n",
            "    'model_input_names': ['input_ids', 'token_type_ids', 'attention_mask'],\n",
            "    'model_max_length': 1000000000000000019884624838656,\n",
            "    'name_or_path': 'gogamza/kobart-base-v1',\n",
            "    'padding_side': 'right',\n",
            "    'verbose': True}\n"
          ]
        }
      ],
      "source": [
        "## Get pretrained tokenizer.\n",
        "tokenizer = transformers.PreTrainedTokenizerFast.from_pretrained(config.pretrained_model_name)\n",
        "\n",
        "## Print elements.\n",
        "print_elements(vars(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VljcCivEZ2P7"
      },
      "source": [
        "### **Dataset and Collater**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`TextAbstractSummarizationDataset`에서는 훈련 전 모든 토크나이징을 미리 진행하게 됩니다. 이때 토크나이징 된 입력의 길이를 내림차순으로 정렬함으로써 배치 단위로 비슷한 길이의 입력끼리 묶일 수 있도록 할 뿐만 아니라 훈련 초기에 Out-Of-Memory (OOM)을 검출할 수 있도록 유도하였습니다.\n",
        "\n",
        "`TextAbstractSummarizationCollator`에서는 가변길이의 배치 단위의 최대 길이로 패딩을 적용함으로써 배치 단위의 병렬 연산이 가능하게끔 변환합니다. 고정된 길이(1024 등)가 아닌 배치 단위의 최대 길이로 패딩을 적용함으로써, 훈련 및 검증 속도를 향상시킬 수 있었습니다.\n"
      ],
      "metadata": {
        "id": "8uqBeuYhAPFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HuggingFace PLM을 이용한 훈련 과정을 처음 접했다면 가장 혼란을 겪으실 부분이 데이터 세트 구축 간 입출력 인자를 맞추는 일이라고 생각합니다. `transformers.BartForConditionalGeneration.forward()`의 인자는 [HuggingFace Document](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartForConditionalGeneration)에서 확인하실 수 있습니다."
      ],
      "metadata": {
        "id": "znSa_j-pEj0t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Soug8gDX40o"
      },
      "outputs": [],
      "source": [
        "def read_tsv(fpath: pathlib.PosixPath) -> pd.DataFrame:\n",
        "    return pd.read_csv(fpath, index_col=False, sep=\"\\t\", encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYsRRDCpXYvQ"
      },
      "outputs": [],
      "source": [
        "class TextAbstractSummarizationDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        fpath: pathlib.PosixPath,\n",
        "        mode: str = \"train\",\n",
        "    ):\n",
        "        super(TextAbstractSummarizationDataset, self).__init__()\n",
        "\n",
        "        self.df = read_tsv(fpath)\n",
        "        # self.tok = tokenizer -> don't keep\n",
        "        \n",
        "        ## Mode.\n",
        "        assert mode in [\"train\", \"test\"]\n",
        "        self.mode = mode\n",
        "\n",
        "        ## Apply tokenize first to speed up in training phase and make code more simply.\n",
        "        tqdm.pandas(desc=\"Tokenizing input texts\")\n",
        "        self.df.loc[:, \"text_tok\"] = self.df.loc[:, \"text\"].progress_apply(lambda x: tokenizer.encode(x))\n",
        "        self.df.loc[:, \"text_tok_len\"] = self.df.loc[:, \"text_tok\"].apply(lambda x: len(x))\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            tqdm.pandas(desc=\"Tokenizing target summaries\")\n",
        "            self.df.loc[:, \"summary_tok\"] = self.df.loc[:, \"summary\"].progress_apply(lambda x: tokenizer.encode(x))\n",
        "            self.df.loc[:, \"summary_tok_len\"] = self.df.loc[:, \"summary_tok\"].apply(lambda x: len(x))\n",
        "\n",
        "        ## Sort by tokenized length with tqdm progress bar.\n",
        "        ## \n",
        "        ## By sorting sequentially, starting with the longest sentence, \n",
        "        ## we can determine the maximum VRAM size the model is using for\n",
        "        ## training. That is, if OOM does not occur for the maximum VRAM\n",
        "        ## size at the beginning of training, it is guaranteed that OOM\n",
        "        ## does not occur during training.\n",
        "        self.df.sort_values(by=[\"text_tok_len\"], axis=0, ascending=False, inplace=True)\n",
        "\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return self.df.shape[0]\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, List[int]]:\n",
        "        instance = self.df.iloc[idx]\n",
        "\n",
        "        return_value = {\n",
        "            \"id\": instance[\"id\"], ## for sorting in inference mode\n",
        "            \"text\": instance[\"text_tok\"],\n",
        "            \"length\": len(instance[\"text_tok\"]),\n",
        "        }\n",
        "        if self.mode == \"train\":\n",
        "            return_value[\"summary\"] = instance[\"summary_tok\"]\n",
        "        \n",
        "        return return_value\n",
        "\n",
        "\n",
        "class TextAbstractSummarizationCollator():\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        bos_token_id: int,\n",
        "        eos_token_id: int,\n",
        "        pad_token_id: int,\n",
        "        inp_max_len: int = 1024,\n",
        "        tar_max_len: int = 256,\n",
        "        ignore_index: int = -100,\n",
        "        mode: str = \"train\",\n",
        "    ):\n",
        "        super(TextAbstractSummarizationCollator, self).__init__()\n",
        "\n",
        "        self.bos_token_id = bos_token_id\n",
        "        self.eos_token_id = eos_token_id\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.inp_max_len = inp_max_len\n",
        "        self.tar_max_len = tar_max_len\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "        ## Mode.\n",
        "        assert mode in [\"train\", \"test\"]\n",
        "        self.mode = mode\n",
        "\n",
        "\n",
        "    def _pad(self, sentences: List[List[int]], token_id: int) -> np.ndarray:\n",
        "        ## We will pad as max length per batch, not \"inp_max_len(=1024, etc)\".\n",
        "        max_length_per_batch = max([len(i) for i in sentences])\n",
        "\n",
        "        ## Stack as dimension 0 (batch dimension).\n",
        "        ## \"token_id\" can be \"tokenizer.pad_token_id(=3)\" or \"ignore_index(=-100)\"\n",
        "        return np.stack([i + [token_id] * (max_length_per_batch - len(i)) for i in sentences], axis=0)\n",
        "\n",
        "\n",
        "    def _train_collator(self, samples: List[Dict[str, List[int]]]) -> Dict[str, List[int]]:\n",
        "        ## Unpack.\n",
        "\n",
        "        ## If input max length > 1024, you can see below error:\n",
        "        ##   1) Assertion `srcIndex < srcSelectDimSize` failed\n",
        "        ##   2) Device-side assert triggered\n",
        "        tokenized_texts     = [s[\"text\"][:self.inp_max_len]        for s in samples]\n",
        "        tokenized_summaries = [s[\"summary\"][:self.tar_max_len - 1] for s in samples] ## <bos> or <eos> token index\n",
        "\n",
        "        ## Inputs for encoder.\n",
        "        input_ids = self._pad(tokenized_texts, token_id=self.pad_token_id)  ## numpy format\n",
        "        attention_mask = (input_ids != self.pad_token_id).astype(float)     ## numpy format\n",
        "\n",
        "        ## Inputs for decoder (generator).\n",
        "        decoder_input_ids = [[self.bos_token_id] + i for i in tokenized_summaries]      ## bos\n",
        "        decoder_input_ids = self._pad(decoder_input_ids, token_id=self.pad_token_id)    ## eos\n",
        "        decoder_attention_mask = (decoder_input_ids != self.pad_token_id).astype(float)\n",
        "\n",
        "        ## Answer.\n",
        "        labels = [i + [self.eos_token_id] for i in tokenized_summaries]\n",
        "        labels = self._pad(labels, token_id=self.ignore_index) ## why != \"padding_id\" ???\n",
        "\n",
        "        ## We ensure that generator's inputs' and outputs' shapes are equal.\n",
        "        assert decoder_input_ids.shape == labels.shape\n",
        "        \n",
        "        ## Pack as pre-defined arguments. See:\n",
        "        ##   https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartForConditionalGeneration\n",
        "        return {\n",
        "            \"input_ids\":                torch.from_numpy(input_ids),\n",
        "            \"attention_mask\":           torch.from_numpy(attention_mask),\n",
        "            \"decoder_input_ids\":        torch.from_numpy(decoder_input_ids),\n",
        "            \"decoder_attention_mask\":   torch.from_numpy(decoder_attention_mask),\n",
        "            \"labels\":                   torch.from_numpy(labels),\n",
        "        }\n",
        "\n",
        "\n",
        "    def _test_collator(self, samples: List[Dict[str, List[int]]]) -> Dict[str, List[int]]:\n",
        "        ## Unpack.\n",
        "        ids              = [s[\"id\"]                      for s in samples]\n",
        "        tokenized_texts  = [s[\"text\"][:self.inp_max_len] for s in samples]   ## no <bos> token included\n",
        "\n",
        "        ## Inputs for encoder.\n",
        "        input_ids = self._pad(tokenized_texts, token_id=self.pad_token_id)  ## numpy format\n",
        "        attention_mask = (input_ids != self.pad_token_id).astype(float)     ## numpy format\n",
        "\n",
        "        ## Pack as pre-defined arguments:\n",
        "        ## See: https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartForConditionalGeneration\n",
        "        return {\n",
        "            \"input_ids\":        torch.from_numpy(input_ids),\n",
        "            \"attention_mask\":   torch.from_numpy(attention_mask),\n",
        "            ## Additional information to make answer.\n",
        "            \"id\":               ids,\n",
        "        }\n",
        "\n",
        "\n",
        "    def __call__(self, samples: List[Dict[str, List[int]]]) -> Dict[str, List[int]]:\n",
        "        return self._train_collator(samples) if self.mode == \"train\" else self._test_collator(samples)\n",
        "\n",
        "\n",
        "def get_datasets(tokenizer, fpath: pathlib.PosixPath, mode: str = \"train\"):\n",
        "    return TextAbstractSummarizationDataset(tokenizer, fpath, mode=mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Iqg9JYsXYsu",
        "outputId": "bc445c31-b405-46eb-a88f-acfc835313f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing input texts: 100%|██████████| 24398/24398 [00:21<00:00, 1125.52it/s]\n",
            "Tokenizing target summaries: 100%|██████████| 24398/24398 [00:03<00:00, 6221.16it/s]\n",
            "Tokenizing input texts: 100%|██████████| 2711/2711 [00:02<00:00, 1076.27it/s]\n",
            "Tokenizing target summaries: 100%|██████████| 2711/2711 [00:00<00:00, 6060.51it/s]\n"
          ]
        }
      ],
      "source": [
        "## Get datasets and index to label map.\n",
        "tr_ds = get_datasets(tokenizer, fpath=Path(config.train))\n",
        "vl_ds = get_datasets(tokenizer, fpath=Path(config.valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnoUicBEeZ48",
        "outputId": "3b8fe4fc-7728-40b5-c1cc-2ec45b78136e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24398, 2711)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "len(tr_ds), len(vl_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TezptaccDGA"
      },
      "source": [
        "### **Pretrained Language Model**\n",
        "\n",
        "PLM으로는 HuggingFace에 게시된 [gogamza/kobart-base-v1](https://huggingface.co/gogamza/kobart-base-v1)을 사용합니다.\n",
        "\n",
        "기타 추천하는 모델들은 다음과 같습니다.\n",
        "  - gogamza/kobart-base-v1\n",
        "  - gogamza/kobart-base-v2\n",
        "  - gogamza/kobart-summarization\n",
        "  - skt/kobert-base-v1\n",
        "  - ainize/kobart-news\n",
        "  - hyunwoongko/kobart\n",
        "\n",
        "아주 간단한 BART 기반의 모델만을 사용했는데, 문서 요약을 목적으로 고안된 구조인 [PEGASUS](https://arxiv.org/abs/1912.08777), [BERTSumExtAbs](https://arxiv.org/abs/1908.08345) 등을 사용한다면 더 높은 성능을 기대할 수 있을 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LURewSFEXYp2"
      },
      "outputs": [],
      "source": [
        "model = transformers.BartForConditionalGeneration.from_pretrained(config.pretrained_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwRp74NIfH4B"
      },
      "source": [
        "### **Train Arguments**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련을 위한 Engine으로는 HuggingFace의 Trainer를 사용합니다."
      ],
      "metadata": {
        "id": "CaUNj2iuCn3r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLRZfgs4XYnR",
        "outputId": "555b26ea-920f-4ac9-9b07-707d7390a462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   '__cached__setup_devices': device(type='cuda', index=0),\n",
            "    '_n_gpu': 1,\n",
            "    'adafactor': False,\n",
            "    'adam_beta1': 0.9,\n",
            "    'adam_beta2': 0.999,\n",
            "    'adam_epsilon': 1e-08,\n",
            "    'bf16': False,\n",
            "    'bf16_full_eval': False,\n",
            "    'dataloader_drop_last': False,\n",
            "    'dataloader_num_workers': 4,\n",
            "    'dataloader_pin_memory': True,\n",
            "    'ddp_bucket_cap_mb': None,\n",
            "    'ddp_find_unused_parameters': None,\n",
            "    'debug': [],\n",
            "    'deepspeed': None,\n",
            "    'disable_tqdm': False,\n",
            "    'do_eval': True,\n",
            "    'do_predict': False,\n",
            "    'do_train': False,\n",
            "    'eval_accumulation_steps': None,\n",
            "    'eval_steps': None,\n",
            "    'evaluation_strategy': <IntervalStrategy.EPOCH: 'epoch'>,\n",
            "    'fp16': True,\n",
            "    'fp16_backend': 'auto',\n",
            "    'fp16_full_eval': False,\n",
            "    'fp16_opt_level': 'O1',\n",
            "    'generation_max_length': None,\n",
            "    'generation_num_beams': None,\n",
            "    'gradient_accumulation_steps': 16,\n",
            "    'gradient_checkpointing': False,\n",
            "    'greater_is_better': False,\n",
            "    'group_by_length': False,\n",
            "    'half_precision_backend': 'auto',\n",
            "    'hub_model_id': None,\n",
            "    'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>,\n",
            "    'hub_token': None,\n",
            "    'ignore_data_skip': False,\n",
            "    'label_names': None,\n",
            "    'label_smoothing_factor': 0.0,\n",
            "    'learning_rate': 5e-05,\n",
            "    'length_column_name': 'length',\n",
            "    'load_best_model_at_end': True,\n",
            "    'local_rank': -1,\n",
            "    'log_level': -1,\n",
            "    'log_level_replica': -1,\n",
            "    'log_on_each_node': True,\n",
            "    'logging_dir': 'logs/20211227-042519/run',\n",
            "    'logging_first_step': False,\n",
            "    'logging_nan_inf_filter': True,\n",
            "    'logging_steps': 10,\n",
            "    'logging_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
            "    'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>,\n",
            "    'max_grad_norm': 1.0,\n",
            "    'max_steps': -1,\n",
            "    'metric_for_best_model': 'loss',\n",
            "    'mp_parameters': '',\n",
            "    'no_cuda': False,\n",
            "    'num_train_epochs': 10,\n",
            "    'output_dir': 'ckpt/20211227-042519',\n",
            "    'overwrite_output_dir': False,\n",
            "    'past_index': -1,\n",
            "    'per_device_eval_batch_size': 8,\n",
            "    'per_device_train_batch_size': 8,\n",
            "    'per_gpu_eval_batch_size': None,\n",
            "    'per_gpu_train_batch_size': None,\n",
            "    'predict_with_generate': False,\n",
            "    'prediction_loss_only': False,\n",
            "    'push_to_hub': False,\n",
            "    'push_to_hub_model_id': None,\n",
            "    'push_to_hub_organization': None,\n",
            "    'push_to_hub_token': None,\n",
            "    'remove_unused_columns': True,\n",
            "    'report_to': ['tensorboard'],\n",
            "    'resume_from_checkpoint': None,\n",
            "    'run_name': 'ckpt/20211227-042519',\n",
            "    'save_on_each_node': False,\n",
            "    'save_steps': 500,\n",
            "    'save_strategy': <IntervalStrategy.EPOCH: 'epoch'>,\n",
            "    'save_total_limit': None,\n",
            "    'seed': 42,\n",
            "    'sharded_ddp': [],\n",
            "    'skip_memory_metrics': True,\n",
            "    'sortish_sampler': False,\n",
            "    'tf32': None,\n",
            "    'tpu_metrics_debug': False,\n",
            "    'tpu_num_cores': None,\n",
            "    'use_legacy_prediction_loop': False,\n",
            "    'warmup_ratio': 0.2,\n",
            "    'warmup_steps': 0,\n",
            "    'weight_decay': 0.01,\n",
            "    'xpu_backend': None}\n"
          ]
        }
      ],
      "source": [
        "## Path arguments.\n",
        "nowtime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "output_dir = Path(config.ckpt, nowtime)\n",
        "logging_dir = Path(config.logs, nowtime, \"run\")\n",
        "\n",
        "training_args = transformers.Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=config.per_replica_batch_size,\n",
        "    per_device_eval_batch_size=config.per_replica_batch_size,\n",
        "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "    learning_rate=config.lr,\n",
        "    weight_decay=config.weight_decay,\n",
        "    warmup_ratio=config.warmup_ratio,\n",
        "    num_train_epochs=config.n_epochs,\n",
        "    logging_dir=logging_dir,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    # save_steps=1000,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=4,\n",
        "    disable_tqdm=False,\n",
        "    load_best_model_at_end=True,\n",
        "    ## As below, only Seq2SeqTrainingArguments' arguments.\n",
        "    # sortish_sampler=True,\n",
        "    # predict_with_generate=True,\n",
        "    # generation_max_length=config.tar_max_len,   ## 256\n",
        "    # generation_num_beams=config.beam_size,      ## 5\n",
        ")\n",
        "\n",
        "## Print elements.\n",
        "print_elements(vars(training_args))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLij2k1gfStQ"
      },
      "source": [
        "### **HuggingFace Trainer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgR4chYOXYkc",
        "outputId": "eb057a4f-3e4c-4fcf-ef45-9ea86cbef107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "## Define trainer.\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=TextAbstractSummarizationCollator(\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        inp_max_len=config.inp_max_len,\n",
        "        tar_max_len=config.tar_max_len,\n",
        "    ),\n",
        "    train_dataset=tr_ds,\n",
        "    eval_dataset=vl_ds,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h362cbfrfb36"
      },
      "source": [
        "## **Train**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련을 진행하는 것은 매우 간단합니다.\n",
        "\n",
        "HuggingFace의 Trainer는 시각화 부분이 다소 약하다는 의견이 있는데, 저도 그 의견에 어느 정도 동의합니다. 좀 더 괜찮은 TQDM Progress Bar를 사용했더라면 좋을 것 같아 커스터마이징 할 수 있는지 좀 찾아봤는데, 그러한 내용이 지원되지 않아 조금 아쉬웠습니다."
      ],
      "metadata": {
        "id": "fYNt2KwWCtnE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2DADv8pCfdeh",
        "outputId": "d0ad5373-4746-469f-ff94-d43f0c595a46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 24398\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 1900\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1900' max='1900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1900/1900 3:48:28, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.559200</td>\n",
              "      <td>1.543973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.417500</td>\n",
              "      <td>1.446895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.298000</td>\n",
              "      <td>1.415649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.181800</td>\n",
              "      <td>1.419801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.069600</td>\n",
              "      <td>1.439469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.025500</td>\n",
              "      <td>1.447175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.940600</td>\n",
              "      <td>1.480663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.877700</td>\n",
              "      <td>1.495439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.845500</td>\n",
              "      <td>1.507249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.816700</td>\n",
              "      <td>1.520043</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 2711\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ckpt/20211227-042519/checkpoint-190\n",
            "Configuration saved in ckpt/20211227-042519/checkpoint-190/config.json\n",
            "Model weights saved in ckpt/20211227-042519/checkpoint-190/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2711\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ckpt/20211227-042519/checkpoint-380\n",
            "Configuration saved in ckpt/20211227-042519/checkpoint-380/config.json\n",
            "Model weights saved in ckpt/20211227-042519/checkpoint-380/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2711\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ckpt/20211227-042519/checkpoint-570\n",
            "Configuration saved in ckpt/20211227-042519/checkpoint-570/config.json\n",
            "Model weights saved in ckpt/20211227-042519/checkpoint-570/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2711\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ckpt/20211227-042519/checkpoint-760\n",
            "Configuration saved in ckpt/20211227-042519/checkpoint-760/config.json\n",
            "Model weights saved in ckpt/20211227-042519/checkpoint-760/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2711\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ckpt/20211227-042519/checkpoint-950\n",
            "Configuration saved in ckpt/20211227-042519/checkpoint-950/config.json\n",
            "Model weights saved in ckpt/20211227-042519/checkpoint-950/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2711\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ckpt/20211227-042519/checkpoint-1140\n",
            "Configuration saved in ckpt/20211227-042519/checkpoint-1140/config.json\n",
            "Model weights saved in ckpt/20211227-042519/checkpoint-1140/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2711\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ckpt/20211227-042519/checkpoint-1330\n",
            "Configuration saved in ckpt/20211227-042519/checkpoint-1330/config.json\n",
            "Model weights saved in ckpt/20211227-042519/checkpoint-1330/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2711\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ckpt/20211227-042519/checkpoint-1520\n",
            "Configuration saved in ckpt/20211227-042519/checkpoint-1520/config.json\n",
            "Model weights saved in ckpt/20211227-042519/checkpoint-1520/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2711\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ckpt/20211227-042519/checkpoint-1710\n",
            "Configuration saved in ckpt/20211227-042519/checkpoint-1710/config.json\n",
            "Model weights saved in ckpt/20211227-042519/checkpoint-1710/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2711\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ckpt/20211227-042519/checkpoint-1900\n",
            "Configuration saved in ckpt/20211227-042519/checkpoint-1900/config.json\n",
            "Model weights saved in ckpt/20211227-042519/checkpoint-1900/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ckpt/20211227-042519/checkpoint-570 (score: 1.415649175643921).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1900, training_loss=1.213880800196999, metrics={'train_runtime': 13715.9503, 'train_samples_per_second': 17.788, 'train_steps_per_second': 0.139, 'total_flos': 9.382096101740544e+16, 'train_loss': 1.213880800196999, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "## Just train.\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YdeNnB3t-8Z"
      },
      "source": [
        "### **Visualize to TensorBoard**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyOIQhnpt-5d"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk4AOxYjueGD"
      },
      "source": [
        "### **(Optional) Upload Logs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1bnAUW6ueJj"
      },
      "outputs": [],
      "source": [
        "# %%bash\n",
        "# tensorboard dev upload --logdir ./logs \\\n",
        "#     --name {SOME_MODEL_NAME} \\\n",
        "#     --description {SOME_MODEL_TITLE} \\\n",
        "#     --one_shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1LgCYYQt9pE"
      },
      "source": [
        "### **Save Best Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞서 `'load_best_model_at_end': True`로 설정했으므로 모델에는 latest weight가 저장되어 있을 것이고, 이를 `torch.save()`를 사용하여 별도로 google drive에 저장 및 호출합니다. 물론 Trainer에서 저장한 체크포인트 `*.bin`을 사용할 수도 있습니다."
      ],
      "metadata": {
        "id": "iXJASrmvDSNI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za5bcaqvfdPe"
      },
      "outputs": [],
      "source": [
        "## Save the best model to your gdrive.\n",
        "torch.save({\n",
        "    \"model\": trainer.model.state_dict(),\n",
        "    \"config\": config,\n",
        "    \"tokenizer\": tokenizer,\n",
        "}, Path(config.model_fpath))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lo-7YpVqjuD"
      },
      "source": [
        "## **Inference**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Latest Model**"
      ],
      "metadata": {
        "id": "3Ulzn6WuU8DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_data = torch.load(\n",
        "    config.model_fpath,\n",
        "    map_location=\"cpu\" if config.gpu_id < 0 else \"cuda:%d\" % config.gpu_id,\n",
        ")"
      ],
      "metadata": {
        "id": "DUzzjN2cU-QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Parse.\n",
        "bart_best = saved_data[\"model\"]\n",
        "train_config = saved_data[\"config\"]\n",
        "tokenizer = transformers.PreTrainedTokenizerFast.from_pretrained(train_config.pretrained_model_name)\n",
        "\n",
        "## Load weights.\n",
        "model = transformers.BartForConditionalGeneration.from_pretrained(train_config.pretrained_model_name)\n",
        "model.load_state_dict(bart_best)"
      ],
      "metadata": {
        "id": "UtXxxSARU7_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-Hh3Bgaqjqi"
      },
      "source": [
        "### **Get Test Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUbtCGELrq_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4eea107-0877-45a9-f6b8-88f10e61cc26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing input texts: 100%|██████████| 302/302 [00:00<00:00, 1002.83it/s]\n"
          ]
        }
      ],
      "source": [
        "ts_ds = get_datasets(tokenizer, fpath=Path(config.test), mode=\"test\")\n",
        "\n",
        "## We will use pytorch dataloader, not huggingface trainer.\n",
        "ts_loader = torch.utils.data.DataLoader(\n",
        "    ts_ds,\n",
        "    batch_size=config.per_replica_batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=TextAbstractSummarizationCollator(\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        inp_max_len=config.inp_max_len,\n",
        "        tar_max_len=config.tar_max_len,\n",
        "        mode=\"test\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_R9ybbkrq70"
      },
      "source": [
        "### **Generate Summaries**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "추론 과정은 `torch.no_grad()`를 사용하여 직접 raw level로 구현합니다.\n",
        "\n",
        "훈련 과정과 마찬가지로 문서를 길이별로 정렬하여 추론한 뒤, `id`별로 재정렬하여 제출 포멧을 맞춥니다.\n",
        "\n",
        "HuggingFace 모델에서의 추론은 `transformers.BartForConditionalGeneration.generate()`를 사용하여 진행되며, 구체적인 인자의 값은 [HuggingFace Document](https://huggingface.co/docs/transformers/v4.15.0/en/main_classes/model#transformers.generation_utils.GenerationMixin.generate)를 참조하시면 될 것 같습니다. 실제로 Beam Search, Trigram Blocking 등과 같은 내용들은 모두 구현되어 있습니다."
      ],
      "metadata": {
        "id": "15kukOkbEB2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hQPDVZqrq41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0432701-5770-4e7d-9ef0-82a816518900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 38/38 [01:12<00:00,  1.90s/it]\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    if config.gpu_id >= 0:\n",
        "        model.cuda(config.gpu_id)\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    ## Don't forget turn-on evaluation mode.\n",
        "    model.eval()\n",
        "\n",
        "    outputs = []\n",
        "    for mini_batch in tqdm(ts_loader, total=len(ts_loader)):\n",
        "        id = mini_batch[\"id\"]\n",
        "        input_ids = mini_batch[\"input_ids\"]\n",
        "        attention_mask = mini_batch[\"attention_mask\"]\n",
        "\n",
        "        if config.var_len:\n",
        "            ## Variable min, max length of target summaries.\n",
        "            ## We know that summaries ~= text * 0.1.\n",
        "            avg_len = int(input_ids.ne(tokenizer.pad_token_id).view(-1).sum() / input_ids.size(0))\n",
        "            min_length = max(64,  int(avg_len * 0.05))\n",
        "            max_length = min(256, int(avg_len * 0.15))\n",
        "            ## And we don't need to set length penalty anymore.\n",
        "            config.length_penalty = 1.0\n",
        "        else:\n",
        "            min_length = config.tar_max_len // 4 ## maybe, less then 64 (e.g. 48 or 32) can be more score..\n",
        "            max_length = config.tar_max_len\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        ## Generate ids of summaries.\n",
        "        output = model.generate(\n",
        "            input_ids, \n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,                  ## maximum summarization size\n",
        "            min_length=min_length,                  ## minimum summarization size\n",
        "            early_stopping=True,                    ## stop the beam search when at least 'num_beams' sentences are finished per batch\n",
        "            num_beams=config.beam_size,             ## beam search size\n",
        "            bos_token_id=tokenizer.bos_token_id,    ## <s> = 0\n",
        "            eos_token_id=tokenizer.eos_token_id,    ## <\\s> = 1\n",
        "            pad_token_id=tokenizer.pad_token_id,    ## 3\n",
        "            length_penalty=config.length_penalty,   ## value > 1.0 in order to encourage the model to produce longer sequences\n",
        "            no_repeat_ngram_size=config.no_repeat_ngram_size,   ## same as 'trigram blocking'\n",
        "        )\n",
        "        ## If you want to decode by each sentence, you may \n",
        "        ## call 'decode' fn, not 'batch_decode'.\n",
        "        output = tokenizer.batch_decode(\n",
        "            output.tolist(), \n",
        "            skip_special_tokens=True,\n",
        "        )\n",
        "\n",
        "        ## Get all.\n",
        "        outputs.extend([{\"id\": id_, \"output\": output_} for id_, output_ in zip(id, output)])\n",
        "\n",
        "## Sort and extract.\n",
        "outputs = sorted(\n",
        "    outputs,\n",
        "    key=itemgetter(\"id\"),\n",
        "    reverse=False,\n",
        ")\n",
        "outputs = [i[\"output\"] for i in outputs]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "5BCq8SCEwDtK",
        "outputId": "6e9edc8d-8d6d-49ea-cfbe-d6db54d3093e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"한국카카오은행(카카오뱅크)은 '모임통장 서비스' 이용자가 출시 한달 만에 100만명을 넘어섰다고 7일 밝히며 동호회, 동아리 등 모임의 회비를 투명하고 편리하게 관리할 수 있어 인기를 끌고 있다고 전하며 계좌당 평균 3.01명 이상이 회비 현황을 공유하고 있는 것으로 나타났다.\""
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNyXUtnPrq1v"
      },
      "source": [
        "### **Save Prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "예측문을 저장하며, 별도의 후처리는 적용하지 않았습니다."
      ],
      "metadata": {
        "id": "Vw5CAXaUGZi7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy_93DB9tVBh"
      },
      "outputs": [],
      "source": [
        "def save_predictions(sample_submission_path: pathlib.PosixPath, predictions: List[Dict[str, str]], save_to: pathlib.PosixPath) -> pathlib.PosixPath:\n",
        "    ## Read a sample file.\n",
        "    df = pd.read_csv(sample_submission_path, index_col=False, sep=\"\\t\", encoding=\"utf-8\")\n",
        "\n",
        "    ## Record it.\n",
        "    ## Thus test datasets are already sorted by 'id', we don't need to\n",
        "    ## worry about shuffing.\n",
        "    # try:\n",
        "    #     df.loc[:, \"summary\"] = np.array(predictions)\n",
        "    # except Exception as e:\n",
        "    #     print(e)\n",
        "    #     df.iloc[:len(np.array(predictions)), 1:] = np.array(predictions)\n",
        "    df.loc[:, \"summary\"] = np.array(predictions)\n",
        "    \n",
        "    ## Strip.\n",
        "    df.loc[:, \"summary\"] = df.loc[:, \"summary\"].apply(lambda x: x.strip())\n",
        "\n",
        "    ## Save.\n",
        "    save_to.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(save_to, index=False, sep=\"\\t\", encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16_ObfSKrqzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51c57396-f979-4298-8a25-f009f172b33c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission save to: /content/prediction.tsv\n"
          ]
        }
      ],
      "source": [
        "save_to = Path.cwd() / Path(config.prediction_path)\n",
        "save_predictions(\n",
        "    sample_submission_path=config.sample_submission_path, \n",
        "    predictions=outputs,\n",
        "    save_to=save_to,\n",
        ")\n",
        "print(f\"Submission save to: {save_to}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(\"answer.tsv\")\n",
        "    files.download(\"prediction.tsv\")\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "WKBOrKoXyCWZ",
        "outputId": "2c6c5b6c-12c5-48fa-b561-f3fa42b0b230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a893fdd5-a41c-4c25-a362-8b3df71bbf15\", \"answer.tsv\", 100359)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d1394492-6353-4f10-b119-c2498d008a55\", \"prediction.tsv\", 130811)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykfXA550rqv5"
      },
      "source": [
        "## **Calculate Rouge Scores**\n",
        "\n",
        "* https://dacon.io/competitions/official/235673/talkboard/401911?page=1&dtype=recent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install Mecab**\n",
        "\n",
        "* https://sosomemo.tistory.com/30"
      ],
      "metadata": {
        "id": "zrQM0F2b2mhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd /tmp\n",
        "sudo wget https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
        "sudo tar xvf mecab-0.996-ko-0.9.2.tar.gz\n",
        "\n",
        "cd /tmp/mecab-0.996-ko-0.9.2\n",
        "sudo ./configure\n",
        "sudo make check\n",
        "sudo make install"
      ],
      "metadata": {
        "id": "og22bpJZ2oNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd /tmp\n",
        "wget https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
        "tar zxvf mecab-ko-dic-2.1.1-20180720.tar.gz\n",
        "\n",
        "cd /tmp/mecab-ko-dic-2.1.1-20180720\n",
        "sudo ./autogen.sh\n",
        "sudo ./configure\n",
        "sudo make\n",
        "sudo make install"
      ],
      "metadata": {
        "id": "YRfkCnLt2tXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd /tmp\n",
        "git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git\n",
        "cd mecab-python-0.996\n",
        "python3 setup.py build\n",
        "python3 setup.py install"
      ],
      "metadata": {
        "id": "Ywb3hBZC2oKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Define Main Functions**"
      ],
      "metadata": {
        "id": "Ma2HtJYL2mc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이콘에서 공식적으로 안내해주신 [평가용 코드](https://dacon.io/competitions/official/235673/talkboard/401911?page=1&dtype=recent)를 그대로 사용합니다."
      ],
      "metadata": {
        "id": "X7aJdo0xGlbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "WbFZhW-t4mSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "metadata": {
        "id": "Sv9_LpYH4nvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import konlpy\n",
        "from konlpy.tag import Mecab"
      ],
      "metadata": {
        "id": "G8BMtNpW2-kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import platform\n",
        "import itertools\n",
        "import collections\n",
        "import pkg_resources  # pip install py-rouge\n",
        "from io import open\n",
        "\n",
        "\n",
        "if platform.system() == \"Windows\":\n",
        "    try:\n",
        "        from eunjeon import Mecab\n",
        "    except:\n",
        "        print(\"please install eunjeon module\")\n",
        "else:  # Ubuntu일 경우\n",
        "    from konlpy.tag import Mecab\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Rouge:\n",
        "    DEFAULT_METRICS = {\"rouge-n\"}\n",
        "    DEFAULT_N = 1\n",
        "    STATS = [\"f\", \"p\", \"r\"]\n",
        "    AVAILABLE_METRICS = {\"rouge-n\", \"rouge-l\", \"rouge-w\"}\n",
        "    AVAILABLE_LENGTH_LIMIT_TYPES = {\"words\", \"bytes\"}\n",
        "    REMOVE_CHAR_PATTERN = re.compile(\"[^A-Za-z0-9가-힣]\")\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        metrics=None,\n",
        "        max_n=None,\n",
        "        limit_length=True,\n",
        "        length_limit=1000,\n",
        "        length_limit_type=\"words\",\n",
        "        apply_avg=True,\n",
        "        apply_best=False,\n",
        "        use_tokenizer=True,\n",
        "        alpha=0.5,\n",
        "        weight_factor=1.0,\n",
        "    ):\n",
        "        self.metrics = metrics[:] if metrics is not None else Rouge.DEFAULT_METRICS\n",
        "        for m in self.metrics:\n",
        "            if m not in Rouge.AVAILABLE_METRICS:\n",
        "                raise ValueError(\"Unknown metric '{}'\".format(m))\n",
        "\n",
        "\n",
        "        self.max_n = max_n if \"rouge-n\" in self.metrics else None\n",
        "        # Add all rouge-n metrics\n",
        "        if self.max_n is not None:\n",
        "            index_rouge_n = self.metrics.index(\"rouge-n\")\n",
        "            del self.metrics[index_rouge_n]\n",
        "            self.metrics += [\"rouge-{}\".format(n) for n in range(1, self.max_n + 1)]\n",
        "        self.metrics = set(self.metrics)\n",
        "\n",
        "\n",
        "        self.limit_length = limit_length\n",
        "        if self.limit_length:\n",
        "            if length_limit_type not in Rouge.AVAILABLE_LENGTH_LIMIT_TYPES:\n",
        "                raise ValueError(\"Unknown length_limit_type '{}'\".format(length_limit_type))\n",
        "\n",
        "\n",
        "        self.length_limit = length_limit\n",
        "        if self.length_limit == 0:\n",
        "            self.limit_length = False\n",
        "        self.length_limit_type = length_limit_type\n",
        "\n",
        "\n",
        "        self.use_tokenizer = use_tokenizer\n",
        "        if use_tokenizer:\n",
        "            self.tokenizer = Mecab()\n",
        "\n",
        "\n",
        "        self.apply_avg = apply_avg\n",
        "        self.apply_best = apply_best\n",
        "        self.alpha = alpha\n",
        "        self.weight_factor = weight_factor\n",
        "        if self.weight_factor <= 0:\n",
        "            raise ValueError(\"ROUGE-W weight factor must greater than 0.\")\n",
        "\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        if self.use_tokenizer:\n",
        "            return self.tokenizer.morphs(text)\n",
        "        else:\n",
        "            return text\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def split_into_sentences(text):\n",
        "        return text.split(\"\\n\")\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_ngrams(n, text):\n",
        "        ngram_set = collections.defaultdict(int)\n",
        "        max_index_ngram_start = len(text) - n\n",
        "        for i in range(max_index_ngram_start + 1):\n",
        "            ngram_set[tuple(text[i : i + n])] += 1\n",
        "        return ngram_set\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _split_into_words(sentences):\n",
        "        return list(itertools.chain(*[_.split() for _ in sentences]))\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_word_ngrams_and_length(n, sentences):\n",
        "        assert len(sentences) > 0\n",
        "        assert n > 0\n",
        "\n",
        "\n",
        "        tokens = Rouge._split_into_words(sentences)\n",
        "        return Rouge._get_ngrams(n, tokens), tokens, len(tokens) - (n - 1)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_unigrams(sentences):\n",
        "        assert len(sentences) > 0\n",
        "\n",
        "\n",
        "        tokens = Rouge._split_into_words(sentences)\n",
        "        unigram_set = collections.defaultdict(int)\n",
        "        for token in tokens:\n",
        "            unigram_set[token] += 1\n",
        "        return unigram_set, len(tokens)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_p_r_f_score(\n",
        "        evaluated_count,\n",
        "        reference_count,\n",
        "        overlapping_count,\n",
        "        alpha=0.5,\n",
        "        weight_factor=1.0,\n",
        "    ):\n",
        "        precision = 0.0 if evaluated_count == 0 else overlapping_count / float(evaluated_count)\n",
        "        if weight_factor != 1.0:\n",
        "            precision = precision ** (1.0 / weight_factor)\n",
        "        recall = 0.0 if reference_count == 0 else overlapping_count / float(reference_count)\n",
        "        if weight_factor != 1.0:\n",
        "            recall = recall ** (1.0 / weight_factor)\n",
        "        f1_score = Rouge._compute_f_score(precision, recall, alpha)\n",
        "        return {\"f\": f1_score, \"p\": precision, \"r\": recall}\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_f_score(precision, recall, alpha=0.5):\n",
        "        return (\n",
        "            0.0\n",
        "            if (recall == 0.0 or precision == 0.0)\n",
        "            else precision * recall / ((1 - alpha) * precision + alpha * recall)\n",
        "        )\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_ngrams(evaluated_sentences, reference_sentences, n):\n",
        "        if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
        "            raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
        "\n",
        "\n",
        "        evaluated_ngrams, _, evaluated_count = Rouge._get_word_ngrams_and_length(\n",
        "            n, evaluated_sentences\n",
        "        )\n",
        "        reference_ngrams, _, reference_count = Rouge._get_word_ngrams_and_length(\n",
        "            n, reference_sentences\n",
        "        )\n",
        "\n",
        "\n",
        "        # Gets the overlapping ngrams between evaluated and reference\n",
        "        overlapping_ngrams = set(evaluated_ngrams.keys()).intersection(set(reference_ngrams.keys()))\n",
        "        overlapping_count = 0\n",
        "        for ngram in overlapping_ngrams:\n",
        "            overlapping_count += min(evaluated_ngrams[ngram], reference_ngrams[ngram])\n",
        "\n",
        "\n",
        "        return evaluated_count, reference_count, overlapping_count\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_ngrams_lcs(evaluated_sentences, reference_sentences, weight_factor=1.0):\n",
        "        def _lcs(x, y):\n",
        "            m = len(x)\n",
        "            n = len(y)\n",
        "            vals = collections.defaultdict(int)\n",
        "            dirs = collections.defaultdict(int)\n",
        "\n",
        "\n",
        "            for i in range(1, m + 1):\n",
        "                for j in range(1, n + 1):\n",
        "                    if x[i - 1] == y[j - 1]:\n",
        "                        vals[i, j] = vals[i - 1, j - 1] + 1\n",
        "                        dirs[i, j] = \"|\"\n",
        "                    elif vals[i - 1, j] >= vals[i, j - 1]:\n",
        "                        vals[i, j] = vals[i - 1, j]\n",
        "                        dirs[i, j] = \"^\"\n",
        "                    else:\n",
        "                        vals[i, j] = vals[i, j - 1]\n",
        "                        dirs[i, j] = \"<\"\n",
        "\n",
        "\n",
        "            return vals, dirs\n",
        "\n",
        "\n",
        "        def _wlcs(x, y, weight_factor):\n",
        "            m = len(x)\n",
        "            n = len(y)\n",
        "            vals = collections.defaultdict(float)\n",
        "            dirs = collections.defaultdict(int)\n",
        "            lengths = collections.defaultdict(int)\n",
        "\n",
        "\n",
        "            for i in range(1, m + 1):\n",
        "                for j in range(1, n + 1):\n",
        "                    if x[i - 1] == y[j - 1]:\n",
        "                        length_tmp = lengths[i - 1, j - 1]\n",
        "                        vals[i, j] = (\n",
        "                            vals[i - 1, j - 1]\n",
        "                            + (length_tmp + 1) ** weight_factor\n",
        "                            - length_tmp ** weight_factor\n",
        "                        )\n",
        "                        dirs[i, j] = \"|\"\n",
        "                        lengths[i, j] = length_tmp + 1\n",
        "                    elif vals[i - 1, j] >= vals[i, j - 1]:\n",
        "                        vals[i, j] = vals[i - 1, j]\n",
        "                        dirs[i, j] = \"^\"\n",
        "                        lengths[i, j] = 0\n",
        "                    else:\n",
        "                        vals[i, j] = vals[i, j - 1]\n",
        "                        dirs[i, j] = \"<\"\n",
        "                        lengths[i, j] = 0\n",
        "\n",
        "\n",
        "            return vals, dirs\n",
        "\n",
        "\n",
        "        def _mark_lcs(mask, dirs, m, n):\n",
        "            while m != 0 and n != 0:\n",
        "                if dirs[m, n] == \"|\":\n",
        "                    m -= 1\n",
        "                    n -= 1\n",
        "                    mask[m] = 1\n",
        "                elif dirs[m, n] == \"^\":\n",
        "                    m -= 1\n",
        "                elif dirs[m, n] == \"<\":\n",
        "                    n -= 1\n",
        "                else:\n",
        "                    raise UnboundLocalError(\"Illegal move\")\n",
        "\n",
        "\n",
        "            return mask\n",
        "\n",
        "\n",
        "        if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
        "            raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
        "\n",
        "\n",
        "        evaluated_unigrams_dict, evaluated_count = Rouge._get_unigrams(evaluated_sentences)\n",
        "        reference_unigrams_dict, reference_count = Rouge._get_unigrams(reference_sentences)\n",
        "\n",
        "\n",
        "        # Has to use weight factor for WLCS\n",
        "        use_WLCS = weight_factor != 1.0\n",
        "        if use_WLCS:\n",
        "            evaluated_count = evaluated_count ** weight_factor\n",
        "            reference_count = 0\n",
        "\n",
        "\n",
        "        overlapping_count = 0.0\n",
        "        for reference_sentence in reference_sentences:\n",
        "            reference_sentence_tokens = reference_sentence.split()\n",
        "            if use_WLCS:\n",
        "                reference_count += len(reference_sentence_tokens) ** weight_factor\n",
        "            hit_mask = [0 for _ in range(len(reference_sentence_tokens))]\n",
        "\n",
        "\n",
        "            for evaluated_sentence in evaluated_sentences:\n",
        "                evaluated_sentence_tokens = evaluated_sentence.split()\n",
        "\n",
        "\n",
        "                if use_WLCS:\n",
        "                    _, lcs_dirs = _wlcs(\n",
        "                        reference_sentence_tokens,\n",
        "                        evaluated_sentence_tokens,\n",
        "                        weight_factor,\n",
        "                    )\n",
        "                else:\n",
        "                    _, lcs_dirs = _lcs(reference_sentence_tokens, evaluated_sentence_tokens)\n",
        "                _mark_lcs(\n",
        "                    hit_mask,\n",
        "                    lcs_dirs,\n",
        "                    len(reference_sentence_tokens),\n",
        "                    len(evaluated_sentence_tokens),\n",
        "                )\n",
        "\n",
        "\n",
        "            overlapping_count_length = 0\n",
        "            for ref_token_id, val in enumerate(hit_mask):\n",
        "                if val == 1:\n",
        "                    token = reference_sentence_tokens[ref_token_id]\n",
        "                    if evaluated_unigrams_dict[token] > 0 and reference_unigrams_dict[token] > 0:\n",
        "                        evaluated_unigrams_dict[token] -= 1\n",
        "                        reference_unigrams_dict[ref_token_id] -= 1\n",
        "\n",
        "\n",
        "                        if use_WLCS:\n",
        "                            overlapping_count_length += 1\n",
        "                            if (\n",
        "                                ref_token_id + 1 < len(hit_mask) and hit_mask[ref_token_id + 1] == 0\n",
        "                            ) or ref_token_id + 1 == len(hit_mask):\n",
        "                                overlapping_count += overlapping_count_length ** weight_factor\n",
        "                                overlapping_count_length = 0\n",
        "                        else:\n",
        "                            overlapping_count += 1\n",
        "\n",
        "\n",
        "        if use_WLCS:\n",
        "            reference_count = reference_count ** weight_factor\n",
        "\n",
        "\n",
        "        return evaluated_count, reference_count, overlapping_count\n",
        "\n",
        "\n",
        "    def get_scores(self, hypothesis, references):\n",
        "        if isinstance(hypothesis, str):\n",
        "            hypothesis, references = [hypothesis], [references]\n",
        "\n",
        "\n",
        "        if type(hypothesis) != type(references):\n",
        "            raise ValueError(\"'hyps' and 'refs' are not of the same type\")\n",
        "\n",
        "\n",
        "        if len(hypothesis) != len(references):\n",
        "            raise ValueError(\"'hyps' and 'refs' do not have the same length\")\n",
        "        scores = {}\n",
        "        has_rouge_n_metric = (\n",
        "            len([metric for metric in self.metrics if metric.split(\"-\")[-1].isdigit()]) > 0\n",
        "        )\n",
        "        if has_rouge_n_metric:\n",
        "            scores.update(self._get_scores_rouge_n(hypothesis, references))\n",
        "            # scores = {**scores, **self._get_scores_rouge_n(hypothesis, references)}\n",
        "\n",
        "\n",
        "        has_rouge_l_metric = (\n",
        "            len([metric for metric in self.metrics if metric.split(\"-\")[-1].lower() == \"l\"]) > 0\n",
        "        )\n",
        "        if has_rouge_l_metric:\n",
        "            scores.update(self._get_scores_rouge_l_or_w(hypothesis, references, False))\n",
        "            # scores = {**scores, **self._get_scores_rouge_l_or_w(hypothesis, references, False)}\n",
        "\n",
        "\n",
        "        has_rouge_w_metric = (\n",
        "            len([metric for metric in self.metrics if metric.split(\"-\")[-1].lower() == \"w\"]) > 0\n",
        "        )\n",
        "        if has_rouge_w_metric:\n",
        "            scores.update(self._get_scores_rouge_l_or_w(hypothesis, references, True))\n",
        "            # scores = {**scores, **self._get_scores_rouge_l_or_w(hypothesis, references, True)}\n",
        "\n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "    def _get_scores_rouge_n(self, all_hypothesis, all_references):\n",
        "        metrics = [metric for metric in self.metrics if metric.split(\"-\")[-1].isdigit()]\n",
        "\n",
        "\n",
        "        if self.apply_avg or self.apply_best:\n",
        "            scores = {metric: {stat: 0.0 for stat in Rouge.STATS} for metric in metrics}\n",
        "        else:\n",
        "            scores = {\n",
        "                metric: [{stat: [] for stat in Rouge.STATS} for _ in range(len(all_hypothesis))]\n",
        "                for metric in metrics\n",
        "            }\n",
        "\n",
        "\n",
        "        for sample_id, (hypothesis, references) in enumerate(zip(all_hypothesis, all_references)):\n",
        "            assert isinstance(hypothesis, str)\n",
        "            has_multiple_references = False\n",
        "            if isinstance(references, list):\n",
        "                has_multiple_references = len(references) > 1\n",
        "                if not has_multiple_references:\n",
        "                    references = references[0]\n",
        "\n",
        "\n",
        "            # Prepare hypothesis and reference(s)\n",
        "            hypothesis = self._preprocess_summary_as_a_whole(hypothesis)\n",
        "            references = (\n",
        "                [self._preprocess_summary_as_a_whole(reference) for reference in references]\n",
        "                if has_multiple_references\n",
        "                else [self._preprocess_summary_as_a_whole(references)]\n",
        "            )\n",
        "\n",
        "\n",
        "            # Compute scores\n",
        "            for metric in metrics:\n",
        "                suffix = metric.split(\"-\")[-1]\n",
        "                n = int(suffix)\n",
        "\n",
        "\n",
        "                # Aggregate\n",
        "                if self.apply_avg:\n",
        "                    # average model\n",
        "                    total_hypothesis_ngrams_count = 0\n",
        "                    total_reference_ngrams_count = 0\n",
        "                    total_ngrams_overlapping_count = 0\n",
        "\n",
        "\n",
        "                    for reference in references:\n",
        "                        (\n",
        "                            hypothesis_count,\n",
        "                            reference_count,\n",
        "                            overlapping_ngrams,\n",
        "                        ) = Rouge._compute_ngrams(hypothesis, reference, n)\n",
        "                        total_hypothesis_ngrams_count += hypothesis_count\n",
        "                        total_reference_ngrams_count += reference_count\n",
        "                        total_ngrams_overlapping_count += overlapping_ngrams\n",
        "\n",
        "\n",
        "                    score = Rouge._compute_p_r_f_score(\n",
        "                        total_hypothesis_ngrams_count,\n",
        "                        total_reference_ngrams_count,\n",
        "                        total_ngrams_overlapping_count,\n",
        "                        self.alpha,\n",
        "                    )\n",
        "\n",
        "\n",
        "                    for stat in Rouge.STATS:\n",
        "                        scores[metric][stat] += score[stat]\n",
        "                else:\n",
        "                    # Best model\n",
        "                    if self.apply_best:\n",
        "                        best_current_score = None\n",
        "                        for reference in references:\n",
        "                            (\n",
        "                                hypothesis_count,\n",
        "                                reference_count,\n",
        "                                overlapping_ngrams,\n",
        "                            ) = Rouge._compute_ngrams(hypothesis, reference, n)\n",
        "                            score = Rouge._compute_p_r_f_score(\n",
        "                                hypothesis_count,\n",
        "                                reference_count,\n",
        "                                overlapping_ngrams,\n",
        "                                self.alpha,\n",
        "                            )\n",
        "                            if best_current_score is None or score[\"r\"] > best_current_score[\"r\"]:\n",
        "                                best_current_score = score\n",
        "\n",
        "\n",
        "                        for stat in Rouge.STATS:\n",
        "                            scores[metric][stat] += best_current_score[stat]\n",
        "                    # Keep all\n",
        "                    else:\n",
        "                        for reference in references:\n",
        "                            (\n",
        "                                hypothesis_count,\n",
        "                                reference_count,\n",
        "                                overlapping_ngrams,\n",
        "                            ) = Rouge._compute_ngrams(hypothesis, reference, n)\n",
        "                            score = Rouge._compute_p_r_f_score(\n",
        "                                hypothesis_count,\n",
        "                                reference_count,\n",
        "                                overlapping_ngrams,\n",
        "                                self.alpha,\n",
        "                            )\n",
        "                            for stat in Rouge.STATS:\n",
        "                                scores[metric][sample_id][stat].append(score[stat])\n",
        "\n",
        "\n",
        "        # Compute final score with the average or the the max\n",
        "        if (self.apply_avg or self.apply_best) and len(all_hypothesis) > 1:\n",
        "            for metric in metrics:\n",
        "                for stat in Rouge.STATS:\n",
        "                    scores[metric][stat] /= len(all_hypothesis)\n",
        "\n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "    def _get_scores_rouge_l_or_w(self, all_hypothesis, all_references, use_w=False):\n",
        "        metric = \"rouge-w\" if use_w else \"rouge-l\"\n",
        "        if self.apply_avg or self.apply_best:\n",
        "            scores = {metric: {stat: 0.0 for stat in Rouge.STATS}}\n",
        "        else:\n",
        "            scores = {\n",
        "                metric: [{stat: [] for stat in Rouge.STATS} for _ in range(len(all_hypothesis))]\n",
        "            }\n",
        "\n",
        "\n",
        "        for sample_id, (hypothesis_sentences, references_sentences) in enumerate(\n",
        "            zip(all_hypothesis, all_references)\n",
        "        ):\n",
        "            assert isinstance(hypothesis_sentences, str)\n",
        "            has_multiple_references = False\n",
        "            if isinstance(references_sentences, list):\n",
        "                has_multiple_references = len(references_sentences) > 1\n",
        "                if not has_multiple_references:\n",
        "                    references_sentences = references_sentences[0]\n",
        "\n",
        "\n",
        "            # Prepare hypothesis and reference(s)\n",
        "            hypothesis_sentences = self._preprocess_summary_per_sentence(hypothesis_sentences)\n",
        "            references_sentences = (\n",
        "                [\n",
        "                    self._preprocess_summary_per_sentence(reference)\n",
        "                    for reference in references_sentences\n",
        "                ]\n",
        "                if has_multiple_references\n",
        "                else [self._preprocess_summary_per_sentence(references_sentences)]\n",
        "            )\n",
        "\n",
        "\n",
        "            # Compute scores\n",
        "            # Aggregate\n",
        "            if self.apply_avg:\n",
        "                # average model\n",
        "                total_hypothesis_ngrams_count = 0\n",
        "                total_reference_ngrams_count = 0\n",
        "                total_ngrams_overlapping_count = 0\n",
        "\n",
        "\n",
        "                for reference_sentences in references_sentences:\n",
        "                    (\n",
        "                        hypothesis_count,\n",
        "                        reference_count,\n",
        "                        overlapping_ngrams,\n",
        "                    ) = Rouge._compute_ngrams_lcs(\n",
        "                        hypothesis_sentences,\n",
        "                        reference_sentences,\n",
        "                        self.weight_factor if use_w else 1.0,\n",
        "                    )\n",
        "                    total_hypothesis_ngrams_count += hypothesis_count\n",
        "                    total_reference_ngrams_count += reference_count\n",
        "                    total_ngrams_overlapping_count += overlapping_ngrams\n",
        "\n",
        "\n",
        "                score = Rouge._compute_p_r_f_score(\n",
        "                    total_hypothesis_ngrams_count,\n",
        "                    total_reference_ngrams_count,\n",
        "                    total_ngrams_overlapping_count,\n",
        "                    self.alpha,\n",
        "                    self.weight_factor if use_w else 1.0,\n",
        "                )\n",
        "                for stat in Rouge.STATS:\n",
        "                    scores[metric][stat] += score[stat]\n",
        "            else:\n",
        "                # Best model\n",
        "                if self.apply_best:\n",
        "                    best_current_score = None\n",
        "                    best_current_score_wlcs = None\n",
        "                    for reference_sentences in references_sentences:\n",
        "                        (\n",
        "                            hypothesis_count,\n",
        "                            reference_count,\n",
        "                            overlapping_ngrams,\n",
        "                        ) = Rouge._compute_ngrams_lcs(\n",
        "                            hypothesis_sentences,\n",
        "                            reference_sentences,\n",
        "                            self.weight_factor if use_w else 1.0,\n",
        "                        )\n",
        "                        score = Rouge._compute_p_r_f_score(\n",
        "                            total_hypothesis_ngrams_count,\n",
        "                            total_reference_ngrams_count,\n",
        "                            total_ngrams_overlapping_count,\n",
        "                            self.alpha,\n",
        "                            self.weight_factor if use_w else 1.0,\n",
        "                        )\n",
        "\n",
        "\n",
        "                        if use_w:\n",
        "                            reference_count_for_score = reference_count ** (\n",
        "                                1.0 / self.weight_factor\n",
        "                            )\n",
        "                            overlapping_ngrams_for_score = overlapping_ngrams\n",
        "                            score_wlcs = (\n",
        "                                overlapping_ngrams_for_score / reference_count_for_score\n",
        "                            ) ** (1.0 / self.weight_factor)\n",
        "\n",
        "\n",
        "                            if (\n",
        "                                best_current_score_wlcs is None\n",
        "                                or score_wlcs > best_current_score_wlcs\n",
        "                            ):\n",
        "                                best_current_score = score\n",
        "                                best_current_score_wlcs = score_wlcs\n",
        "                        else:\n",
        "                            if best_current_score is None or score[\"r\"] > best_current_score[\"r\"]:\n",
        "                                best_current_score = score\n",
        "\n",
        "\n",
        "                    for stat in Rouge.STATS:\n",
        "                        scores[metric][stat] += best_current_score[stat]\n",
        "                # Keep all\n",
        "                else:\n",
        "                    for reference_sentences in references_sentences:\n",
        "                        (\n",
        "                            hypothesis_count,\n",
        "                            reference_count,\n",
        "                            overlapping_ngrams,\n",
        "                        ) = Rouge._compute_ngrams_lcs(\n",
        "                            hypothesis_sentences,\n",
        "                            reference_sentences,\n",
        "                            self.weight_factor if use_w else 1.0,\n",
        "                        )\n",
        "                        score = Rouge._compute_p_r_f_score(\n",
        "                            hypothesis_count,\n",
        "                            reference_count,\n",
        "                            overlapping_ngrams,\n",
        "                            self.alpha,\n",
        "                            self.weight_factor,\n",
        "                        )\n",
        "\n",
        "\n",
        "                        for stat in Rouge.STATS:\n",
        "                            scores[metric][sample_id][stat].append(score[stat])\n",
        "\n",
        "\n",
        "        # Compute final score with the average or the the max\n",
        "        if (self.apply_avg or self.apply_best) and len(all_hypothesis) > 1:\n",
        "            for stat in Rouge.STATS:\n",
        "                scores[metric][stat] /= len(all_hypothesis)\n",
        "\n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "    def _preprocess_summary_as_a_whole(self, summary):\n",
        "        sentences = Rouge.split_into_sentences(summary)\n",
        "\n",
        "\n",
        "        # Truncate\n",
        "        if self.limit_length:\n",
        "            # By words\n",
        "            if self.length_limit_type == \"words\":\n",
        "                summary = \" \".join(sentences)\n",
        "                all_tokens = summary.split()  # Counting as in the perls script\n",
        "                summary = \" \".join(all_tokens[: self.length_limit])\n",
        "\n",
        "\n",
        "            # By bytes\n",
        "            elif self.length_limit_type == \"bytes\":\n",
        "                summary = \"\"\n",
        "                current_len = 0\n",
        "                for sentence in sentences:\n",
        "                    sentence = sentence.strip()\n",
        "                    sentence_len = len(sentence)\n",
        "\n",
        "\n",
        "                    if current_len + sentence_len < self.length_limit:\n",
        "                        if current_len != 0:\n",
        "                            summary += \" \"\n",
        "                        summary += sentence\n",
        "                        current_len += sentence_len\n",
        "                    else:\n",
        "                        if current_len > 0:\n",
        "                            summary += \" \"\n",
        "                        summary += sentence[: self.length_limit - current_len]\n",
        "                        break\n",
        "        else:\n",
        "            summary = \" \".join(sentences)\n",
        "\n",
        "\n",
        "        summary = Rouge.REMOVE_CHAR_PATTERN.sub(\" \", summary.lower()).strip()\n",
        "\n",
        "\n",
        "        tokens = self.tokenize_text(Rouge.REMOVE_CHAR_PATTERN.sub(\" \", summary))\n",
        "        preprocessed_summary = [\" \".join(tokens)]\n",
        "\n",
        "\n",
        "        return preprocessed_summary\n",
        "\n",
        "\n",
        "    def _preprocess_summary_per_sentence(self, summary):\n",
        "        sentences = Rouge.split_into_sentences(summary)\n",
        "\n",
        "\n",
        "        # Truncate\n",
        "        if self.limit_length:\n",
        "            final_sentences = []\n",
        "            current_len = 0\n",
        "            # By words\n",
        "            if self.length_limit_type == \"words\":\n",
        "                for sentence in sentences:\n",
        "                    tokens = sentence.strip().split()\n",
        "                    tokens_len = len(tokens)\n",
        "                    if current_len + tokens_len < self.length_limit:\n",
        "                        sentence = \" \".join(tokens)\n",
        "                        final_sentences.append(sentence)\n",
        "                        current_len += tokens_len\n",
        "                    else:\n",
        "                        sentence = \" \".join(tokens[: self.length_limit - current_len])\n",
        "                        final_sentences.append(sentence)\n",
        "                        break\n",
        "            # By bytes\n",
        "            elif self.length_limit_type == \"bytes\":\n",
        "                for sentence in sentences:\n",
        "                    sentence = sentence.strip()\n",
        "                    sentence_len = len(sentence)\n",
        "                    if current_len + sentence_len < self.length_limit:\n",
        "                        final_sentences.append(sentence)\n",
        "                        current_len += sentence_len\n",
        "                    else:\n",
        "                        sentence = sentence[: self.length_limit - current_len]\n",
        "                        final_sentences.append(sentence)\n",
        "                        break\n",
        "            sentences = final_sentences\n",
        "\n",
        "\n",
        "        final_sentences = []\n",
        "        for sentence in sentences:\n",
        "            sentence = Rouge.REMOVE_CHAR_PATTERN.sub(\" \", sentence.lower()).strip()\n",
        "\n",
        "\n",
        "            tokens = self.tokenize_text(Rouge.REMOVE_CHAR_PATTERN.sub(\" \", sentence))\n",
        "\n",
        "\n",
        "            sentence = \" \".join(tokens)\n",
        "\n",
        "\n",
        "            final_sentences.append(sentence)\n",
        "\n",
        "\n",
        "        return final_sentences"
      ],
      "metadata": {
        "id": "mE9Pn4YH0BAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "class RougeScorer:\n",
        "    def __init__(self):\n",
        "        self.rouge_evaluator = Rouge(\n",
        "            metrics=[\"rouge-n\", \"rouge-l\"],\n",
        "            max_n=2,\n",
        "            limit_length=True,\n",
        "            length_limit=1000,\n",
        "            length_limit_type=\"words\",\n",
        "            use_tokenizer=True,\n",
        "            apply_avg=True,\n",
        "            apply_best=False,\n",
        "            alpha=0.5,  # Default F1_score\n",
        "            weight_factor=1.2,\n",
        "        )\n",
        "\n",
        "    def compute_rouge(self, ref_df, hyp_df):\n",
        "        #ref_df = pd.read_csv(ref_path)\n",
        "        #hyp_df = pd.read_csv(hyp_path)\n",
        "        hyp_df.iloc[:,1] = hyp_df.iloc[:,1].fillna(' ')\n",
        "        ids = ref_df['id']\n",
        "        hyp_df = hyp_df[hyp_df['id'].isin(ids)]\n",
        "        hyp_df.index = ref_df.index\n",
        "\n",
        "        ref_df = ref_df.sort_values(by=[\"id\"])\n",
        "        hyp_df = hyp_df.sort_values(by=[\"id\"])\n",
        "        ref_df[\"id\"] = ref_df[\"id\"].astype(int)\n",
        "        hyp_df[\"id\"] = hyp_df[\"id\"].astype(int)\n",
        "\n",
        "        hyps = [tuple(row) for row in hyp_df.values]\n",
        "        refs = [tuple(row) for row in ref_df.values]\n",
        "\n",
        "        reference_summaries = []\n",
        "        generated_summaries = []\n",
        "\n",
        "        for ref_tp, hyp_tp in zip(refs, hyps):\n",
        "            ref_id, ref = ref_tp\n",
        "            hyp_id, hyp = hyp_tp\n",
        "\n",
        "            assert ref_id == hyp_id\n",
        "\n",
        "            reference_summaries.append(ref)\n",
        "            generated_summaries.append(hyp)\n",
        "\n",
        "        scores = self.rouge_evaluator.get_scores(generated_summaries, reference_summaries)\n",
        "        str_scores = self.format_rouge_scores(scores)\n",
        "        #self.save_rouge_scores(str_scores)\n",
        "        return str_scores\n",
        "\n",
        "    def save_rouge_scores(self, str_scores):\n",
        "        with open(\"rouge_scores.txt\", \"w\") as output:\n",
        "            output.write(str_scores)\n",
        "\n",
        "    def format_rouge_scores(self, scores):\n",
        "    \treturn \"{:.3f},{:.3f},{:.3f}\".format(\n",
        "            scores[\"rouge-1\"][\"f\"],\n",
        "            scores[\"rouge-2\"][\"f\"],\n",
        "            scores[\"rouge-l\"][\"f\"],\n",
        "        )"
      ],
      "metadata": {
        "id": "glEQHxI3zX-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scoring**"
      ],
      "metadata": {
        "id": "Vo3W2ljoG3KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Ref: answer.tsv, Hyp: prediction.tsv\n",
        "ref_df = pd.read_csv(Path.cwd() / Path(config.answer_path), index_col=False, sep=\"\\t\", encoding=\"utf-8\")\n",
        "hyp_df = pd.read_csv(Path.cwd() / Path(config.prediction_path), index_col=False, sep=\"\\t\", encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "Gk1Wh5D-zX7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhLWGG0jqjnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa59200-5723-4043-880c-e2964fec9ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rouge-1: 0.490\n",
            "Rouge-2: 0.311\n",
            "Rouge-N: 0.376\n"
          ]
        }
      ],
      "source": [
        "## Compute rouge score.\n",
        "scorer = RougeScorer()\n",
        "scores = scorer.compute_rouge(ref_df, hyp_df)\n",
        "\n",
        "print(\"\\n\".join([f\"Rouge-{i}: {score}\" for i, score in zip([1, 2, \"N\"], scores.split(\",\"))]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Show Samples**\n",
        "\n",
        "실제 정답에 비해 요약문이 다소 길게 생성된 것처럼 보이므로, model.generate()의 `min_length` 값을 줄이거나, 혹은 `length_penalty`를 조정할 수 있을 것입니다."
      ],
      "metadata": {
        "id": "nrFSWKhP_TqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = ref_df.copy()\n",
        "tmp = pd.merge(tmp, hyp_df.copy(), how=\"outer\", on=\"id\")\n",
        "tmp.rename(columns={\"summary_x\": \"label\", \"summary_y\": \"prediction\"}, inplace=True)\n",
        "tmp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "la1NhzvD_TkI",
        "outputId": "8547c0dd-5b59-4ec0-dcce-22a2bee141ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-85dbb473-bb2c-4dba-9ec3-7c565d61f126\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>330061292</td>\n",
              "      <td>한국카카오은행(카카오뱅크)이 7일 발표한 내용에 따르면 지난해 12월 3일 출시한 ...</td>\n",
              "      <td>한국카카오은행(카카오뱅크)은 '모임통장 서비스' 이용자가 출시 한달 만에 100만명...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>330852492</td>\n",
              "      <td>국도 교통부는 국토의 홍수 대응능력을 강화하기 위해 하천설계 기준을 전면 개정하여 ...</td>\n",
              "      <td>국토교통부는 14일 상습 도시침수지역, 기후변화로 인한 국지성 호우 등에 대한 대비...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>332591740</td>\n",
              "      <td>지난 28일 송파구 잠실롯대호텔에서 열린 '2019년 정기총회'에서 8715표 중 ...</td>\n",
              "      <td>28일 잠실 송파구 잠실롯데호텔에서 열린 '2019년 정기총회'에서 서울지방변호사회...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>333847004</td>\n",
              "      <td>한국산업기술진흥원이 11일 발표한 '글로벌 1000대 기업의 2017년 R&amp;D 투자...</td>\n",
              "      <td>한국산업기술진흥원(KIAT)은 11일 '글로벌 1000대 기업의 2017년 R&amp;D ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>334025004</td>\n",
              "      <td>2학년 수업에 대부분 편성되어 있는 물리학Ⅰ, 화학Ⅰ, 생명과학Ⅰ, 지구과학Ⅰ 수업...</td>\n",
              "      <td>두림학원 박창현 물리 대표강사는 2015 개정교육과정 물리학I은 이전 물리에 비해 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>370100982</td>\n",
              "      <td>LG화학과 산업은행, 수출입은행, 농협은행은 9일 서울 마곡 LG사이언스파크에서 '...</td>\n",
              "      <td>LG화학과 산업은행, 수출입은행, 농협은행은 9일 서울 마곡 LG사이언스파크에서 '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>370421105</td>\n",
              "      <td>10일 김상도 국토부 종합교통정책관은 정부세종청사 보고에서, 타다 개정안은 기존산업...</td>\n",
              "      <td>국토교통부는 '자동차운수사업법' 개정안에 대한 논란이 '금지'냐 '허용'이냐라는 이...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>370977175</td>\n",
              "      <td>단속을 피해 달아나던 불법취업 외국인 근로자가 추락사하는 사고가 발생하자 유족은 업...</td>\n",
              "      <td>서울행정법원 행정합의3부(박성규 부장판사)는 A씨 유족이 근로복지공단을 상대로 낸 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>371845887</td>\n",
              "      <td>현대중공업그룹이 연말 막바지 수주 총력전을 펼치며 수주 속도를 내면서 수주목표를 7...</td>\n",
              "      <td>현대중공업그룹은 건조한 가스운반선 현대중공업그룹은 최근 해외 선사로부터 17만400...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>372005720</td>\n",
              "      <td>서울 강남구는 오는 31일 삼성동 코엑스 광장 및 영동대로에서 '2020년 새해맞이...</td>\n",
              "      <td>서울 강남구가 오는 31일 삼성동 코엑스 광장과 영동대로에서 '2020년 새해맞이 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>302 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85dbb473-bb2c-4dba-9ec3-7c565d61f126')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-85dbb473-bb2c-4dba-9ec3-7c565d61f126 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-85dbb473-bb2c-4dba-9ec3-7c565d61f126');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            id  ...                                         prediction\n",
              "0    330061292  ...  한국카카오은행(카카오뱅크)은 '모임통장 서비스' 이용자가 출시 한달 만에 100만명...\n",
              "1    330852492  ...  국토교통부는 14일 상습 도시침수지역, 기후변화로 인한 국지성 호우 등에 대한 대비...\n",
              "2    332591740  ...  28일 잠실 송파구 잠실롯데호텔에서 열린 '2019년 정기총회'에서 서울지방변호사회...\n",
              "3    333847004  ...  한국산업기술진흥원(KIAT)은 11일 '글로벌 1000대 기업의 2017년 R&D ...\n",
              "4    334025004  ...  두림학원 박창현 물리 대표강사는 2015 개정교육과정 물리학I은 이전 물리에 비해 ...\n",
              "..         ...  ...                                                ...\n",
              "297  370100982  ...  LG화학과 산업은행, 수출입은행, 농협은행은 9일 서울 마곡 LG사이언스파크에서 '...\n",
              "298  370421105  ...  국토교통부는 '자동차운수사업법' 개정안에 대한 논란이 '금지'냐 '허용'이냐라는 이...\n",
              "299  370977175  ...  서울행정법원 행정합의3부(박성규 부장판사)는 A씨 유족이 근로복지공단을 상대로 낸 ...\n",
              "300  371845887  ...  현대중공업그룹은 건조한 가스운반선 현대중공업그룹은 최근 해외 선사로부터 17만400...\n",
              "301  372005720  ...  서울 강남구가 오는 31일 삼성동 코엑스 광장과 영동대로에서 '2020년 새해맞이 ...\n",
              "\n",
              "[302 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _, instance in tmp.sample(n=3).iterrows():\n",
        "    print(f\"id: {instance['id']}\")\n",
        "    print(f\"  label: {instance['label']}\")\n",
        "    print(f\"  prediction: {instance['prediction']}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6tkniylBGrs",
        "outputId": "17afef03-43a9-437c-9496-925192371d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id: 356915849\n",
            "  label: 모테기 도시미츠 일본 경제재정재생상과 라이트 하이저 미국 통상대표부(USTR) 대표가 21일부터 미국 워싱턴에서 쇠고기 등 농산품과 자동차 등의 분야를 중점으로 장관급 무역협상을 재개하기로 할 것으로 보인다.\n",
            "  prediction: 미국과 중국, 무역협상이 진전을 보지 못하는 가운데 미국과 일본이 21일부터 미국 워싱턴에서 장관급 무역협상을 진행하는데, 핵심 쟁점은 쇠고기 등 농산품과 자동차 등의 분야에서 양국이 얼마나 양보할지를 두고 밀고 당기기가 있을 것으로 보이며, 일본 언론은 9월 말까지 타결이 이뤄져야 후속조치를 취할 수 있다고 전했다.\n",
            "\n",
            "id: 366909380\n",
            "  label: 삼성전자의 전 전무는 세계 최초로 7나노 극자외선(EUV) 생산라인을 구축해 우리나라 반도체 품질 향상과 산업 경쟁력 강화에 기여한 공로를 인정받아  국가품질경영대회 최고 영예인 '은탑산업훈장'을 수훈했다.\n",
            "  prediction: 정상섭 삼성전자 전무가 세계 최초로 7나노 극자외선(EUV) 생산라인을 구축해 우리나라 반도체 품질 향상과 산업 경쟁력 강화에 기여한 공로를 인정받아 산업통상자원부 국가기술표준원은 13일 서울 강남구 코엑스에서 '제45회 국가품질경영대회'를 개최하고 품질 경영 활동으로 탁월한 성과를 창출한 유공자와 기업에게 포상을 수여했다.\n",
            "\n",
            "id: 360197501\n",
            "  label: 중국 베이징에서 개최된 태양광용 폴리실리콘 반덤핑 일몰재심 공청회에서 산업통상자원부가 참석하여 국에 한국산 태양광용 폴리실리콘 반덤핑 관세 부과 종료를 공식 요청하였으며 관세 부과 조치를 종료하더라도 덤핑이 재발될 우려가 없다는 의견을 제시했다.\n",
            "  prediction: 산업통상자원부는 한국산 태양광용 폴리실리콘 반덤핑 관세 부과 종료를 공식 요청했으며 중국 태양광 산업에서 고품질 한국 제품에 대한 수요가 증가하고 있다는 점을 증명, 중국 수요 산업 소재조달 문제를 방지하고 양국 산업 공동발전을 위한 중국 상무부의 합리적 판정을 촉구했다고 18일 밝혔다..  산업부 관계자는 중국 정부에 양국간 교역에 미치는 부정 영향을 최소화 할 수 있도록 협조해 줄 것을 당부했다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c0vaEPGgNPO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Reference.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}