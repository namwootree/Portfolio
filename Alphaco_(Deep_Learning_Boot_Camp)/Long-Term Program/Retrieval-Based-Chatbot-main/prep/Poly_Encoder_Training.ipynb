{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Poly Encoder Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOXtnmQ2BeGTpnquIWJMuVS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dotsnangles/Retrieval-Based-Chatbot/blob/main/Poly_Encoder_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXufVK0iRN3Z",
        "outputId": "6cc168f3-074c-4c01-f22c-f39cac23e285"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnEJHV_6YR4k",
        "outputId": "6957d09b-3787-43d8-b237-68aa77bb1625"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jun 25 05:11:06 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets folium==0.2.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-ZyBZPWfX5m",
        "outputId": "da80646d-ca94-4fe3-c705-705d24be0ec9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 7.0 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n",
            "\u001b[K     |████████████████████████████████| 362 kB 71.7 MB/s \n",
            "\u001b[?25hCollecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 67.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 81.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 75.9 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 65.6 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 77.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 98.3 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 71.5 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=9107e71d104de36832affa5577c2625d1b4039e764d0865239a30e2ae8c18480\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, pyyaml, fsspec, aiohttp, xxhash, tokenizers, responses, huggingface-hub, transformers, folium, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.3.2 folium-0.2.1 frozenlist-1.3.0 fsspec-2022.5.0 huggingface-hub-0.8.1 multidict-6.0.2 pyyaml-6.0 responses-0.18.0 tokenizers-0.12.1 transformers-4.20.1 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4lrYnocYNH4",
        "outputId": "1e109826-6c53-4e01-adc8-6ca1f1b1f9e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Poly-Encoder'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 77 (delta 8), reused 14 (delta 5), pack-reused 57\u001b[K\n",
            "Unpacking objects: 100% (77/77), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dotsnangles/Poly-Encoder.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2020_02_20/all_bert_models.zip\n",
        "!unzip /content/all_bert_models.zip -d /content/all_bert_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCM3W9NdYfyd",
        "outputId": "0ca6e4c9-363a-4b5a-c37e-d6f8e7480b0d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-25 05:11:22--  https://storage.googleapis.com/bert_models/2020_02_20/all_bert_models.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.195.128, 172.253.117.128, 142.250.99.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.195.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2941379946 (2.7G) [application/zip]\n",
            "Saving to: ‘all_bert_models.zip’\n",
            "\n",
            "all_bert_models.zip 100%[===================>]   2.74G   133MB/s    in 20s     \n",
            "\n",
            "2022-06-25 05:11:42 (143 MB/s) - ‘all_bert_models.zip’ saved [2941379946/2941379946]\n",
            "\n",
            "Archive:  /content/all_bert_models.zip\n",
            " extracting: /content/all_bert_models/uncased_L-10_H-128_A-2.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-10_H-256_A-4.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-10_H-512_A-8.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-10_H-768_A-12.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-12_H-128_A-2.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-12_H-256_A-4.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-12_H-512_A-8.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-12_H-768_A-12.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-2_H-128_A-2.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-2_H-256_A-4.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-2_H-512_A-8.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-2_H-768_A-12.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-4_H-128_A-2.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-4_H-256_A-4.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-4_H-512_A-8.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-4_H-768_A-12.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-6_H-128_A-2.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-6_H-256_A-4.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-6_H-512_A-8.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-6_H-768_A-12.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-8_H-128_A-2.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-8_H-256_A-4.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-8_H-512_A-8.zip  \n",
            " extracting: /content/all_bert_models/uncased_L-8_H-768_A-12.zip  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/all_bert_models/uncased_L-4_H-512_A-8.zip -d /content/Poly-Encoder/bert_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIYCevHedHxf",
        "outputId": "6f669105-1435-4a23-d9f0-df81dbd2e4db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/all_bert_models/uncased_L-4_H-512_A-8.zip\n",
            "  inflating: /content/Poly-Encoder/bert_model/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: /content/Poly-Encoder/bert_model/bert_config.json  \n",
            "  inflating: /content/Poly-Encoder/bert_model/vocab.txt  \n",
            "  inflating: /content/Poly-Encoder/bert_model/bert_model.ckpt.index  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder 1uC1pSCrh9xlieF60z78QuCg7OxvU9kUs\n",
        "!mv /content/dstc7/*.json /content/Poly-Encoder/dstc7\n",
        "!mv /content/dstc7/*.tsv /content/Poly-Encoder/dstc7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edrW8JmxZWrb",
        "outputId": "d8a0fb54-faad-47bf-d8c8-0112b56d12ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder list\n",
            "Processing file 1YXsj5U-P1nj3ID2ni62vvW4m_4AMYh_v ubuntu_dev_subtask_1.json\n",
            "Processing file 1I46tBMSYrDCFb0UoequBSl4Uqo0GOVms ubuntu_responses_subtask_1.tsv\n",
            "Processing file 1tHAe_WGFqQUHmdM28RttlVsYukL69qtg ubuntu_test_subtask_1.json\n",
            "Processing file 1s2Fz0wQD-YL0tR14PmgdYGuuI7pLarFS ubuntu_train_subtask_1.json\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YXsj5U-P1nj3ID2ni62vvW4m_4AMYh_v\n",
            "To: /content/dstc7/ubuntu_dev_subtask_1.json\n",
            "100% 92.8M/92.8M [00:00<00:00, 214MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1I46tBMSYrDCFb0UoequBSl4Uqo0GOVms\n",
            "To: /content/dstc7/ubuntu_responses_subtask_1.tsv\n",
            "100% 101k/101k [00:00<00:00, 83.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tHAe_WGFqQUHmdM28RttlVsYukL69qtg\n",
            "To: /content/dstc7/ubuntu_test_subtask_1.json\n",
            "100% 18.4M/18.4M [00:00<00:00, 98.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1s2Fz0wQD-YL0tR14PmgdYGuuI7pLarFS\n",
            "To: /content/dstc7/ubuntu_train_subtask_1.json\n",
            "100% 1.86G/1.86G [00:08<00:00, 211MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Poly-Encoder/bert_model\n",
        "!bash run.sh\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6xv5gdygBZw",
        "outputId": "65d620c1-0208-4795-9dcd-a34c2eb2e60c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Poly-Encoder/bert_model\n",
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "Converting TensorFlow checkpoint from /content/Poly-Encoder/bert_model/bert_model.ckpt\n",
            "Loading TF weight bert/embeddings/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/embeddings/position_embeddings with shape [512, 512]\n",
            "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 512]\n",
            "Loading TF weight bert/embeddings/word_embeddings with shape [30522, 512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [2048]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [512, 2048]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [2048, 512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [2048]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [512, 2048]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [2048, 512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [2048]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [512, 2048]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [2048, 512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [512, 512]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [2048]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [512, 2048]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [512]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [2048, 512]\n",
            "Loading TF weight bert/pooler/dense/bias with shape [512]\n",
            "Loading TF weight bert/pooler/dense/kernel with shape [512, 512]\n",
            "Loading TF weight cls/predictions/output_bias with shape [30522]\n",
            "Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [512]\n",
            "Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [512]\n",
            "Loading TF weight cls/predictions/transform/dense/bias with shape [512]\n",
            "Loading TF weight cls/predictions/transform/dense/kernel with shape [512, 512]\n",
            "Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
            "Loading TF weight cls/seq_relationship/output_weights with shape [2, 512]\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
            "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n",
            "Save PyTorch model to .//pytorch_model.bin\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Poly-Encoder/dstc7\n",
        "!bash parse.sh\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGQaVPejhcsG",
        "outputId": "85dd43fe-754a-46e2-ea87-800d14458401"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Poly-Encoder/dstc7\n",
            "tcmalloc: large alloc 1859141632 bytes == 0x246e000 @  0x7fa82b4761e7 0x4a3940 0x5b438c 0x5ea94f 0x5939cb 0x594cd3 0x5d0ecb 0x5939af 0x594cd3 0x594f8e 0x59526e 0x5bfba0 0x59aeca 0x515655 0x549e0e 0x593fce 0x548ae9 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7fa82b073c87 0x5b621a\n",
            "tcmalloc: large alloc 1859141632 bytes == 0x71172000 @  0x7fa82b4761e7 0x4a3940 0x52ab72 0x527cf3 0x51d358 0x59358d 0x548c51 0x51566f 0x549576 0x4bcb19 0x59c019 0x59588e 0x595e64 0x4d8924 0x5bfbcb 0x59aeca 0x515655 0x549e0e 0x593fce 0x548ae9 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7fa82b073c87 0x5b621a\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Poly-Encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrQapiRNjwOU",
        "outputId": "46503449-8c71-4002-d38a-23f22665ba5c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Poly-Encoder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --bert_model bert_model/ --output_dir output_dstc7/ --train_dir dstc7/ --use_pretrain --architecture bi\n",
        "# !python3 run.py --bert_model bert_model/ --output_dir output_dstc7/ --train_dir dstc7/ --use_pretrain --architecture poly --poly_m 16\n",
        "# !python3 run.py --bert_model bert_model/ --output_dir output_dstc7/ --train_dir dstc7/ --use_pretrain --architecture cross"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch5QnDV6jVZj",
        "outputId": "c1d7e196-901f-43fb-fb44-e9c2560eaf56"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(adam_epsilon=1e-08, architecture='bi', bert_model='bert_model/', eval=False, eval_batch_size=32, fp16=False, fp16_opt_level='O1', gpu=0, gradient_accumulation_steps=1, learning_rate=5e-05, max_contexts_length=128, max_grad_norm=1.0, max_response_length=32, model_type='bert', num_train_epochs=10.0, output_dir='output_dstc7/', poly_m=0, print_freq=100, seed=12345, train_batch_size=32, train_dir='dstc7/', use_pretrain=True, warmup_steps=100, weight_decay=0.01)\n",
            "================================================================================\n",
            "Train dir: dstc7/\n",
            "Output dir: output_dstc7/\n",
            "================================================================================\n",
            "Loading parameters from bert_model/pytorch_model.bin\n",
            "Some weights of the model checkpoint at bert_model/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "Print freq: 100 Eval freq: 1000\n",
            "  3% 100/3125 [00:18<09:20,  5.40it/s]100 13.571730039119721\n",
            "  6% 200/3125 [00:36<08:52,  5.49it/s]200 8.412441533803939\n",
            " 10% 300/3125 [00:54<08:31,  5.53it/s]300 6.545246367454529\n",
            " 13% 400/3125 [01:11<08:06,  5.60it/s]400 5.58800742983818\n",
            " 16% 500/3125 [01:29<07:44,  5.65it/s]500 4.99717125916481\n",
            " 19% 600/3125 [01:47<07:34,  5.56it/s]600 4.60038396179676\n",
            " 22% 700/3125 [02:09<07:41,  5.25it/s]700 4.294534110682351\n",
            " 26% 800/3125 [02:30<07:39,  5.05it/s]800 4.074664087295532\n",
            " 29% 900/3125 [02:51<07:29,  4.95it/s]900 3.8960051169660357\n",
            " 32% 1000/3125 [03:12<07:15,  4.88it/s]1000 3.7517764822244644\n",
            "Global Step 1000 VAL res:\n",
            " {'train_loss': 3.7517764822244644, 'eval_loss': 3.1897413954138756, 'R1': 0.332, 'R2': 0.394, 'R5': 0.515, 'R10': 0.604, 'MRR': 0.4247505026126117, 'epoch': 1, 'global_step': 1000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 35% 1100/3125 [04:03<10:04,  3.35it/s]1100 3.629265968799591\n",
            " 38% 1200/3125 [04:24<08:40,  3.70it/s]1200 3.527230738500754\n",
            " 42% 1300/3125 [04:45<07:38,  3.98it/s]1300 3.4372618936575376\n",
            " 45% 1400/3125 [05:05<06:50,  4.21it/s]1400 3.3562978317056382\n",
            " 48% 1500/3125 [05:26<06:10,  4.38it/s]1500 3.2914089992841085\n",
            " 51% 1600/3125 [05:46<05:36,  4.53it/s]1600 3.2306373140960933\n",
            " 54% 1700/3125 [06:07<05:08,  4.62it/s]1700 3.1751634369176975\n",
            " 58% 1800/3125 [06:27<04:42,  4.70it/s]1800 3.124394140574667\n",
            " 61% 1900/3125 [06:48<04:16,  4.77it/s]1900 3.0794818097039274\n",
            " 64% 2000/3125 [07:08<03:53,  4.82it/s]2000 3.0365415125489235\n",
            "Global Step 2000 VAL res:\n",
            " {'train_loss': 3.0365415125489235, 'eval_loss': 3.0139527767896652, 'R1': 0.352, 'R2': 0.439, 'R5': 0.551, 'R10': 0.648, 'MRR': 0.45296734095592733, 'epoch': 1, 'global_step': 2000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 67% 2100/3125 [07:59<05:05,  3.36it/s]2100 2.993749990633556\n",
            " 70% 2200/3125 [08:19<04:08,  3.73it/s]2200 2.9576360243017024\n",
            " 74% 2300/3125 [08:38<03:24,  4.04it/s]2300 2.9255412557850713\n",
            " 77% 2400/3125 [08:58<02:48,  4.30it/s]2400 2.8954501922925315\n",
            " 80% 2500/3125 [09:18<02:19,  4.49it/s]2500 2.866195161151886\n",
            " 83% 2600/3125 [09:38<01:53,  4.64it/s]2600 2.8381727487307327\n",
            " 86% 2700/3125 [09:58<01:29,  4.75it/s]2700 2.8134382242626614\n",
            " 90% 2800/3125 [10:18<01:07,  4.83it/s]2800 2.789573505776269\n",
            " 93% 2900/3125 [10:38<00:46,  4.88it/s]2900 2.7643370888150973\n",
            " 96% 3000/3125 [10:58<00:25,  4.92it/s]3000 2.740389097968737\n",
            "Global Step 3000 VAL res:\n",
            " {'train_loss': 2.740389097968737, 'eval_loss': 2.8183560110628605, 'R1': 0.383, 'R2': 0.488, 'R5': 0.582, 'R10': 0.665, 'MRR': 0.48478330254548846, 'epoch': 1, 'global_step': 3000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 99% 3100/3125 [11:46<00:07,  3.48it/s]3100 2.719608185483563\n",
            " 99% 3100/3125 [11:51<00:05,  4.36it/s]\n",
            "Epoch 1, Global Step 3125 VAL res:\n",
            " {'train_loss': 2.7144490142822266, 'eval_loss': 2.839960180222988, 'R1': 0.376, 'R2': 0.476, 'R5': 0.586, 'R10': 0.68, 'MRR': 0.4803161471672755, 'epoch': 1, 'global_step': 3125}\n",
            "3125 2.7144490142822266\n",
            "  3% 100/3125 [00:17<08:40,  5.82it/s]3225 1.8288913512229918\n",
            "  6% 200/3125 [00:34<08:24,  5.80it/s]3325 1.8470110565423965\n",
            " 10% 300/3125 [00:53<08:35,  5.48it/s]3425 1.8322181963920594\n",
            " 13% 400/3125 [01:13<08:30,  5.34it/s]3525 1.8363841924071311\n",
            " 16% 500/3125 [01:32<08:17,  5.28it/s]3625 1.8391437394618988\n",
            " 19% 600/3125 [01:52<08:04,  5.21it/s]3725 1.8414369342724481\n",
            " 22% 700/3125 [02:11<07:48,  5.18it/s]3825 1.8409987345763614\n",
            " 26% 800/3125 [02:31<07:28,  5.18it/s]3925 1.8403005777299404\n",
            "Global Step 4000 VAL res:\n",
            " {'train_loss': 1.8406075060708182, 'eval_loss': 2.7597117573022842, 'R1': 0.384, 'R2': 0.5, 'R5': 0.615, 'R10': 0.713, 'MRR': 0.49751720944578287, 'epoch': 2, 'global_step': 4000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 29% 900/3125 [03:19<10:29,  3.54it/s]4025 1.837944534884559\n",
            " 32% 1000/3125 [03:38<09:00,  3.93it/s]4125 1.8380642393827438\n",
            " 35% 1100/3125 [03:57<07:55,  4.26it/s]4225 1.8371518615159121\n",
            " 38% 1200/3125 [04:16<07:06,  4.51it/s]4325 1.8377049753069878\n",
            " 42% 1300/3125 [04:35<06:26,  4.72it/s]4425 1.8381061975772564\n",
            " 45% 1400/3125 [04:54<05:54,  4.87it/s]4525 1.8383635889632362\n",
            " 48% 1500/3125 [05:13<05:25,  4.99it/s]4625 1.8341617086728415\n",
            " 51% 1600/3125 [05:32<04:59,  5.09it/s]4725 1.8326683957874774\n",
            " 54% 1700/3125 [05:50<04:36,  5.15it/s]4825 1.8333100030001472\n",
            " 58% 1800/3125 [06:09<04:15,  5.19it/s]4925 1.8329932808213765\n",
            "Global Step 5000 VAL res:\n",
            " {'train_loss': 1.8339720300038655, 'eval_loss': 2.7046801932156086, 'R1': 0.403, 'R2': 0.494, 'R5': 0.603, 'R10': 0.703, 'MRR': 0.502804696996922, 'epoch': 2, 'global_step': 5000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 61% 1900/3125 [06:58<05:44,  3.56it/s]5025 1.8331628084182738\n",
            " 64% 2000/3125 [07:17<04:44,  3.96it/s]5125 1.831067921578884\n",
            " 67% 2100/3125 [07:36<03:59,  4.28it/s]5225 1.8302301018578666\n",
            " 70% 2200/3125 [07:54<03:22,  4.56it/s]5325 1.8292580275643955\n",
            " 74% 2300/3125 [08:13<02:52,  4.78it/s]5425 1.8288097034329953\n",
            " 77% 2400/3125 [08:31<02:26,  4.95it/s]5525 1.8246303508182367\n",
            " 80% 2500/3125 [08:50<02:03,  5.07it/s]5625 1.8236798286914826\n",
            " 83% 2600/3125 [09:09<01:41,  5.15it/s]5725 1.8215772797969672\n",
            " 86% 2700/3125 [09:27<01:21,  5.22it/s]5825 1.8214186748751888\n",
            " 90% 2800/3125 [09:46<01:01,  5.28it/s]5925 1.8182556068045752\n",
            "Global Step 6000 VAL res:\n",
            " {'train_loss': 1.8162921385972397, 'eval_loss': 2.5889941677451134, 'R1': 0.428, 'R2': 0.516, 'R5': 0.642, 'R10': 0.727, 'MRR': 0.5273776126510793, 'epoch': 2, 'global_step': 6000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 93% 2900/3125 [10:32<01:01,  3.66it/s]6025 1.8156042013086122\n",
            " 96% 3000/3125 [10:51<00:30,  4.06it/s]6125 1.8131815887093543\n",
            " 99% 3100/3125 [11:10<00:05,  4.37it/s]6225 1.8114218735118066\n",
            " 99% 3100/3125 [11:14<00:05,  4.59it/s]\n",
            "Epoch 2, Global Step 6250 VAL res:\n",
            " {'train_loss': 1.8110347936058044, 'eval_loss': 2.629047852009535, 'R1': 0.418, 'R2': 0.509, 'R5': 0.635, 'R10': 0.725, 'MRR': 0.5206939752640921, 'epoch': 2, 'global_step': 6250}\n",
            "6250 1.8110347936058044\n",
            "  3% 100/3125 [00:17<08:39,  5.82it/s]6350 1.452624039053917\n",
            "  6% 200/3125 [00:34<08:23,  5.81it/s]6450 1.4785789528489113\n",
            " 10% 300/3125 [00:51<08:05,  5.81it/s]6550 1.4893456371625264\n",
            " 13% 400/3125 [01:08<07:48,  5.81it/s]6650 1.4886369083821773\n",
            " 16% 500/3125 [01:26<07:31,  5.81it/s]6750 1.4915720502138137\n",
            " 19% 600/3125 [01:43<07:14,  5.81it/s]6850 1.494795747101307\n",
            " 22% 700/3125 [02:00<06:57,  5.81it/s]6950 1.4931151184865405\n",
            "Global Step 7000 VAL res:\n",
            " {'train_loss': 1.4984088761806489, 'eval_loss': 2.644649613648653, 'R1': 0.427, 'R2': 0.522, 'R5': 0.645, 'R10': 0.734, 'MRR': 0.5303969419216235, 'epoch': 3, 'global_step': 7000}\n",
            " 26% 800/3125 [02:46<10:14,  3.78it/s]7050 1.495532714277506\n",
            " 29% 900/3125 [03:04<08:49,  4.20it/s]7150 1.5019147819942897\n",
            " 32% 1000/3125 [03:22<07:48,  4.54it/s]7250 1.4965590144395828\n",
            " 35% 1100/3125 [03:40<07:02,  4.80it/s]7350 1.4949256213686684\n",
            " 38% 1200/3125 [03:58<06:25,  4.99it/s]7450 1.4961259627342225\n",
            " 42% 1300/3125 [04:17<05:54,  5.14it/s]7550 1.4982410242924324\n",
            " 45% 1400/3125 [04:35<05:28,  5.25it/s]7650 1.5014920765161515\n",
            " 48% 1500/3125 [04:53<05:04,  5.33it/s]7750 1.502930943330129\n",
            " 51% 1600/3125 [05:11<04:42,  5.39it/s]7850 1.5035321112722158\n",
            " 54% 1700/3125 [05:29<04:22,  5.44it/s]7950 1.5024736006119672\n",
            "Global Step 8000 VAL res:\n",
            " {'train_loss': 1.5016257216589792, 'eval_loss': 2.5875454246997833, 'R1': 0.432, 'R2': 0.526, 'R5': 0.633, 'R10': 0.741, 'MRR': 0.5330863544237353, 'epoch': 3, 'global_step': 8000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 58% 1800/3125 [06:16<05:58,  3.69it/s]8050 1.5011884033679963\n",
            " 61% 1900/3125 [06:34<04:58,  4.11it/s]8150 1.5008487876151737\n",
            " 64% 2000/3125 [06:52<04:12,  4.45it/s]8250 1.5009860382676126\n",
            " 67% 2100/3125 [07:10<03:36,  4.74it/s]8350 1.503112175975527\n",
            " 70% 2200/3125 [07:28<03:06,  4.96it/s]8450 1.5056458005580036\n",
            " 74% 2300/3125 [07:46<02:41,  5.12it/s]8550 1.505378701194473\n",
            " 77% 2400/3125 [08:04<02:18,  5.24it/s]8650 1.5055848495165507\n",
            " 80% 2500/3125 [08:22<01:57,  5.32it/s]8750 1.5058830609083176\n",
            " 83% 2600/3125 [08:40<01:37,  5.39it/s]8850 1.506193943803127\n",
            " 86% 2700/3125 [08:58<01:18,  5.44it/s]8950 1.5059917983081605\n",
            "Global Step 9000 VAL res:\n",
            " {'train_loss': 1.5061790179122578, 'eval_loss': 2.5311031453311443, 'R1': 0.44, 'R2': 0.536, 'R5': 0.656, 'R10': 0.749, 'MRR': 0.5425868814082601, 'epoch': 3, 'global_step': 9000}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            " 90% 2800/3125 [09:45<01:27,  3.73it/s]9050 1.5053215889206955\n",
            " 93% 2900/3125 [10:03<00:54,  4.15it/s]9150 1.5047079460785306\n",
            " 96% 3000/3125 [10:20<00:27,  4.51it/s]9250 1.5045845820705095\n",
            " 99% 3100/3125 [10:38<00:05,  4.78it/s]9350 1.5047386050416578\n",
            " 99% 3100/3125 [10:43<00:05,  4.82it/s]\n",
            "Epoch 3, Global Step 9375 VAL res:\n",
            " {'train_loss': 1.5057830925559998, 'eval_loss': 2.5012777112424374, 'R1': 0.446, 'R2': 0.531, 'R5': 0.659, 'R10': 0.747, 'MRR': 0.5449461371387436, 'epoch': 3, 'global_step': 9375}\n",
            "[Saving at] output_dstc7/bi_0_pytorch_model.bin\n",
            "9375 1.5057830925559998\n",
            "  3% 100/3125 [00:17<08:39,  5.82it/s]9475 1.2384413594007493\n",
            "  6% 200/3125 [00:34<08:22,  5.82it/s]9575 1.256750482916832\n",
            " 10% 300/3125 [00:51<08:06,  5.81it/s]9675 1.2577847478787105\n",
            " 13% 400/3125 [01:08<07:48,  5.82it/s]9775 1.2670501278340816\n",
            " 16% 500/3125 [01:25<07:29,  5.83it/s]9875 1.263414671421051\n",
            " 19% 600/3125 [01:42<07:12,  5.84it/s]9975 1.2701090648770332\n",
            "Global Step 10000 VAL res:\n",
            " {'train_loss': 1.2739633338928222, 'eval_loss': 2.5367139652371407, 'R1': 0.45, 'R2': 0.54, 'R5': 0.649, 'R10': 0.737, 'MRR': 0.5475422685001902, 'epoch': 4, 'global_step': 10000}\n",
            " 22% 700/3125 [02:28<10:40,  3.79it/s]10075 1.2689562196390969\n",
            " 26% 800/3125 [02:45<09:06,  4.26it/s]10175 1.2710889033228159\n",
            " 29% 900/3125 [03:02<07:58,  4.65it/s]10275 1.2722867162360085\n",
            " 32% 1000/3125 [03:20<07:08,  4.96it/s]10375 1.2700139663219452\n",
            " 35% 1100/3125 [03:37<06:29,  5.19it/s]10475 1.2730735583738848\n",
            " 38% 1200/3125 [03:54<05:58,  5.37it/s]10575 1.272766955991586\n",
            " 42% 1300/3125 [04:11<05:32,  5.48it/s]10675 1.2744880843162536\n",
            " 45% 1400/3125 [04:29<05:10,  5.55it/s]10775 1.2748419301424707\n",
            " 48% 1500/3125 [04:46<04:50,  5.60it/s]10875 1.2751033019224802\n",
            " 51% 1600/3125 [05:04<04:30,  5.63it/s]10975 1.2729867029562592\n",
            "Global Step 11000 VAL res:\n",
            " {'train_loss': 1.2716744679671068, 'eval_loss': 2.5677478201687336, 'R1': 0.439, 'R2': 0.529, 'R5': 0.65, 'R10': 0.751, 'MRR': 0.5414607552385721, 'epoch': 4, 'global_step': 11000}\n",
            " 54% 1700/3125 [06:07<07:30,  3.17it/s]11075 1.2731183807639515\n",
            " 58% 1800/3125 [06:25<06:01,  3.67it/s]11175 1.2739216905832291\n",
            " 61% 1900/3125 [06:42<04:56,  4.13it/s]11275 1.2757430652568216\n",
            " 64% 2000/3125 [06:59<04:08,  4.52it/s]11375 1.2757830908298493\n",
            " 67% 2100/3125 [07:16<03:31,  4.85it/s]11475 1.2769624840929394\n",
            " 70% 2200/3125 [07:34<03:01,  5.09it/s]11575 1.276536101265387\n",
            " 74% 2300/3125 [07:51<02:36,  5.26it/s]11675 1.2781735611998517\n",
            " 77% 2400/3125 [08:08<02:14,  5.39it/s]11775 1.278281829232971\n",
            " 80% 2500/3125 [08:26<01:53,  5.49it/s]11875 1.2782084985256195\n",
            " 83% 2600/3125 [08:43<01:34,  5.56it/s]11975 1.2794339796671501\n",
            "Global Step 12000 VAL res:\n",
            " {'train_loss': 1.2782945064363025, 'eval_loss': 2.512604508548975, 'R1': 0.462, 'R2': 0.55, 'R5': 0.657, 'R10': 0.755, 'MRR': 0.5589844256622255, 'epoch': 4, 'global_step': 12000}\n",
            " 86% 2700/3125 [09:47<02:14,  3.16it/s]12075 1.2795204982934174\n",
            " 90% 2800/3125 [10:04<01:28,  3.65it/s]12175 1.2804205776325295\n",
            " 93% 2900/3125 [10:22<00:54,  4.09it/s]12275 1.279259900985093\n",
            " 96% 3000/3125 [10:39<00:27,  4.48it/s]12375 1.2796054047544798\n",
            " 99% 3100/3125 [10:57<00:05,  4.80it/s]12475 1.2802593556142623\n",
            " 99% 3100/3125 [11:01<00:05,  4.69it/s]\n",
            "Epoch 4, Global Step 12500 VAL res:\n",
            " {'train_loss': 1.2799328974342346, 'eval_loss': 2.5640233494341373, 'R1': 0.449, 'R2': 0.553, 'R5': 0.668, 'R10': 0.769, 'MRR': 0.5540865835214488, 'epoch': 4, 'global_step': 12500}\n",
            "12500 1.2799328974342346\n",
            "  3% 100/3125 [00:25<13:00,  3.88it/s]12600 1.0426703932881356\n",
            "  6% 200/3125 [01:02<15:41,  3.11it/s]12700 1.065064624994993\n",
            " 10% 300/3125 [01:39<16:07,  2.92it/s]12800 1.0767946476737658\n",
            " 13% 400/3125 [02:15<15:57,  2.85it/s]12900 1.0817058730870486\n",
            " 16% 500/3125 [02:52<15:36,  2.80it/s]13000 1.093292308986187\n",
            "Global Step 13000 VAL res:\n",
            " {'train_loss': 1.093292308986187, 'eval_loss': 2.6533002257347107, 'R1': 0.437, 'R2': 0.532, 'R5': 0.642, 'R10': 0.752, 'MRR': 0.5402743119138136, 'epoch': 5, 'global_step': 13000}\n",
            " 19% 600/3125 [04:15<21:52,  1.92it/s]13100 1.0885800962150096\n",
            " 22% 700/3125 [04:52<19:00,  2.13it/s]13200 1.0854058103050503\n",
            " 26% 800/3125 [05:29<16:56,  2.29it/s]13300 1.0873417284339666\n",
            " 29% 900/3125 [06:06<15:23,  2.41it/s]13400 1.0827624198463228\n",
            " 32% 1000/3125 [06:42<14:09,  2.50it/s]13500 1.08257623898983\n",
            " 35% 1100/3125 [07:19<13:08,  2.57it/s]13600 1.0870216863805597\n",
            " 38% 1200/3125 [07:55<12:15,  2.62it/s]13700 1.0873521996289492\n",
            " 42% 1300/3125 [08:32<11:28,  2.65it/s]13800 1.0875795812560962\n",
            " 45% 1400/3125 [09:08<10:44,  2.68it/s]13900 1.0880316043751581\n",
            " 48% 1500/3125 [09:45<10:02,  2.69it/s]14000 1.0857946106592815\n",
            "Global Step 14000 VAL res:\n",
            " {'train_loss': 1.0857946106592815, 'eval_loss': 2.618437595665455, 'R1': 0.453, 'R2': 0.553, 'R5': 0.66, 'R10': 0.763, 'MRR': 0.5560851618062607, 'epoch': 5, 'global_step': 14000}\n",
            " 51% 1600/3125 [11:08<12:58,  1.96it/s]14100 1.086156267747283\n",
            " 54% 1700/3125 [11:45<11:05,  2.14it/s]14200 1.0885298241236632\n",
            " 58% 1800/3125 [12:22<09:38,  2.29it/s]14300 1.087940952612294\n",
            " 61% 1900/3125 [12:58<08:28,  2.41it/s]14400 1.0862214068362588\n",
            " 64% 2000/3125 [13:35<07:30,  2.50it/s]14500 1.08853822812438\n",
            " 67% 2100/3125 [14:11<06:39,  2.57it/s]14600 1.0901793998196012\n",
            " 70% 2200/3125 [14:47<05:52,  2.62it/s]14700 1.0931038191372697\n",
            " 74% 2300/3125 [15:24<05:10,  2.66it/s]14800 1.092560143561467\n",
            " 77% 2400/3125 [16:00<04:29,  2.69it/s]14900 1.0935731287673116\n",
            " 80% 2500/3125 [16:37<03:51,  2.70it/s]15000 1.0934097954154014\n",
            "Global Step 15000 VAL res:\n",
            " {'train_loss': 1.0934097954154014, 'eval_loss': 2.5642396360635757, 'R1': 0.458, 'R2': 0.556, 'R5': 0.675, 'R10': 0.769, 'MRR': 0.5606416990264372, 'epoch': 5, 'global_step': 15000}\n",
            " 83% 2600/3125 [18:00<04:26,  1.97it/s]15100 1.094038149588383\n",
            " 86% 2700/3125 [18:36<03:17,  2.15it/s]15200 1.0943820561854927\n",
            " 90% 2800/3125 [19:12<02:21,  2.30it/s]15300 1.096044352427125\n",
            " 93% 2900/3125 [19:49<01:32,  2.42it/s]15400 1.096905066185984\n",
            " 96% 3000/3125 [20:25<00:49,  2.51it/s]15500 1.0987328791419666\n",
            " 99% 3100/3125 [21:02<00:09,  2.57it/s]15600 1.0977756217025942\n",
            " 99% 3100/3125 [21:11<00:10,  2.44it/s]\n",
            "Epoch 5, Global Step 15625 VAL res:\n",
            " {'train_loss': 1.0970307064819336, 'eval_loss': 2.6434300541877747, 'R1': 0.442, 'R2': 0.545, 'R5': 0.67, 'R10': 0.763, 'MRR': 0.549863578880342, 'epoch': 5, 'global_step': 15625}\n",
            "15625 1.0970307064819336\n",
            "  3% 100/3125 [00:17<08:44,  5.76it/s]15725 0.9213831168413162\n",
            "  6% 200/3125 [00:34<08:25,  5.78it/s]15825 0.9090042369067669\n",
            " 10% 300/3125 [00:51<08:07,  5.79it/s]15925 0.9047710396846136\n",
            "Global Step 16000 VAL res:\n",
            " {'train_loss': 0.9234492330551147, 'eval_loss': 2.7245793975889683, 'R1': 0.444, 'R2': 0.553, 'R5': 0.667, 'R10': 0.762, 'MRR': 0.5517525587451048, 'epoch': 6, 'global_step': 16000}\n",
            " 13% 400/3125 [01:37<12:55,  3.51it/s]16025 0.9219388435781002\n",
            " 16% 500/3125 [01:54<10:40,  4.10it/s]16125 0.9279756587743759\n",
            " 19% 600/3125 [02:20<10:26,  4.03it/s]16225 0.9223166214923064\n",
            " 22% 700/3125 [02:56<11:32,  3.50it/s]16325 0.9226629287430218\n",
            " 26% 800/3125 [03:32<12:01,  3.22it/s]16425 0.9253624822944403\n",
            " 29% 900/3125 [04:09<12:07,  3.06it/s]16525 0.9301705513066716\n",
            " 32% 1000/3125 [04:45<11:58,  2.96it/s]16625 0.9318916165530682\n",
            " 35% 1100/3125 [05:21<11:39,  2.89it/s]16725 0.937273508229039\n",
            " 38% 1200/3125 [05:57<11:14,  2.86it/s]16825 0.9406124855329593\n",
            " 42% 1300/3125 [06:34<10:45,  2.83it/s]16925 0.9384588572382927\n",
            "Global Step 17000 VAL res:\n",
            " {'train_loss': 0.9411837715452368, 'eval_loss': 2.6539364233613014, 'R1': 0.45, 'R2': 0.542, 'R5': 0.67, 'R10': 0.756, 'MRR': 0.5522921705325025, 'epoch': 6, 'global_step': 17000}\n",
            " 45% 1400/3125 [07:56<14:17,  2.01it/s]17025 0.9404975658016546\n",
            " 48% 1500/3125 [08:33<12:22,  2.19it/s]17125 0.9427474803725878\n",
            " 51% 1600/3125 [09:09<10:54,  2.33it/s]17225 0.9439093902520835\n",
            " 54% 1700/3125 [09:46<09:43,  2.44it/s]17325 0.9447338905404595\n",
            " 58% 1800/3125 [10:22<08:44,  2.53it/s]17425 0.9443266803357336\n",
            " 61% 1900/3125 [10:58<07:52,  2.59it/s]17525 0.9419140647587023\n",
            " 64% 2000/3125 [11:35<07:06,  2.64it/s]17625 0.941900680795312\n",
            " 67% 2100/3125 [12:11<06:23,  2.67it/s]17725 0.942106067722752\n",
            " 70% 2200/3125 [12:47<05:43,  2.70it/s]17825 0.9433412242342125\n",
            " 74% 2300/3125 [13:24<05:04,  2.71it/s]17925 0.9433859643728837\n",
            "Global Step 18000 VAL res:\n",
            " {'train_loss': 0.9424749270363858, 'eval_loss': 2.651536598801613, 'R1': 0.45, 'R2': 0.55, 'R5': 0.663, 'R10': 0.763, 'MRR': 0.5529258510179088, 'epoch': 6, 'global_step': 18000}\n",
            " 77% 2400/3125 [14:47<06:08,  1.97it/s]18025 0.9429778454701105\n",
            " 80% 2500/3125 [15:23<04:50,  2.15it/s]18125 0.9447180144906044\n",
            " 83% 2600/3125 [16:00<03:49,  2.29it/s]18225 0.9460131429708921\n",
            " 86% 2700/3125 [16:37<02:57,  2.40it/s]18325 0.9457773772875467\n",
            " 90% 2800/3125 [17:14<02:10,  2.49it/s]18425 0.9451192709271397\n",
            " 93% 2900/3125 [17:50<01:27,  2.57it/s]18525 0.9461683512350608\n",
            " 96% 3000/3125 [18:26<00:47,  2.62it/s]18625 0.9466289300620556\n",
            " 99% 3100/3125 [19:03<00:09,  2.66it/s]18725 0.9446446378673277\n",
            " 99% 3100/3125 [19:12<00:09,  2.69it/s]\n",
            "Epoch 6, Global Step 18750 VAL res:\n",
            " {'train_loss': 0.9446950907325745, 'eval_loss': 2.6987198777496815, 'R1': 0.444, 'R2': 0.553, 'R5': 0.669, 'R10': 0.761, 'MRR': 0.552264513800579, 'epoch': 6, 'global_step': 18750}\n",
            "18750 0.9446950907325745\n",
            "  3% 100/3125 [00:17<08:44,  5.77it/s]18850 0.8173256531357765\n",
            "  6% 200/3125 [00:34<08:26,  5.78it/s]18950 0.7945162980258464\n",
            "Global Step 19000 VAL res:\n",
            " {'train_loss': 0.7924001857042312, 'eval_loss': 2.83044021576643, 'R1': 0.454, 'R2': 0.559, 'R5': 0.677, 'R10': 0.758, 'MRR': 0.5589140718448545, 'epoch': 7, 'global_step': 19000}\n",
            " 10% 300/3125 [01:20<14:14,  3.30it/s]19050 0.8142290976643562\n",
            " 13% 400/3125 [01:37<11:24,  3.98it/s]19150 0.8142582651227712\n",
            " 16% 500/3125 [01:54<09:43,  4.50it/s]19250 0.8164506836533546\n",
            " 19% 600/3125 [02:11<08:38,  4.87it/s]19350 0.811861567646265\n",
            " 22% 700/3125 [02:29<07:52,  5.13it/s]19450 0.8114086025527545\n",
            " 26% 800/3125 [02:46<07:16,  5.32it/s]19550 0.8148196480795741\n",
            " 29% 900/3125 [03:03<06:47,  5.46it/s]19650 0.8140432113409042\n",
            " 32% 1000/3125 [03:20<06:21,  5.57it/s]19750 0.8183219436705113\n",
            " 35% 1100/3125 [03:38<05:58,  5.65it/s]19850 0.8216712676665999\n",
            " 38% 1200/3125 [03:55<05:37,  5.70it/s]19950 0.8189192420989275\n",
            "Global Step 20000 VAL res:\n",
            " {'train_loss': 0.8178735256671905, 'eval_loss': 2.828763298690319, 'R1': 0.436, 'R2': 0.531, 'R5': 0.659, 'R10': 0.755, 'MRR': 0.5428527916199362, 'epoch': 7, 'global_step': 20000}\n",
            " 42% 1300/3125 [04:40<07:54,  3.85it/s]20050 0.8185934304503294\n",
            " 45% 1400/3125 [04:57<06:42,  4.29it/s]20150 0.8186465063478265\n",
            " 48% 1500/3125 [05:14<05:48,  4.66it/s]20250 0.8187982922991117\n",
            " 51% 1600/3125 [05:32<05:07,  4.95it/s]20350 0.8215401950851082\n",
            " 54% 1700/3125 [05:49<04:35,  5.18it/s]20450 0.822302146887078\n",
            " 58% 1800/3125 [06:06<04:07,  5.35it/s]20550 0.823263415131304\n",
            " 61% 1900/3125 [06:23<03:43,  5.49it/s]20650 0.8230842918941849\n",
            " 64% 2000/3125 [06:41<03:21,  5.58it/s]20750 0.8246645529717207\n",
            " 67% 2100/3125 [06:58<03:01,  5.64it/s]20850 0.8247005693969273\n",
            " 70% 2200/3125 [07:15<02:42,  5.68it/s]20950 0.823304771469398\n",
            "Global Step 21000 VAL res:\n",
            " {'train_loss': 0.8236782823271221, 'eval_loss': 2.8200533129274845, 'R1': 0.449, 'R2': 0.546, 'R5': 0.666, 'R10': 0.761, 'MRR': 0.5527509744534357, 'epoch': 7, 'global_step': 21000}\n",
            " 74% 2300/3125 [08:01<03:34,  3.84it/s]21050 0.8223252689708834\n",
            " 77% 2400/3125 [08:18<02:49,  4.28it/s]21150 0.8235543167777359\n",
            " 80% 2500/3125 [08:35<02:14,  4.64it/s]21250 0.8254783295452595\n",
            " 83% 2600/3125 [08:53<01:46,  4.94it/s]21350 0.825699149410312\n",
            " 86% 2700/3125 [09:10<01:22,  5.17it/s]21450 0.8266570829517311\n",
            " 90% 2800/3125 [09:27<01:00,  5.34it/s]21550 0.8259982562437653\n",
            " 93% 2900/3125 [09:44<00:41,  5.47it/s]21650 0.8264562404618181\n",
            " 96% 3000/3125 [10:01<00:22,  5.57it/s]21750 0.8248420793960491\n",
            " 99% 3100/3125 [10:19<00:04,  5.65it/s]21850 0.824395208930777\n",
            " 99% 3100/3125 [10:26<00:05,  4.95it/s]\n",
            "Epoch 7, Global Step 21875 VAL res:\n",
            " {'train_loss': 0.8241346987867355, 'eval_loss': 2.788484498858452, 'R1': 0.45, 'R2': 0.545, 'R5': 0.674, 'R10': 0.761, 'MRR': 0.5531627287336376, 'epoch': 7, 'global_step': 21875}\n",
            "21875 0.8241346987867355\n",
            "  3% 100/3125 [00:17<08:38,  5.83it/s]21975 0.7110405802726746\n",
            "Global Step 22000 VAL res:\n",
            " {'train_loss': 0.7147826173305512, 'eval_loss': 2.89245617762208, 'R1': 0.45, 'R2': 0.542, 'R5': 0.675, 'R10': 0.759, 'MRR': 0.5527249574439208, 'epoch': 8, 'global_step': 22000}\n",
            "  6% 200/3125 [01:02<16:33,  2.94it/s]22075 0.7147904146462679\n",
            " 10% 300/3125 [01:20<12:23,  3.80it/s]22175 0.7147325709958872\n",
            " 13% 400/3125 [01:37<10:18,  4.40it/s]22275 0.7119607996568084\n",
            " 16% 500/3125 [01:54<09:03,  4.83it/s]22375 0.7136193951070309\n",
            " 19% 600/3125 [02:11<08:12,  5.12it/s]22475 0.7080739147712787\n",
            " 22% 700/3125 [02:28<07:35,  5.32it/s]22575 0.7075166644155979\n",
            " 26% 800/3125 [02:46<07:04,  5.47it/s]22675 0.7146254455856978\n",
            " 29% 900/3125 [03:03<06:39,  5.57it/s]22775 0.7117848717338509\n",
            " 32% 1000/3125 [03:23<06:33,  5.40it/s]22875 0.7102149451822043\n",
            " 35% 1100/3125 [03:58<08:01,  4.21it/s]22975 0.710133453390815\n",
            "Global Step 23000 VAL res:\n",
            " {'train_loss': 0.7100695365269979, 'eval_loss': 2.917872991412878, 'R1': 0.445, 'R2': 0.54, 'R5': 0.662, 'R10': 0.759, 'MRR': 0.54783969471416, 'epoch': 8, 'global_step': 23000}\n",
            " 38% 1200/3125 [05:20<13:18,  2.41it/s]23075 0.7090114247053861\n",
            " 42% 1300/3125 [05:56<12:04,  2.52it/s]23175 0.7129026826069905\n",
            " 45% 1400/3125 [06:32<11:04,  2.60it/s]23275 0.7150563893999372\n",
            " 48% 1500/3125 [07:07<10:11,  2.66it/s]23375 0.7163335292140642\n",
            " 51% 1600/3125 [07:43<09:24,  2.70it/s]23475 0.7185324856825173\n",
            " 54% 1700/3125 [08:19<08:42,  2.73it/s]23575 0.7198074789082303\n",
            " 58% 1800/3125 [08:54<08:00,  2.76it/s]23675 0.7204153930644194\n",
            " 61% 1900/3125 [09:30<07:21,  2.77it/s]23775 0.7195149657757659\n",
            " 64% 2000/3125 [10:05<06:44,  2.78it/s]23875 0.7211303498446942\n",
            " 67% 2100/3125 [10:41<06:07,  2.79it/s]23975 0.7200163867218153\n",
            "Global Step 24000 VAL res:\n",
            " {'train_loss': 0.7209596454956952, 'eval_loss': 2.895393617451191, 'R1': 0.448, 'R2': 0.539, 'R5': 0.664, 'R10': 0.759, 'MRR': 0.5500127447199256, 'epoch': 8, 'global_step': 24000}\n",
            " 70% 2200/3125 [12:02<07:37,  2.02it/s]24075 0.7203642749650911\n",
            " 74% 2300/3125 [12:38<06:13,  2.21it/s]24175 0.7190109234141266\n",
            " 77% 2400/3125 [13:13<05:06,  2.36it/s]24275 0.719579395763576\n",
            " 80% 2500/3125 [13:49<04:12,  2.48it/s]24375 0.720150669157505\n",
            " 83% 2600/3125 [14:24<03:24,  2.57it/s]24475 0.7199541236460208\n",
            " 86% 2700/3125 [15:00<02:41,  2.64it/s]24575 0.7218515161562848\n",
            " 90% 2800/3125 [15:35<02:00,  2.69it/s]24675 0.7228211409598589\n",
            " 93% 2900/3125 [16:11<01:22,  2.73it/s]24775 0.7224804665199641\n",
            " 96% 3000/3125 [16:46<00:45,  2.76it/s]24875 0.7223607066969077\n",
            " 99% 3100/3125 [17:22<00:09,  2.77it/s]24975 0.7229621065239753\n",
            "Global Step 25000 VAL res:\n",
            " {'train_loss': 0.7227404034996032, 'eval_loss': 2.8816743940114975, 'R1': 0.446, 'R2': 0.541, 'R5': 0.668, 'R10': 0.76, 'MRR': 0.5497908587824284, 'epoch': 8, 'global_step': 25000}\n",
            " 99% 3100/3125 [18:15<00:08,  2.83it/s]\n",
            "Epoch 8, Global Step 25000 VAL res:\n",
            " {'train_loss': 0.7227404034996032, 'eval_loss': 2.8816743940114975, 'R1': 0.446, 'R2': 0.541, 'R5': 0.668, 'R10': 0.76, 'MRR': 0.5497908587824284, 'epoch': 8, 'global_step': 25000}\n",
            "25000 0.7227404034996032\n",
            "  3% 100/3125 [00:17<08:40,  5.81it/s]25100 0.6503307105600834\n",
            "  6% 200/3125 [00:34<08:23,  5.81it/s]25200 0.6480720023065806\n",
            " 10% 300/3125 [00:51<08:06,  5.80it/s]25300 0.6341565450529257\n",
            " 13% 400/3125 [01:08<07:49,  5.80it/s]25400 0.6408446283265948\n",
            " 16% 500/3125 [01:26<07:33,  5.79it/s]25500 0.6380901338458062\n",
            " 19% 600/3125 [01:43<07:15,  5.79it/s]25600 0.6402920637528101\n",
            " 22% 700/3125 [02:00<06:58,  5.79it/s]25700 0.6365106286747115\n",
            " 26% 800/3125 [02:18<06:41,  5.79it/s]25800 0.6343621853552759\n",
            " 29% 900/3125 [02:35<06:24,  5.79it/s]25900 0.6385246192581123\n",
            " 32% 1000/3125 [02:52<06:06,  5.79it/s]26000 0.6373311235159635\n",
            "Global Step 26000 VAL res:\n",
            " {'train_loss': 0.6373311235159635, 'eval_loss': 3.0064191669225693, 'R1': 0.445, 'R2': 0.536, 'R5': 0.658, 'R10': 0.749, 'MRR': 0.5467153328637989, 'epoch': 9, 'global_step': 26000}\n",
            " 35% 1100/3125 [03:38<08:44,  3.86it/s]26100 0.6373978441140868\n",
            " 38% 1200/3125 [03:55<07:27,  4.30it/s]26200 0.6366416204099854\n",
            " 42% 1300/3125 [04:12<06:31,  4.66it/s]26300 0.6364699234297643\n",
            " 45% 1400/3125 [04:29<05:48,  4.96it/s]26400 0.6370656530559063\n",
            " 48% 1500/3125 [04:47<05:14,  5.16it/s]26500 0.6387195624113083\n",
            " 51% 1600/3125 [05:04<04:45,  5.35it/s]26600 0.6399385653622448\n",
            " 54% 1700/3125 [05:21<04:20,  5.48it/s]26700 0.6418255983556018\n",
            " 58% 1800/3125 [05:38<03:57,  5.58it/s]26800 0.6432476583123207\n",
            " 61% 1900/3125 [05:56<03:37,  5.64it/s]26900 0.6458452135873468\n",
            " 64% 2000/3125 [06:13<03:17,  5.69it/s]27000 0.6458015943616628\n",
            "Global Step 27000 VAL res:\n",
            " {'train_loss': 0.6458015943616628, 'eval_loss': 2.952001206576824, 'R1': 0.442, 'R2': 0.538, 'R5': 0.663, 'R10': 0.754, 'MRR': 0.5470657110470124, 'epoch': 9, 'global_step': 27000}\n",
            " 67% 2100/3125 [06:58<04:26,  3.85it/s]27100 0.6468184405423346\n",
            " 70% 2200/3125 [07:16<03:36,  4.27it/s]27200 0.6458258866654201\n",
            " 74% 2300/3125 [07:33<02:57,  4.64it/s]27300 0.6454125329009864\n",
            " 77% 2400/3125 [07:50<02:27,  4.92it/s]27400 0.6465914012057086\n",
            " 80% 2500/3125 [08:08<02:01,  5.14it/s]27500 0.6474424201905727\n",
            " 83% 2600/3125 [08:25<01:38,  5.31it/s]27600 0.647652550265193\n",
            " 86% 2700/3125 [08:43<01:18,  5.45it/s]27700 0.6477844225532479\n",
            " 90% 2800/3125 [09:00<00:58,  5.54it/s]27800 0.6489547856205277\n",
            " 93% 2900/3125 [09:17<00:40,  5.61it/s]27900 0.6484064368132887\n",
            " 96% 3000/3125 [09:35<00:22,  5.65it/s]28000 0.6484393992722034\n",
            "Global Step 28000 VAL res:\n",
            " {'train_loss': 0.6484393992722034, 'eval_loss': 2.9803047068417072, 'R1': 0.443, 'R2': 0.546, 'R5': 0.662, 'R10': 0.754, 'MRR': 0.5484607127521385, 'epoch': 9, 'global_step': 28000}\n",
            " 99% 3100/3125 [10:37<00:07,  3.22it/s]28100 0.6468094173938997\n",
            " 99% 3100/3125 [10:41<00:05,  4.83it/s]\n",
            "Epoch 9, Global Step 28125 VAL res:\n",
            " {'train_loss': 0.6469432482528686, 'eval_loss': 3.0080268383026123, 'R1': 0.449, 'R2': 0.545, 'R5': 0.664, 'R10': 0.744, 'MRR': 0.5504840697316571, 'epoch': 9, 'global_step': 28125}\n",
            "28125 0.6469432482528686\n",
            "  3% 100/3125 [00:17<08:51,  5.69it/s]28225 0.6014024356007576\n",
            "  6% 200/3125 [00:34<08:29,  5.74it/s]28325 0.6088003887236119\n",
            " 10% 300/3125 [00:52<08:10,  5.76it/s]28425 0.5982950060566267\n",
            " 13% 400/3125 [01:09<07:52,  5.77it/s]28525 0.5908195790275932\n",
            " 16% 500/3125 [01:26<07:34,  5.78it/s]28625 0.5945057748556137\n",
            " 19% 600/3125 [01:43<07:16,  5.78it/s]28725 0.5944916861752669\n",
            " 22% 700/3125 [02:01<06:59,  5.78it/s]28825 0.588150265536138\n",
            " 26% 800/3125 [02:18<06:41,  5.78it/s]28925 0.5909068632125855\n",
            "Global Step 29000 VAL res:\n",
            " {'train_loss': 0.5897392387219837, 'eval_loss': 3.0944519713521004, 'R1': 0.446, 'R2': 0.538, 'R5': 0.664, 'R10': 0.746, 'MRR': 0.5475923470382633, 'epoch': 10, 'global_step': 29000}\n",
            " 29% 900/3125 [03:20<11:34,  3.20it/s]29025 0.5889256768922011\n",
            " 32% 1000/3125 [03:37<09:31,  3.72it/s]29125 0.5914592051878571\n",
            " 35% 1100/3125 [03:54<08:05,  4.17it/s]29225 0.5902265785566785\n",
            " 38% 1200/3125 [04:12<07:02,  4.56it/s]29325 0.5908769987213115\n",
            " 42% 1300/3125 [04:29<06:14,  4.87it/s]29425 0.5920810001687362\n",
            " 45% 1400/3125 [04:46<05:37,  5.12it/s]29525 0.5920074107338276\n",
            " 48% 1500/3125 [05:04<05:06,  5.30it/s]29625 0.5923194824407498\n",
            " 51% 1600/3125 [05:21<04:40,  5.44it/s]29725 0.5917206952860579\n",
            " 54% 1700/3125 [05:38<04:18,  5.51it/s]29825 0.5926334450130953\n",
            " 58% 1800/3125 [05:56<03:57,  5.59it/s]29925 0.5921444493242436\n",
            "Global Step 30000 VAL res:\n",
            " {'train_loss': 0.5924541519125303, 'eval_loss': 3.0796384550631046, 'R1': 0.444, 'R2': 0.544, 'R5': 0.664, 'R10': 0.75, 'MRR': 0.5473791084497236, 'epoch': 10, 'global_step': 30000}\n",
            " 61% 1900/3125 [06:41<05:19,  3.83it/s]30025 0.5919666498781819\n",
            " 64% 2000/3125 [06:58<04:23,  4.27it/s]30125 0.5938658441789448\n",
            " 67% 2100/3125 [07:15<03:40,  4.65it/s]30225 0.5944576029798814\n",
            " 70% 2200/3125 [07:32<03:07,  4.95it/s]30325 0.5938235646181486\n",
            " 74% 2300/3125 [07:49<02:39,  5.19it/s]30425 0.5937996378443812\n",
            " 77% 2400/3125 [08:07<02:15,  5.37it/s]30525 0.5951021213301768\n",
            " 80% 2500/3125 [08:33<02:11,  4.76it/s]30625 0.5949272220820189\n",
            " 83% 2600/3125 [09:08<02:12,  3.95it/s]30725 0.5932840604936848\n",
            " 86% 2700/3125 [09:44<02:00,  3.53it/s]30825 0.592579130511041\n",
            " 90% 2800/3125 [10:19<01:38,  3.29it/s]30925 0.5924382796165134\n",
            "Global Step 31000 VAL res:\n",
            " {'train_loss': 0.5935447934580886, 'eval_loss': 3.073608662933111, 'R1': 0.447, 'R2': 0.544, 'R5': 0.669, 'R10': 0.747, 'MRR': 0.5492928484629115, 'epoch': 10, 'global_step': 31000}\n",
            " 93% 2900/3125 [11:40<01:42,  2.19it/s]31025 0.5938380827754736\n",
            " 96% 3000/3125 [12:15<00:53,  2.35it/s]31125 0.5940995634074012\n",
            " 99% 3100/3125 [12:51<00:10,  2.48it/s]31225 0.5939112534277862\n",
            " 99% 3100/3125 [13:00<00:06,  3.97it/s]\n",
            "Epoch 10, Global Step 31250 VAL res:\n",
            " {'train_loss': 0.5945128940272332, 'eval_loss': 3.070882584899664, 'R1': 0.446, 'R2': 0.544, 'R5': 0.668, 'R10': 0.749, 'MRR': 0.548849017566035, 'epoch': 10, 'global_step': 31250}\n",
            "31250 0.5945128940272332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py --bert_model bert_model/ --output_dir output_dstc7/ --train_dir dstc7/ --use_pretrain --architecture bi --eval\n",
        "# !python3 run.py --bert_model bert_model/ --output_dir output_dstc7/ --train_dir dstc7/ --use_pretrain --architecture poly --poly_m 16 --eval\n",
        "# !python3 run.py --bert_model bert_model/ --output_dir output_dstc7/ --train_dir dstc7/ --use_pretrain --architecture cross --eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POq7e5BjjXSr",
        "outputId": "d8abe4a8-3a4c-4482-f027-8ea0352c8adb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(adam_epsilon=1e-08, architecture='bi', bert_model='bert_model/', eval=True, eval_batch_size=32, fp16=False, fp16_opt_level='O1', gpu=0, gradient_accumulation_steps=1, learning_rate=5e-05, max_contexts_length=128, max_grad_norm=1.0, max_response_length=32, model_type='bert', num_train_epochs=10.0, output_dir='output_dstc7/', poly_m=0, print_freq=100, seed=12345, train_batch_size=32, train_dir='dstc7/', use_pretrain=True, warmup_steps=100, weight_decay=0.01)\n",
            "================================================================================\n",
            "Train dir: dstc7/\n",
            "Output dir: output_dstc7/\n",
            "================================================================================\n",
            "Loading parameters from output_dstc7/bi_0_pytorch_model.bin\n",
            "{'eval_loss': 2.34547645971179, 'R1': 0.481, 'R2': 0.581, 'R5': 0.685, 'R10': 0.77, 'MRR': 0.581299067078589}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/Poly-Encoder/output_dstc7 '/content/drive/MyDrive/Data Science/알파코 딥러닝 부트캠프/프로젝트/Poly Encoder'"
      ],
      "metadata": {
        "id": "Y1Qdur4wRAhb"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}