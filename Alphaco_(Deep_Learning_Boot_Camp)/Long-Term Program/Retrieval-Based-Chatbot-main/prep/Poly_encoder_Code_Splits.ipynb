{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Poly encoder Code Splits.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwjvsJp550o4ZqitmKN28g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dotsnangles/Retrieval-Based-Chatbot/blob/main/Poly_encoder_Code_Splits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ocPOP4u2nbP",
        "outputId": "d9d40fcf-c137-44f1-b605-74bc581acbbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.4 MB 15.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 49.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import argparse\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import BertModel, BertConfig, BertTokenizer, BertTokenizerFast\n",
        "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# from dataset import SelectionDataset\n",
        "# from transform import SelectionSequentialTransform, SelectionJoinTransform, SelectionConcatTransform\n",
        "# from encoder import PolyEncoder, BiEncoder, CrossEncoder\n",
        "\n",
        "from sklearn.metrics import label_ranking_average_precision_score\n",
        "\n",
        "import logging"
      ],
      "metadata": {
        "id": "J8IXT3Sd2mgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst = [1,2,3,4,5]\n",
        "lst[-3:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tonu9GF1LdW3",
        "outputId": "ec0960d5-5177-4407-8207-1e9dc90aa6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 4, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0aUOkdr2Odf"
      },
      "outputs": [],
      "source": [
        "class SelectionSequentialTransform(object):\n",
        "    def __init__(self, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __call__(self, texts):\n",
        "        input_ids_list, segment_ids_list, input_masks_list, contexts_masks_list = [], [], [], []\n",
        "        for text in texts:\n",
        "            tokenized_dict = self.tokenizer.encode_plus(text, max_length=self.max_len, padding='max_length', truncation=True)\n",
        "            input_ids, input_masks = tokenized_dict['input_ids'], tokenized_dict['attention_mask']\n",
        "            assert len(input_ids) == self.max_len\n",
        "            assert len(input_masks) == self.max_len\n",
        "            input_ids_list.append(input_ids)\n",
        "            input_masks_list.append(input_masks)\n",
        "\n",
        "        return input_ids_list, input_masks_list\n",
        "\n",
        "\n",
        "class SelectionJoinTransform(object):\n",
        "    def __init__(self, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.cls_id = self.tokenizer.convert_tokens_to_ids('[CLS]')\n",
        "        self.sep_id = self.tokenizer.convert_tokens_to_ids('[SEP]')\n",
        "        self.tokenizer.add_tokens(['\\n'], special_tokens=True)\n",
        "        self.pad_id = 0\n",
        "\n",
        "    def __call__(self, texts):\n",
        "        # another option is to use [SEP], but here we follow the discussion at:\n",
        "        # https://github.com/facebookresearch/ParlAI/issues/2306#issuecomment-599180186\n",
        "        context = '\\n'.join(texts)\n",
        "        tokenized_dict = self.tokenizer.encode_plus(context)\n",
        "        input_ids, input_masks = tokenized_dict['input_ids'], tokenized_dict['attention_mask']\n",
        "        input_ids = input_ids[-self.max_len:]\n",
        "        input_ids[0] = self.cls_id\n",
        "        input_masks = input_masks[-self.max_len:]\n",
        "        input_ids += [self.pad_id] * (self.max_len - len(input_ids))\n",
        "        input_masks += [0] * (self.max_len - len(input_masks))\n",
        "        assert len(input_ids) == self.max_len\n",
        "        assert len(input_masks) == self.max_len\n",
        "\n",
        "        return input_ids, input_masks\n",
        "    \n",
        "\n",
        "class SelectionConcatTransform(object):\n",
        "    def __init__(self, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        # in cross encoder mode, we simply add max_contexts_length and max_response_length together to form max_len\n",
        "        # this (in almost all cases) ensures all the response tokens are used and as many context tokens are used as possible\n",
        "        # the intuition is that responses and the last few contexts are the most important\n",
        "        self.max_len = max_len\n",
        "        self.cls_id = self.tokenizer.convert_tokens_to_ids('[CLS]')\n",
        "        self.sep_id = self.tokenizer.convert_tokens_to_ids('[SEP]')\n",
        "        self.tokenizer.add_tokens(['\\n'], special_tokens=True)\n",
        "        self.pad_id = 0\n",
        "\n",
        "    def __call__(self, context, responses):\n",
        "        # another option is to use [SEP], but here we follow the discussion at:\n",
        "        # https://github.com/facebookresearch/ParlAI/issues/2306#issuecomment-599180186\n",
        "        context = '\\n'.join(context)\n",
        "        tokenized_dict = self.tokenizer.encode_plus(context)\n",
        "        context_ids, context_masks, context_segment_ids = tokenized_dict['input_ids'], tokenized_dict['attention_mask'], tokenized_dict['token_type_ids']\n",
        "        ret_input_ids = []\n",
        "        ret_input_masks = []\n",
        "        ret_segment_ids = []\n",
        "        for response in responses:\n",
        "            tokenized_dict = self.tokenizer.encode_plus(response)\n",
        "            response_ids, response_masks, response_segment_ids = tokenized_dict['input_ids'], tokenized_dict['attention_mask'], tokenized_dict['token_type_ids']\n",
        "            response_segment_ids = [1]*(len(response_segment_ids)-1)\n",
        "            input_ids = context_ids + response_ids[1:]\n",
        "            input_ids = input_ids[-self.max_len:]\n",
        "            input_masks = context_masks + response_masks[1:]\n",
        "            input_masks = input_masks[-self.max_len:]\n",
        "            input_segment_ids = context_segment_ids + response_segment_ids\n",
        "            input_segment_ids = input_segment_ids[-self.max_len:]\n",
        "            input_ids[0] = self.cls_id\n",
        "            input_ids += [self.pad_id] * (self.max_len - len(input_ids))\n",
        "            input_masks += [0] * (self.max_len - len(input_masks))\n",
        "            input_segment_ids += [0] * (self.max_len - len(input_segment_ids))\n",
        "            assert len(input_ids) == self.max_len\n",
        "            assert len(input_masks) == self.max_len\n",
        "            assert len(input_segment_ids) == self.max_len\n",
        "            ret_input_ids.append(input_ids)\n",
        "            ret_input_masks.append(input_masks)\n",
        "            ret_segment_ids.append(input_segment_ids)\n",
        "        return ret_input_ids, ret_input_masks, ret_segment_ids"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "\n",
        "class SelectionDataset(Dataset):\n",
        "    def __init__(self, file_path, context_transform, response_transform, concat_transform, sample_cnt=None, mode='poly'):\n",
        "        self.context_transform = context_transform\n",
        "        self.response_transform = response_transform\n",
        "        self.concat_transform = concat_transform\n",
        "        self.data_source = []\n",
        "        self.mode = mode\n",
        "        neg_responses = []\n",
        "        with open(file_path, encoding='utf-8') as f:\n",
        "            group = {\n",
        "                'context': None,\n",
        "                'responses': [],\n",
        "                'labels': []\n",
        "            }\n",
        "            for line in f:\n",
        "                split = line.strip('\\n').split('\\t')\n",
        "                lbl, context, response = int(split[0]), split[1:-1], split[-1]\n",
        "                if lbl == 1 and len(group['responses']) > 0:\n",
        "                    self.data_source.append(group)\n",
        "                    group = {\n",
        "                        'context': None,\n",
        "                        'responses': [],\n",
        "                        'labels': []\n",
        "                    }\n",
        "                    if sample_cnt is not None and len(self.data_source) >= sample_cnt:\n",
        "                        break\n",
        "                else:\n",
        "                        neg_responses.append(response)\n",
        "                group['responses'].append(response)\n",
        "                group['labels'].append(lbl)\n",
        "                group['context'] = context\n",
        "            if len(group['responses']) > 0:\n",
        "                self.data_source.append(group)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_source)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        group = self.data_source[index]\n",
        "        context, responses, labels = group['context'], group['responses'], group['labels']\n",
        "        if self.mode == 'cross':\n",
        "            transformed_text = self.concat_transform(context, responses)\n",
        "            ret = transformed_text, labels\n",
        "        else:\n",
        "            transformed_context = self.context_transform(context)  # [token_ids],[seg_ids],[masks]\n",
        "            transformed_responses = self.response_transform(responses)  # [token_ids],[seg_ids],[masks]\n",
        "            ret = transformed_context, transformed_responses, labels\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def batchify_join_str(self, batch):\n",
        "        if self.mode == 'cross':\n",
        "            text_token_ids_list_batch, text_input_masks_list_batch, text_segment_ids_list_batch = [], [], []\n",
        "            labels_batch = []\n",
        "            for sample in batch:\n",
        "                text_token_ids_list, text_input_masks_list, text_segment_ids_list = sample[0]\n",
        "\n",
        "                text_token_ids_list_batch.append(text_token_ids_list)\n",
        "                text_input_masks_list_batch.append(text_input_masks_list)\n",
        "                text_segment_ids_list_batch.append(text_segment_ids_list)\n",
        "\n",
        "                labels_batch.append(sample[1])\n",
        "\n",
        "            long_tensors = [text_token_ids_list_batch, text_input_masks_list_batch, text_segment_ids_list_batch]\n",
        "\n",
        "            text_token_ids_list_batch, text_input_masks_list_batch, text_segment_ids_list_batch = (\n",
        "                torch.tensor(t, dtype=torch.long) for t in long_tensors)\n",
        "\n",
        "            labels_batch = torch.tensor(labels_batch, dtype=torch.long)\n",
        "            return text_token_ids_list_batch, text_input_masks_list_batch, text_segment_ids_list_batch, labels_batch\n",
        "\n",
        "        else:\n",
        "            contexts_token_ids_list_batch, contexts_input_masks_list_batch, \\\n",
        "            responses_token_ids_list_batch, responses_input_masks_list_batch = [], [], [], []\n",
        "            labels_batch = []\n",
        "            for sample in batch:\n",
        "                (contexts_token_ids_list, contexts_input_masks_list), (responses_token_ids_list, responses_input_masks_list) = sample[:2]\n",
        "\n",
        "                contexts_token_ids_list_batch.append(contexts_token_ids_list)\n",
        "                contexts_input_masks_list_batch.append(contexts_input_masks_list)\n",
        "\n",
        "                responses_token_ids_list_batch.append(responses_token_ids_list)\n",
        "                responses_input_masks_list_batch.append(responses_input_masks_list)\n",
        "\n",
        "                labels_batch.append(sample[-1])\n",
        "\n",
        "            long_tensors = [contexts_token_ids_list_batch, contexts_input_masks_list_batch,\n",
        "                                            responses_token_ids_list_batch, responses_input_masks_list_batch]\n",
        "\n",
        "            contexts_token_ids_list_batch, contexts_input_masks_list_batch, \\\n",
        "            responses_token_ids_list_batch, responses_input_masks_list_batch = (\n",
        "                torch.tensor(t, dtype=torch.long) for t in long_tensors)\n",
        "\n",
        "            labels_batch = torch.tensor(labels_batch, dtype=torch.long)\n",
        "            return contexts_token_ids_list_batch, contexts_input_masks_list_batch, \\\n",
        "                          responses_token_ids_list_batch, responses_input_masks_list_batch, labels_batch"
      ],
      "metadata": {
        "id": "anl5ANXQ2RHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "\n",
        "\n",
        "class BiEncoder(BertPreTrainedModel):\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super().__init__(config, *inputs, **kwargs)\n",
        "        self.bert = kwargs['bert']\n",
        "\n",
        "    def forward(self, context_input_ids, context_input_masks,\n",
        "                            responses_input_ids, responses_input_masks, labels=None):\n",
        "        ## only select the first response (whose lbl==1)\n",
        "        if labels is not None:\n",
        "            responses_input_ids = responses_input_ids[:, 0, :].unsqueeze(1)\n",
        "            responses_input_masks = responses_input_masks[:, 0, :].unsqueeze(1)\n",
        "\n",
        "        context_vec = self.bert(context_input_ids, context_input_masks)[0][:,0,:]  # [bs,dim]\n",
        "\n",
        "        batch_size, res_cnt, seq_length = responses_input_ids.shape\n",
        "        responses_input_ids = responses_input_ids.view(-1, seq_length)\n",
        "        responses_input_masks = responses_input_masks.view(-1, seq_length)\n",
        "\n",
        "        responses_vec = self.bert(responses_input_ids, responses_input_masks)[0][:,0,:]  # [bs,dim]\n",
        "        responses_vec = responses_vec.view(batch_size, res_cnt, -1)\n",
        "\n",
        "        if labels is not None:\n",
        "            responses_vec = responses_vec.squeeze(1)\n",
        "            dot_product = torch.matmul(context_vec, responses_vec.t())  # [bs, bs]\n",
        "            mask = torch.eye(context_input_ids.size(0)).to(context_input_ids.device)\n",
        "            loss = F.log_softmax(dot_product, dim=-1) * mask\n",
        "            loss = (-loss.sum(dim=1)).mean()\n",
        "            return loss\n",
        "        else:\n",
        "            context_vec = context_vec.unsqueeze(1)\n",
        "            dot_product = torch.matmul(context_vec, responses_vec.permute(0, 2, 1)).squeeze()\n",
        "            return dot_product\n",
        "\n",
        "\n",
        "class CrossEncoder(BertPreTrainedModel):\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super().__init__(config, *inputs, **kwargs)\n",
        "        self.bert = kwargs['bert']\n",
        "        self.linear = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, text_input_ids, text_input_masks, text_input_segments, labels=None):\n",
        "        batch_size, neg, dim = text_input_ids.shape\n",
        "        text_input_ids = text_input_ids.reshape(-1, dim)\n",
        "        text_input_masks = text_input_masks.reshape(-1, dim)\n",
        "        text_input_segments = text_input_segments.reshape(-1, dim)\n",
        "        text_vec = self.bert(text_input_ids, text_input_masks, text_input_segments)[0][:,0,:]  # [bs,dim]\n",
        "        score = self.linear(text_vec)\n",
        "        score = score.view(-1, neg)\n",
        "        if labels is not None:\n",
        "            loss = -F.log_softmax(score, -1)[:,0].mean()\n",
        "            return loss\n",
        "        else:\n",
        "            return score\n",
        "\n",
        "\n",
        "class PolyEncoder(BertPreTrainedModel):\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super().__init__(config, *inputs, **kwargs)\n",
        "        self.bert = kwargs['bert']\n",
        "        self.poly_m = kwargs['poly_m']\n",
        "        self.poly_code_embeddings = nn.Embedding(self.poly_m, config.hidden_size)\n",
        "        # https://github.com/facebookresearch/ParlAI/blob/master/parlai/agents/transformer/polyencoder.py#L355\n",
        "        torch.nn.init.normal_(self.poly_code_embeddings.weight, config.hidden_size ** -0.5)\n",
        "\n",
        "    def dot_attention(self, q, k, v):\n",
        "        # q: [bs, poly_m, dim] or [bs, res_cnt, dim]\n",
        "        # k=v: [bs, length, dim] or [bs, poly_m, dim]\n",
        "        attn_weights = torch.matmul(q, k.transpose(2, 1)) # [bs, poly_m, length]\n",
        "        attn_weights = F.softmax(attn_weights, -1)\n",
        "        output = torch.matmul(attn_weights, v) # [bs, poly_m, dim]\n",
        "        return output\n",
        "\n",
        "    def forward(self, context_input_ids, context_input_masks,\n",
        "                            responses_input_ids, responses_input_masks, labels=None):\n",
        "        # during training, only select the first response\n",
        "        # we are using other instances in a batch as negative examples\n",
        "        if labels is not None:\n",
        "            responses_input_ids = responses_input_ids[:, 0, :].unsqueeze(1)\n",
        "            responses_input_masks = responses_input_masks[:, 0, :].unsqueeze(1)\n",
        "        batch_size, res_cnt, seq_length = responses_input_ids.shape # res_cnt is 1 during training\n",
        "\n",
        "        # context encoder\n",
        "        ctx_out = self.bert(context_input_ids, context_input_masks)[0]  # [bs, length, dim]\n",
        "        poly_code_ids = torch.arange(self.poly_m, dtype=torch.long).to(context_input_ids.device)\n",
        "        poly_code_ids = poly_code_ids.unsqueeze(0).expand(batch_size, self.poly_m)\n",
        "        poly_codes = self.poly_code_embeddings(poly_code_ids) # [bs, poly_m, dim]\n",
        "        embs = self.dot_attention(poly_codes, ctx_out, ctx_out) # [bs, poly_m, dim]\n",
        "\n",
        "        # response encoder\n",
        "        responses_input_ids = responses_input_ids.view(-1, seq_length)\n",
        "        responses_input_masks = responses_input_masks.view(-1, seq_length)\n",
        "        cand_emb = self.bert(responses_input_ids, responses_input_masks)[0][:,0,:] # [bs, dim]\n",
        "        cand_emb = cand_emb.view(batch_size, res_cnt, -1) # [bs, res_cnt, dim]\n",
        "\n",
        "        # merge\n",
        "        if labels is not None:\n",
        "            # we are recycling responses for faster training\n",
        "            # we repeat responses for batch_size times to simulate test phase\n",
        "            # so that every context is paired with batch_size responses\n",
        "            cand_emb = cand_emb.permute(1, 0, 2) # [1, bs, dim]\n",
        "            cand_emb = cand_emb.expand(batch_size, batch_size, cand_emb.shape[2]) # [bs, bs, dim]\n",
        "            ctx_emb = self.dot_attention(cand_emb, embs, embs).squeeze() # [bs, bs, dim]\n",
        "            dot_product = (ctx_emb*cand_emb).sum(-1) # [bs, bs]\n",
        "            mask = torch.eye(batch_size).to(context_input_ids.device) # [bs, bs]\n",
        "            loss = F.log_softmax(dot_product, dim=-1) * mask\n",
        "            loss = (-loss.sum(dim=1)).mean()\n",
        "            return loss\n",
        "        else:\n",
        "            ctx_emb = self.dot_attention(cand_emb, embs, embs) # [bs, res_cnt, dim]\n",
        "            dot_product = (ctx_emb*cand_emb).sum(-1)\n",
        "            return dot_product"
      ],
      "metadata": {
        "id": "g-zgpAbi2VAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "def eval_running_model(dataloader, test=False):\n",
        "    model.eval()\n",
        "    eval_loss, eval_hit_times = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    r10 = r2 = r1 = r5 = 0\n",
        "    mrr = []\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        if args.architecture == 'cross':\n",
        "            text_token_ids_list_batch, text_input_masks_list_batch, text_segment_ids_list_batch, labels_batch = batch\n",
        "            with torch.no_grad():\n",
        "                logits = model(text_token_ids_list_batch, text_input_masks_list_batch, text_segment_ids_list_batch)\n",
        "                loss = F.cross_entropy(logits, torch.argmax(labels_batch, 1))\n",
        "        else:\n",
        "            context_token_ids_list_batch, context_input_masks_list_batch, \\\n",
        "            response_token_ids_list_batch, response_input_masks_list_batch, labels_batch = batch\n",
        "            with torch.no_grad():\n",
        "                logits = model(context_token_ids_list_batch, context_input_masks_list_batch,\n",
        "                                              response_token_ids_list_batch, response_input_masks_list_batch)\n",
        "                loss = F.cross_entropy(logits, torch.argmax(labels_batch, 1))\n",
        "        r2_indices = torch.topk(logits, 2)[1] # R 2 @ 100\n",
        "        r5_indices = torch.topk(logits, 5)[1] # R 5 @ 100\n",
        "        r10_indices = torch.topk(logits, 10)[1] # R 10 @ 100\n",
        "        r1 += (logits.argmax(-1) == 0).sum().item()\n",
        "        r2 += ((r2_indices==0).sum(-1)).sum().item()\n",
        "        r5 += ((r5_indices==0).sum(-1)).sum().item()\n",
        "        r10 += ((r10_indices==0).sum(-1)).sum().item()\n",
        "        # mrr\n",
        "        logits = logits.data.cpu().numpy()\n",
        "        for logit in logits:\n",
        "            y_true = np.zeros(len(logit))\n",
        "            y_true[0] = 1\n",
        "            mrr.append(label_ranking_average_precision_score([y_true], [logit]))\n",
        "        eval_loss += loss.item()\n",
        "        nb_eval_examples += labels_batch.size(0)\n",
        "        nb_eval_steps += 1\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = r1 / nb_eval_examples\n",
        "    if not test:\n",
        "        result = {\n",
        "            'train_loss': tr_loss / nb_tr_steps,\n",
        "            'eval_loss': eval_loss,\n",
        "            'R1': r1 / nb_eval_examples,\n",
        "            'R2': r2 / nb_eval_examples,\n",
        "            'R5': r5 / nb_eval_examples,\n",
        "            'R10': r10 / nb_eval_examples,\n",
        "            'MRR': np.mean(mrr),\n",
        "            'epoch': epoch,\n",
        "            'global_step': global_step,\n",
        "        }\n",
        "    else:\n",
        "        result = {\n",
        "            'eval_loss': eval_loss,\n",
        "            'R1': r1 / nb_eval_examples,\n",
        "            'R2': r2 / nb_eval_examples,\n",
        "            'R5': r5 / nb_eval_examples,\n",
        "            'R10': r10 / nb_eval_examples,\n",
        "            'MRR': np.mean(mrr),\n",
        "        }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "CtaB4Qhz4ps7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "        \"bert_model\": 'ckpt/pretrained/bert-small-uncased',\n",
        "        \"eval\": \"store_true\",\n",
        "        \"model_type\": 'bert',\n",
        "        \"output_dir\": 'output',\n",
        "        \"train_dir\": 'data/ubuntu_data',\n",
        "\n",
        "        \"use_pretrain\": \"store_true\",\n",
        "        \"architecture\": 'poly',\n",
        "\n",
        "        \"max_contexts_length\": 128,\n",
        "        \"max_response_length\": 32,\n",
        "        \"train_batch_size\": 32,\n",
        "        \"eval_batch_size\": 32,\n",
        "        \"print_freq\": 100,\n",
        "\n",
        "        \"poly_m\": 0,\n",
        "\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"warmup_steps\": 100,\n",
        "        \"adam_epsilon\": 1e-8,\n",
        "        \"max_grad_norm\": 1.0,\n",
        "\n",
        "        \"num_train_epochs\": 10.0,\n",
        "        'seed': 12345,\n",
        "        'gradient_accumulation_steps': 1,\n",
        "        \"fp16\": \"store_true\",\n",
        "        \"fp16_opt_level\": \"O1\",\n",
        "        'gpu': 0\n",
        "        }"
      ],
      "metadata": {
        "id": "jKhi-3lB4tvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from easydict import EasyDict as edict\n",
        "args = edict(args)"
      ],
      "metadata": {
        "id": "CLTdqAqq7BZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"%d\" % args.gpu\n",
        "set_seed(args)\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertTokenizerFast, BertModel),\n",
        "}\n",
        "\n",
        "ConfigClass, TokenizerClass, BertModelClass = MODEL_CLASSES[args.model_type]\n",
        "\n",
        "## init dataset and bert model\n",
        "tokenizer = TokenizerClass.from_pretrained(args.bert_model, vocab_file=os.path.join(args.bert_model, \"vocab.txt\"), do_lower_case=True, clean_text=False)\n",
        "context_transform = SelectionJoinTransform(tokenizer=tokenizer, max_len=args.max_contexts_length)\n",
        "response_transform = SelectionSequentialTransform(tokenizer=tokenizer, max_len=args.max_response_length)\n",
        "concat_transform = SelectionConcatTransform(tokenizer=tokenizer, max_len=args.max_response_length+args.max_contexts_length)\n",
        "\n",
        "print('=' * 80)\n",
        "print('Train dir:', args.train_dir)\n",
        "print('Output dir:', args.output_dir)\n",
        "print('=' * 80)\n",
        "\n",
        "if not args.eval:\n",
        "    train_dataset = SelectionDataset(os.path.join(args.train_dir, 'train.txt'),\n",
        "                                                                    context_transform, response_transform, concat_transform, sample_cnt=None, mode=args.architecture)\n",
        "    val_dataset = SelectionDataset(os.path.join(args.train_dir, 'dev.txt'),\n",
        "                                                                context_transform, response_transform, concat_transform, sample_cnt=1000, mode=args.architecture)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size, collate_fn=train_dataset.batchify_join_str, shuffle=True, num_workers=0)\n",
        "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "else: # test\n",
        "    val_dataset = SelectionDataset(os.path.join(args.train_dir, 'test.txt'),\n",
        "                                                                context_transform, response_transform, concat_transform, sample_cnt=None, mode=args.architecture)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=args.eval_batch_size, collate_fn=val_dataset.batchify_join_str, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "epoch_start = 1\n",
        "global_step = 0\n",
        "best_eval_loss = float('inf')\n",
        "best_test_loss = float('inf')\n",
        "\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)\n",
        "shutil.copyfile(os.path.join(args.bert_model, 'vocab.txt'), os.path.join(args.output_dir, 'vocab.txt'))\n",
        "shutil.copyfile(os.path.join(args.bert_model, 'config.json'), os.path.join(args.output_dir, 'config.json'))\n",
        "log_wf = open(os.path.join(args.output_dir, 'log.txt'), 'a', encoding='utf-8')\n",
        "print (args, file=log_wf)\n",
        "\n",
        "state_save_path = os.path.join(args.output_dir, '{}_{}_pytorch_model.bin'.format(args.architecture, args.poly_m))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "########################################\n",
        "## build BERT encoder\n",
        "########################################\n",
        "bert_config = ConfigClass.from_json_file(os.path.join(args.bert_model, 'config.json'))\n",
        "if args.use_pretrain and not args.eval:\n",
        "    previous_model_file = os.path.join(args.bert_model, \"pytorch_model.bin\")\n",
        "    print('Loading parameters from', previous_model_file)\n",
        "    log_wf.write('Loading parameters from %s' % previous_model_file + '\\n')\n",
        "    model_state_dict = torch.load(previous_model_file, map_location=\"cpu\")\n",
        "    bert = BertModelClass.from_pretrained(args.bert_model, state_dict=model_state_dict)\n",
        "    del model_state_dict\n",
        "else:\n",
        "    bert = BertModelClass(bert_config)\n",
        "\n",
        "if args.architecture == 'poly':\n",
        "    model = PolyEncoder(bert_config, bert=bert, poly_m=args.poly_m)\n",
        "elif args.architecture == 'bi':\n",
        "    model = BiEncoder(bert_config, bert=bert)\n",
        "elif args.architecture == 'cross':\n",
        "    model = CrossEncoder(bert_config, bert=bert)\n",
        "else:\n",
        "    raise Exception('Unknown architecture.')\n",
        "model.resize_token_embeddings(len(tokenizer)) \n",
        "model.to(device)\n",
        "\n",
        "if args.eval:\n",
        "    print('Loading parameters from', state_save_path)\n",
        "    model.load_state_dict(torch.load(state_save_path))\n",
        "    test_result = eval_running_model(val_dataloader, test=True)\n",
        "    print (test_result)\n",
        "    exit()\n",
        "    \n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": args.weight_decay,\n",
        "    },\n",
        "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        ")\n",
        "if args.fp16:\n",
        "    try:\n",
        "        from apex import amp\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "    model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "print_freq = args.print_freq//args.gradient_accumulation_steps\n",
        "eval_freq = min(len(train_dataloader) // 2, 1000)\n",
        "eval_freq = eval_freq//args.gradient_accumulation_steps\n",
        "print('Print freq:', print_freq, \"Eval freq:\", eval_freq)\n",
        "\n",
        "for epoch in range(epoch_start, int(args.num_train_epochs) + 1):\n",
        "    tr_loss = 0\n",
        "    nb_tr_steps = 0\n",
        "    with tqdm(total=len(train_dataloader)//args.gradient_accumulation_steps) as bar:\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            if args.architecture == 'cross':\n",
        "                text_token_ids_list_batch, text_input_masks_list_batch, text_segment_ids_list_batch, labels_batch = batch\n",
        "                loss = model(text_token_ids_list_batch, text_input_masks_list_batch, text_segment_ids_list_batch, labels_batch)\n",
        "            else:\n",
        "                context_token_ids_list_batch, context_input_masks_list_batch, \\\n",
        "                response_token_ids_list_batch, response_input_masks_list_batch, labels_batch = batch\n",
        "                loss = model(context_token_ids_list_batch, context_input_masks_list_batch,\n",
        "                                        response_token_ids_list_batch, response_input_masks_list_batch,\n",
        "                                        labels_batch)\n",
        "\n",
        "            loss = loss / args.gradient_accumulation_steps\n",
        "            \n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "            \n",
        "            tr_loss += loss.item()\n",
        "\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                if args.fp16:\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                nb_tr_steps += 1\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if nb_tr_steps and nb_tr_steps % print_freq == 0:\n",
        "                    bar.update(min(print_freq, nb_tr_steps))\n",
        "                    time.sleep(0.02)\n",
        "                    print(global_step, tr_loss / nb_tr_steps)\n",
        "                    log_wf.write('%d\\t%f\\n' % (global_step, tr_loss / nb_tr_steps))\n",
        "\n",
        "                if global_step and global_step % eval_freq == 0:\n",
        "                    val_result = eval_running_model(val_dataloader)\n",
        "                    print('Global Step %d VAL res:\\n' % global_step, val_result)\n",
        "                    log_wf.write('Global Step %d VAL res:\\n' % global_step)\n",
        "                    log_wf.write(str(val_result) + '\\n')\n",
        "\n",
        "                    if val_result['eval_loss'] < best_eval_loss:\n",
        "                        best_eval_loss = val_result['eval_loss']\n",
        "                        val_result['best_eval_loss'] = best_eval_loss\n",
        "                        # save model\n",
        "                        print('[Saving at]', state_save_path)\n",
        "                        log_wf.write('[Saving at] %s\\n' % state_save_path)\n",
        "                        torch.save(model.state_dict(), state_save_path)\n",
        "            log_wf.flush()\n",
        "\n",
        "    # add a eval step after each epoch\n",
        "    val_result = eval_running_model(val_dataloader)\n",
        "    print('Epoch %d, Global Step %d VAL res:\\n' % (epoch, global_step), val_result)\n",
        "    log_wf.write('Global Step %d VAL res:\\n' % global_step)\n",
        "    log_wf.write(str(val_result) + '\\n')\n",
        "\n",
        "    if val_result['eval_loss'] < best_eval_loss:\n",
        "        best_eval_loss = val_result['eval_loss']\n",
        "        val_result['best_eval_loss'] = best_eval_loss\n",
        "        # save model\n",
        "        print('[Saving at]', state_save_path)\n",
        "        log_wf.write('[Saving at] %s\\n' % state_save_path)\n",
        "        torch.save(model.state_dict(), state_save_path)\n",
        "    print(global_step, tr_loss / nb_tr_steps)\n",
        "    log_wf.write('%d\\t%f\\n' % (global_step, tr_loss / nb_tr_steps))"
      ],
      "metadata": {
        "id": "jgSd5pJL4Swz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run.py BK"
      ],
      "metadata": {
        "id": "riGO066O4Up4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "def eval_running_model(dataloader, test=False):\n",
        "    model.eval()\n",
        "    eval_loss, eval_hit_times = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    r10 = r2 = r1 = r5 = 0\n",
        "    mrr = []\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        if args.architecture == 'cross':\n",
        "            text_token_ids_list_batch, text_input_masks_list_batch, text_segment_ids_list_batch, labels_batch = batch\n",
        "            with torch.no_grad():\n",
        "                logits = model(text_token_ids_list_batch, text_input_masks_list_batch, text_segment_ids_list_batch)\n",
        "                loss = F.cross_entropy(logits, torch.argmax(labels_batch, 1))\n",
        "        else:\n",
        "            context_token_ids_list_batch, context_input_masks_list_batch, \\\n",
        "            response_token_ids_list_batch, response_input_masks_list_batch, labels_batch = batch\n",
        "            with torch.no_grad():\n",
        "                logits = model(context_token_ids_list_batch, context_input_masks_list_batch,\n",
        "                                              response_token_ids_list_batch, response_input_masks_list_batch)\n",
        "                loss = F.cross_entropy(logits, torch.argmax(labels_batch, 1))\n",
        "        r2_indices = torch.topk(logits, 2)[1] # R 2 @ 100\n",
        "        r5_indices = torch.topk(logits, 5)[1] # R 5 @ 100\n",
        "        r10_indices = torch.topk(logits, 10)[1] # R 10 @ 100\n",
        "        r1 += (logits.argmax(-1) == 0).sum().item()\n",
        "        r2 += ((r2_indices==0).sum(-1)).sum().item()\n",
        "        r5 += ((r5_indices==0).sum(-1)).sum().item()\n",
        "        r10 += ((r10_indices==0).sum(-1)).sum().item()\n",
        "        # mrr\n",
        "        logits = logits.data.cpu().numpy()\n",
        "        for logit in logits:\n",
        "            y_true = np.zeros(len(logit))\n",
        "            y_true[0] = 1\n",
        "            mrr.append(label_ranking_average_precision_score([y_true], [logit]))\n",
        "        eval_loss += loss.item()\n",
        "        nb_eval_examples += labels_batch.size(0)\n",
        "        nb_eval_steps += 1\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = r1 / nb_eval_examples\n",
        "    if not test:\n",
        "        result = {\n",
        "            'train_loss': tr_loss / nb_tr_steps,\n",
        "            'eval_loss': eval_loss,\n",
        "            'R1': r1 / nb_eval_examples,\n",
        "            'R2': r2 / nb_eval_examples,\n",
        "            'R5': r5 / nb_eval_examples,\n",
        "            'R10': r10 / nb_eval_examples,\n",
        "            'MRR': np.mean(mrr),\n",
        "            'epoch': epoch,\n",
        "            'global_step': global_step,\n",
        "        }\n",
        "    else:\n",
        "        result = {\n",
        "            'eval_loss': eval_loss,\n",
        "            'R1': r1 / nb_eval_examples,\n",
        "            'R2': r2 / nb_eval_examples,\n",
        "            'R5': r5 / nb_eval_examples,\n",
        "            'R10': r10 / nb_eval_examples,\n",
        "            'MRR': np.mean(mrr),\n",
        "        }\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--bert_model\", default='ckpt/pretrained/bert-small-uncased', type=str)\n",
        "    parser.add_argument(\"--eval\", action=\"store_true\")\n",
        "    parser.add_argument(\"--model_type\", default='bert', type=str)\n",
        "    parser.add_argument(\"--output_dir\", required=True, type=str)\n",
        "    parser.add_argument(\"--train_dir\", default='data/ubuntu_data', type=str)\n",
        "\n",
        "    parser.add_argument(\"--use_pretrain\", action=\"store_true\")\n",
        "    parser.add_argument(\"--architecture\", required=True, type=str, help='[poly, bi, cross]')\n",
        "\n",
        "    parser.add_argument(\"--max_contexts_length\", default=128, type=int)\n",
        "    parser.add_argument(\"--max_response_length\", default=32, type=int)\n",
        "    parser.add_argument(\"--train_batch_size\", default=32, type=int, help=\"Total batch size for training.\")\n",
        "    parser.add_argument(\"--eval_batch_size\", default=32, type=int, help=\"Total batch size for eval.\")\n",
        "    parser.add_argument(\"--print_freq\", default=100, type=int, help=\"Log frequency\")\n",
        "\n",
        "    parser.add_argument(\"--poly_m\", default=0, type=int, help=\"Number of m of polyencoder\")\n",
        "\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.01, type=float)\n",
        "    parser.add_argument(\"--warmup_steps\", default=100, type=float)\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "\n",
        "    parser.add_argument(\"--num_train_epochs\", default=10.0, type=float,\n",
        "                                            help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument('--seed', type=int, default=12345, help=\"random seed for initialization\")\n",
        "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                                            help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fp16_opt_level\",\n",
        "        type=str,\n",
        "        default=\"O1\",\n",
        "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                  \"See details at https://nvidia.github.io/apex/amp.html\",\n",
        "    )\n",
        "    parser.add_argument('--gpu', type=int, default=0)\n",
        "    args = parser.parse_args()\n",
        "    print(args)\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"%d\" % args.gpu\n",
        "    set_seed(args)\n",
        "\n",
        "    MODEL_CLASSES = {\n",
        "        'bert': (BertConfig, BertTokenizerFast, BertModel),\n",
        "    }\n",
        "    ConfigClass, TokenizerClass, BertModelClass = MODEL_CLASSES[args.model_type]\n",
        "\n",
        "    ## init dataset and bert model\n",
        "    tokenizer = TokenizerClass.from_pretrained(args.bert_model, vocab_file=os.path.join(args.bert_model, \"vocab.txt\"), do_lower_case=True, clean_text=False)\n",
        "    context_transform = SelectionJoinTransform(tokenizer=tokenizer, max_len=args.max_contexts_length)\n",
        "    response_transform = SelectionSequentialTransform(tokenizer=tokenizer, max_len=args.max_response_length)\n",
        "    concat_transform = SelectionConcatTransform(tokenizer=tokenizer, max_len=args.max_response_length+args.max_contexts_length)\n",
        "\n",
        "    print('=' * 80)\n",
        "    print('Train dir:', args.train_dir)\n",
        "    print('Output dir:', args.output_dir)\n",
        "    print('=' * 80)\n",
        "\n",
        "    if not args.eval:\n",
        "        train_dataset = SelectionDataset(os.path.join(args.train_dir, 'train.txt'),\n",
        "                                                                      context_transform, response_transform, concat_transform, sample_cnt=None, mode=args.architecture)\n",
        "        val_dataset = SelectionDataset(os.path.join(args.train_dir, 'dev.txt'),\n",
        "                                                                  context_transform, response_transform, concat_transform, sample_cnt=1000, mode=args.architecture)\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size, collate_fn=train_dataset.batchify_join_str, shuffle=True, num_workers=0)\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "    else: # test\n",
        "        val_dataset = SelectionDataset(os.path.join(args.train_dir, 'test.txt'),\n",
        "                                                                  context_transform, response_transform, concat_transform, sample_cnt=None, mode=args.architecture)\n",
        "\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=args.eval_batch_size, collate_fn=val_dataset.batchify_join_str, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "    epoch_start = 1\n",
        "    global_step = 0\n",
        "    best_eval_loss = float('inf')\n",
        "    best_test_loss = float('inf')\n",
        "\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "    shutil.copyfile(os.path.join(args.bert_model, 'vocab.txt'), os.path.join(args.output_dir, 'vocab.txt'))\n",
        "    shutil.copyfile(os.path.join(args.bert_model, 'config.json'), os.path.join(args.output_dir, 'config.json'))\n",
        "    log_wf = open(os.path.join(args.output_dir, 'log.txt'), 'a', encoding='utf-8')\n",
        "    print (args, file=log_wf)\n",
        "\n",
        "    state_save_path = os.path.join(args.output_dir, '{}_{}_pytorch_model.bin'.format(args.architecture, args.poly_m))\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ########################################\n",
        "    ## build BERT encoder\n",
        "    ########################################\n",
        "    bert_config = ConfigClass.from_json_file(os.path.join(args.bert_model, 'config.json'))\n",
        "    if args.use_pretrain and not args.eval:\n",
        "        previous_model_file = os.path.join(args.bert_model, \"pytorch_model.bin\")\n",
        "        print('Loading parameters from', previous_model_file)\n",
        "        log_wf.write('Loading parameters from %s' % previous_model_file + '\\n')\n",
        "        model_state_dict = torch.load(previous_model_file, map_location=\"cpu\")\n",
        "        bert = BertModelClass.from_pretrained(args.bert_model, state_dict=model_state_dict)\n",
        "        del model_state_dict\n",
        "    else:\n",
        "        bert = BertModelClass(bert_config)\n",
        "\n",
        "    if args.architecture == 'poly':\n",
        "        model = PolyEncoder(bert_config, bert=bert, poly_m=args.poly_m)\n",
        "    elif args.architecture == 'bi':\n",
        "        model = BiEncoder(bert_config, bert=bert)\n",
        "    elif args.architecture == 'cross':\n",
        "        model = CrossEncoder(bert_config, bert=bert)\n",
        "    else:\n",
        "        raise Exception('Unknown architecture.')\n",
        "    model.resize_token_embeddings(len(tokenizer)) \n",
        "    model.to(device)\n",
        "    \n",
        "    if args.eval:\n",
        "        print('Loading parameters from', state_save_path)\n",
        "        model.load_state_dict(torch.load(state_save_path))\n",
        "        test_result = eval_running_model(val_dataloader, test=True)\n",
        "        print (test_result)\n",
        "        exit()\n",
        "        \n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    \n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    print_freq = args.print_freq//args.gradient_accumulation_steps\n",
        "    eval_freq = min(len(train_dataloader) // 2, 1000)\n",
        "    eval_freq = eval_freq//args.gradient_accumulation_steps\n",
        "    print('Print freq:', print_freq, \"Eval freq:\", eval_freq)\n",
        "\n",
        "    for epoch in range(epoch_start, int(args.num_train_epochs) + 1):\n",
        "        tr_loss = 0\n",
        "        nb_tr_steps = 0\n",
        "        with tqdm(total=len(train_dataloader)//args.gradient_accumulation_steps) as bar:\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                model.train()\n",
        "                optimizer.zero_grad()\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                if args.architecture == 'cross':\n",
        "                    text_token_ids_list_batch, text_input_masks_list_batch, text_segment_ids_list_batch, labels_batch = batch\n",
        "                    loss = model(text_token_ids_list_batch, text_input_masks_list_batch, text_segment_ids_list_batch, labels_batch)\n",
        "                else:\n",
        "                    context_token_ids_list_batch, context_input_masks_list_batch, \\\n",
        "                    response_token_ids_list_batch, response_input_masks_list_batch, labels_batch = batch\n",
        "                    loss = model(context_token_ids_list_batch, context_input_masks_list_batch,\n",
        "                                          response_token_ids_list_batch, response_input_masks_list_batch,\n",
        "                                          labels_batch)\n",
        "\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "                \n",
        "                if args.fp16:\n",
        "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                        scaled_loss.backward()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                \n",
        "                tr_loss += loss.item()\n",
        "\n",
        "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                    if args.fp16:\n",
        "                        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                    else:\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                    nb_tr_steps += 1\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    model.zero_grad()\n",
        "                    global_step += 1\n",
        "\n",
        "                    if nb_tr_steps and nb_tr_steps % print_freq == 0:\n",
        "                        bar.update(min(print_freq, nb_tr_steps))\n",
        "                        time.sleep(0.02)\n",
        "                        print(global_step, tr_loss / nb_tr_steps)\n",
        "                        log_wf.write('%d\\t%f\\n' % (global_step, tr_loss / nb_tr_steps))\n",
        "\n",
        "                    if global_step and global_step % eval_freq == 0:\n",
        "                        val_result = eval_running_model(val_dataloader)\n",
        "                        print('Global Step %d VAL res:\\n' % global_step, val_result)\n",
        "                        log_wf.write('Global Step %d VAL res:\\n' % global_step)\n",
        "                        log_wf.write(str(val_result) + '\\n')\n",
        "\n",
        "                        if val_result['eval_loss'] < best_eval_loss:\n",
        "                            best_eval_loss = val_result['eval_loss']\n",
        "                            val_result['best_eval_loss'] = best_eval_loss\n",
        "                            # save model\n",
        "                            print('[Saving at]', state_save_path)\n",
        "                            log_wf.write('[Saving at] %s\\n' % state_save_path)\n",
        "                            torch.save(model.state_dict(), state_save_path)\n",
        "                log_wf.flush()\n",
        "\n",
        "        # add a eval step after each epoch\n",
        "        val_result = eval_running_model(val_dataloader)\n",
        "        print('Epoch %d, Global Step %d VAL res:\\n' % (epoch, global_step), val_result)\n",
        "        log_wf.write('Global Step %d VAL res:\\n' % global_step)\n",
        "        log_wf.write(str(val_result) + '\\n')\n",
        "\n",
        "        if val_result['eval_loss'] < best_eval_loss:\n",
        "            best_eval_loss = val_result['eval_loss']\n",
        "            val_result['best_eval_loss'] = best_eval_loss\n",
        "            # save model\n",
        "            print('[Saving at]', state_save_path)\n",
        "            log_wf.write('[Saving at] %s\\n' % state_save_path)\n",
        "            torch.save(model.state_dict(), state_save_path)\n",
        "        print(global_step, tr_loss / nb_tr_steps)\n",
        "        log_wf.write('%d\\t%f\\n' % (global_step, tr_loss / nb_tr_steps))"
      ],
      "metadata": {
        "id": "IVdboO_P2YQ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}