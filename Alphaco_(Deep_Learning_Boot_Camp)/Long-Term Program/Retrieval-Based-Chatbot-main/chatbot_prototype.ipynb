{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot-prototype.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMJ/cWczEcLlMcn4oeB1f0o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dotsnangles/Retrieval-Based-Chatbot/blob/main/chatbot_prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Click here and do Shift + Enter 3 times!"
      ],
      "metadata": {
        "id": "gTpU8rVA0uhv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-ZyBZPWfX5m",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!pip install -q transformers datasets folium==0.2.1\n",
        "!git clone https://github.com/dotsnangles/Poly-Encoder.git\n",
        "%cd /content/Poly-Encoder\n",
        "\n",
        "!gdown -q --folder 1Ipr-aNF5ELMY0HTXAmeV26LlgktKUfmG\n",
        "!gdown -q --folder 1RH7laK4WlucCw68ZeExFvyg7vs-kB_x3\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer, AutoModel\n",
        "from encoder import PolyEncoder\n",
        "from transform import SelectionJoinTransform, SelectionSequentialTransform\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)\n",
        "\n",
        "PATH = '/content/Poly-Encoder/chatbot_output/poly_16_pytorch_model.bin'\n",
        "\n",
        "bert_name = 'klue/bert-base'\n",
        "bert_config = BertConfig.from_pretrained(bert_name)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_name)\n",
        "tokenizer.add_tokens(['\\n'], special_tokens=True)\n",
        "\n",
        "context_transform = SelectionJoinTransform(tokenizer=tokenizer, max_len=256)\n",
        "response_transform = SelectionSequentialTransform(tokenizer=tokenizer, max_len=128)\n",
        "\n",
        "bert = BertModel.from_pretrained(bert_name, config=bert_config)\n",
        "\n",
        "model = PolyEncoder(bert_config, bert=bert, poly_m=16)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.to(device)\n",
        "model.device\n",
        "\n",
        "context = ['This framework generates embeddings for each input sentence', \n",
        "            'Sentences are passed as a list of string.', \n",
        "            'The quick brown fox jumps over the lazy dog.']\n",
        "\n",
        "candidates = ['This framework generates embeddings for each input sentence', \n",
        "            'Sentences are passed as a list of string.', \n",
        "            'The quick brown fox jumps over the lazy dog.']\n",
        "\n",
        "def context_input(context):\n",
        "    context_input_ids, context_input_masks = context_transform(context)\n",
        "    contexts_token_ids_list_batch, contexts_input_masks_list_batch = [context_input_ids], [context_input_masks]\n",
        "\n",
        "    long_tensors = [contexts_token_ids_list_batch, contexts_input_masks_list_batch]\n",
        "\n",
        "    contexts_token_ids_list_batch, contexts_input_masks_list_batch = (torch.tensor(t, dtype=torch.long, device=device) for t in long_tensors)\n",
        "\n",
        "    return contexts_token_ids_list_batch, contexts_input_masks_list_batch\n",
        "contexts_token_ids_list_batch, contexts_input_masks_list_batch = context_input(context)\n",
        "\n",
        "def response_input(candidates):\n",
        "    responses_token_ids_list, responses_input_masks_list = response_transform(candidates)\n",
        "    responses_token_ids_list_batch, responses_input_masks_list_batch = [responses_token_ids_list], [responses_input_masks_list]\n",
        "\n",
        "    long_tensors = [responses_token_ids_list_batch, responses_input_masks_list_batch]\n",
        "\n",
        "    responses_token_ids_list_batch, responses_input_masks_list_batch = (torch.tensor(t, dtype=torch.long, device=device) for t in long_tensors)\n",
        "\n",
        "    return responses_token_ids_list_batch, responses_input_masks_list_batch\n",
        "responses_token_ids_list_batch, responses_input_masks_list_batch = response_input(candidates)\n",
        "\n",
        "def embs_gen(contexts_token_ids_list_batch, contexts_input_masks_list_batch):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        \n",
        "        ctx_out = model.bert(contexts_token_ids_list_batch, contexts_input_masks_list_batch)[0]  # [bs, length, dim]\n",
        "        poly_code_ids = torch.arange(model.poly_m, dtype=torch.long).to(contexts_token_ids_list_batch.device)\n",
        "        poly_code_ids = poly_code_ids.unsqueeze(0).expand(1, model.poly_m)\n",
        "        poly_codes = model.poly_code_embeddings(poly_code_ids) # [bs, poly_m, dim]\n",
        "        embs = model.dot_attention(poly_codes, ctx_out, ctx_out) # [bs, poly_m, dim]\n",
        "\n",
        "        return embs\n",
        "embs = embs_gen(contexts_token_ids_list_batch, contexts_input_masks_list_batch)\n",
        "\n",
        "def cand_emb_gen(responses_token_ids_list_batch, responses_input_masks_list_batch):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "                \n",
        "        batch_size, res_cnt, seq_length = responses_token_ids_list_batch.shape # res_cnt is 1 during training\n",
        "        responses_token_ids_list_batch = responses_token_ids_list_batch.view(-1, seq_length)\n",
        "        responses_input_masks_list_batch = responses_input_masks_list_batch.view(-1, seq_length)\n",
        "        cand_emb = model.bert(responses_token_ids_list_batch, responses_input_masks_list_batch)[0][:,0,:] # [bs, dim]\n",
        "        cand_emb = cand_emb.view(batch_size, res_cnt, -1) # [bs, res_cnt, dim]\n",
        "\n",
        "        return cand_emb\n",
        "cand_emb = cand_emb_gen(responses_token_ids_list_batch, responses_input_masks_list_batch)\n",
        "\n",
        "def loss(embs, cand_emb, contexts_token_ids_list_batch, responses_token_ids_list_batch):\n",
        "    batch_size, res_cnt, seq_length = responses_token_ids_list_batch.shape\n",
        "\n",
        "    ctx_emb = model.dot_attention(cand_emb, embs, embs) # [bs, bs, dim]\n",
        "    # print(ctx_emb)\n",
        "    ctx_emb = ctx_emb.squeeze()\n",
        "    # print(ctx_emb)\n",
        "    dot_product = (ctx_emb*cand_emb) # [bs, bs]\n",
        "    # print(dot_product)\n",
        "    dot_product = dot_product.sum(-1)\n",
        "    print(dot_product)\n",
        "    mask = torch.eye(batch_size).to(contexts_token_ids_list_batch.device) # [bs, bs]\n",
        "    print(mask)\n",
        "    loss = F.log_softmax(dot_product, dim=-1)\n",
        "    print(loss)\n",
        "    loss = loss * mask\n",
        "    print(loss)\n",
        "    loss = (-loss.sum(dim=1))\n",
        "    print(loss)\n",
        "    loss = loss.mean()\n",
        "    print(loss)\n",
        "    return loss\n",
        "# loss_ = loss(embs, cand_emb, contexts_token_ids_list_batch, responses_token_ids_list_batch)\n",
        "\n",
        "def score(embs, cand_emb):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        ctx_emb = model.dot_attention(cand_emb, embs, embs) # [bs, res_cnt, dim]\n",
        "        dot_product = (ctx_emb*cand_emb).sum(-1)\n",
        "        \n",
        "        return dot_product\n",
        "score_ = score(embs, cand_emb)\n",
        "\n",
        "# forward\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    \n",
        "    model_foward = model(contexts_token_ids_list_batch, contexts_input_masks_list_batch, responses_token_ids_list_batch, responses_input_masks_list_batch)\n",
        "\n",
        "### ë°ì´í„° ê²€ì¦\n",
        "import pickle\n",
        "\n",
        "with open('/content/Poly-Encoder/ê°ì„±ëŒ€í™”ì±—ë´‡ë°ì´í„°/train_data_source.pickle', 'rb') as f:\n",
        "    train = pickle.load(f)\n",
        "with open('/content/Poly-Encoder/ê°ì„±ëŒ€í™”ì±—ë´‡ë°ì´í„°/val_data_source.pickle', 'rb') as f:\n",
        "    dev = pickle.load(f)\n",
        "# index = 500\n",
        "# train[index]['context']\n",
        "# train[index]['responses']\n",
        "# dev[index]['context']\n",
        "# dev[index]['responses']\n",
        "\n",
        "### ì±—ë´‡ í…Œì´ë¸” ìƒì„±\n",
        "data = {\n",
        "    'context' : [],\n",
        "    'response': []\n",
        "}\n",
        "\n",
        "for sample in train:\n",
        "    data['context'].append(sample['context'])\n",
        "    data['response'].append([sample['responses'][0]])\n",
        "# len(data['context']), len(data['response'])\n",
        "# idx = 400\n",
        "# print(data['context'][idx])\n",
        "# print(data['response'][idx])\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "### generate cand_embs & create tensor table\n",
        "# response_input_srs = df['response'].apply(response_input)\n",
        "# response_input_lst = response_input_srs.to_list()\n",
        "\n",
        "# cand_embs_lst = []\n",
        "# for sample in response_input_lst:\n",
        "#     cand_embs_lst.append(cand_emb_gen(*sample).to('cpu'))\n",
        "# df['response embedding'] = cand_embs_lst\n",
        "# df[['response', 'response embedding']]\n",
        "# cand_embs = cand_embs_lst[0]\n",
        "# for idx in range(1, len(cand_embs_lst)):\n",
        "#     y = cand_embs_lst[idx]\n",
        "#     cand_embs = torch.cat((cand_embs, y), 1)\n",
        "# cand_embs = cand_embs.to(device)\n",
        "\n",
        "import pickle\n",
        "with open('/content/Poly-Encoder/ê°ì„±ëŒ€í™”ì±—ë´‡ë°ì´í„°/cand_embs.pickle', 'rb') as f:\n",
        "    cand_embs = pickle.load(f)\n",
        "cand_embs.to(device)\n",
        "\n",
        "### generate context_embs\n",
        "query = ['ë„ˆë¬´ ì„±ê¸‰í•œ ê²°ì •ì„ í•œ ê²ƒ ê°™ì•„.']\n",
        "embs = embs_gen(*context_input(query))\n",
        "\n",
        "### Score & Retrieve\n",
        "import time\n",
        "start = time.time()\n",
        "s = score(embs, cand_embs)\n",
        "end = time.time()\n",
        "idx = s.argmax(1)\n",
        "idx = int(idx[0])\n",
        "# df['response'][idx]\n",
        "# df.iloc[idx]['context']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "### Chatbot UI\n",
        "consult_context = {\n",
        "    'num':[],\n",
        "    'name': [],\n",
        "    'customer':[],\n",
        "    'chatbot':[]\n",
        "}\n",
        "\n",
        "print('ì•ˆë…•í•˜ì„¸ìš”. ê³µê° ë§Œë•…ì´~~â­ï¸ ê³µê°ì´ğŸ€ ì…ë‹ˆë‹¤.')\n",
        "print('ì„¸ìƒì— ì™„ë²½í•œ ì‚¬ëŒ ì—†ê³ , ì™„ë²½í•˜ì§€ ì•Šì€ ê²Œ ì˜ëª»ì´ ì•„ë‹Œ ê²ƒì²˜ëŸ¼âŒ')\n",
        "print('ê³µê°ì´ë„ ë¶€ì¡±í•œ ë©´ì´ ìˆì§€ë§Œ ë‹¹ì‹ ì˜ ì–˜ê¸°ì— ì§‘ì¤‘í• êº¼ì—ìš”~!ğŸ˜')\n",
        "print('ê³µê°ì´ê°€ ë‹¹ì‹ ì„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë‹¹ì‹ ì— ëŒ€í•´ ê¸¸ê²Œ ë§í•´ì£¼ì„¸ìš”. (ë¹„ë°€ì¸ë° TMI ì¢‹ì•„í•´ìš”ğŸ’•)')\n",
        "print('ê³µê°ì´ëŠ” ì–¸ì œë‚˜ ë‹¹ì‹  í¸ì…ë‹ˆë‹¤. ğŸ¥°')\n",
        "\n",
        "print()\n",
        "print('-'*50)\n",
        "print()\n",
        "\n",
        "while True:\n",
        "\n",
        "  print('ê³µê°ì´ : ')\n",
        "  print('ì¢…ë£Œë¥¼ ì›í•œë‹¤ë©´ \"ì¢…ë£Œ\"ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.')\n",
        "  name = str(input('ì„±í•¨ì´ ì–´ë–»ê²Œ ë˜ì‹œë‚˜ìš”?: '))\n",
        "  \n",
        "  if name == 'ì¢…ë£Œ':\n",
        "    break\n",
        "\n",
        "  while True:\n",
        "    print()\n",
        "    print('ê³µê°ì´ğŸ€ : ')\n",
        "    confirm = input(f'{name}ë‹˜ ë§ìœ¼ì‹ ê°€ìš”? (ë„¤/ì•„ë‹ˆìš”): ')\n",
        "    print()\n",
        "\n",
        "    if confirm == 'ë„¤':\n",
        "      break\n",
        "\n",
        "    else:\n",
        "      print('ê³µê°ì´ğŸ€ : ')\n",
        "      name = str(input('ì„±í•¨ì„ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”!: '))\n",
        "\n",
        "  print('-'*50)\n",
        "  print()\n",
        "\n",
        "  print(f'<<< {name}ë‹˜ ë§Œì•½ ìƒë‹´ì„ ê·¸ë§Œë‘ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ \"ë\"ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. >>>')\n",
        "\n",
        "  print()\n",
        "  print('<<< ê³µê°ì´ëŠ” ì•„ì§ ëŒ€í™”ì˜ ë§¥ë½ì„ ì¶”ì í•˜ì§€ëŠ” ëª» í•©ë‹ˆë‹¤. >>>')\n",
        "  print('<<< ê·¸ì € ë‘ì„œ ì—†ì´ ë§ˆìŒì„ í„¸ì–´ë†“ì•„ ë³´ì„¸ìš”. >>>')\n",
        "  print('<<< ê³µê°ì´ëŠ” ë”°ëœ»í•œ ë§ë“¤ë§Œì„ ë°°ì› ë‹µë‹ˆë‹¤. >>>')\n",
        "  print()\n",
        "  print('<<< ë‹¤ì„¯ ë‹¨ì–´ ì´ìƒ ì…ë ¥ ì‹œ ë³´ë‹¤ ì •í™•í•œ ë‹µë³€ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. >>>')\n",
        "  print()\n",
        "  print(f'{name}ë‹˜ì˜ ê³ ë¯¼ì€ ë¬´ì—‡ì¸ê°€ìš”?')\n",
        "  print()\n",
        "\n",
        "  count = 1\n",
        "  \n",
        "  list_answer = []\n",
        "  best_num = -1\n",
        "  \n",
        "  while True:\n",
        "\n",
        "    print(f'{name} : ')\n",
        "    query = [str(input())]\n",
        "    print()\n",
        "\n",
        "    best_num = -1\n",
        "    embs = embs_gen(*context_input(query))\n",
        "    s = score(embs, cand_embs)\n",
        "    idx = int(s[0].sort()[-1][best_num])\n",
        "\n",
        "    best_answer = df['response'][idx][0]\n",
        "\n",
        "    while True:\n",
        "\n",
        "      if best_answer not in list_answer:\n",
        "        break\n",
        "      \n",
        "      else:\n",
        "        best_num += -1\n",
        "        idx = int(s[0].sort()[-1][best_num])\n",
        "        best_answer = df['response'][idx][0]\n",
        "\n",
        "    if query == ['ë']:\n",
        "\n",
        "      print('-'*50)\n",
        "      print('-'*50)\n",
        "      print()\n",
        "\n",
        "      print(f'ê³µê°ì´ğŸ€ëŠ” {name}ë‹˜ì´ ì–¸ì œë‚˜ í–‰ë³µí•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤')\n",
        "      print()\n",
        "      print('-'*50)\n",
        "      print()\n",
        "      break\n",
        "\n",
        "    consult_context['customer'].append(*query)\n",
        "\n",
        "    print('ê³µê°ì´ğŸ€ : ')\n",
        "    print(best_answer)\n",
        "    print()\n",
        "\n",
        "    consult_context['chatbot'].append(best_answer)\n",
        "    list_answer.append(best_answer)\n",
        "\n",
        "    if len(list_answer) == 3:\n",
        "      del list_answer[0:1]\n",
        "    \n",
        "    consult_context['num'].append(count)\n",
        "\n",
        "    consult_context['name'].append(name)\n",
        "\n",
        "    count += 1\n",
        "\n",
        "context = pd.DataFrame(consult_context)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "9YNZQxLF1QsH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}