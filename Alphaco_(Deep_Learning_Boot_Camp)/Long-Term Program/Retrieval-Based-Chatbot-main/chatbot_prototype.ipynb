{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot-prototype.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMJ/cWczEcLlMcn4oeB1f0o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dotsnangles/Retrieval-Based-Chatbot/blob/main/chatbot_prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Click here and do Shift + Enter 3 times!"
      ],
      "metadata": {
        "id": "gTpU8rVA0uhv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-ZyBZPWfX5m",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!pip install -q transformers datasets folium==0.2.1\n",
        "!git clone https://github.com/dotsnangles/Poly-Encoder.git\n",
        "%cd /content/Poly-Encoder\n",
        "\n",
        "!gdown -q --folder 1Ipr-aNF5ELMY0HTXAmeV26LlgktKUfmG\n",
        "!gdown -q --folder 1RH7laK4WlucCw68ZeExFvyg7vs-kB_x3\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer, AutoModel\n",
        "from encoder import PolyEncoder\n",
        "from transform import SelectionJoinTransform, SelectionSequentialTransform\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)\n",
        "\n",
        "PATH = '/content/Poly-Encoder/chatbot_output/poly_16_pytorch_model.bin'\n",
        "\n",
        "bert_name = 'klue/bert-base'\n",
        "bert_config = BertConfig.from_pretrained(bert_name)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_name)\n",
        "tokenizer.add_tokens(['\\n'], special_tokens=True)\n",
        "\n",
        "context_transform = SelectionJoinTransform(tokenizer=tokenizer, max_len=256)\n",
        "response_transform = SelectionSequentialTransform(tokenizer=tokenizer, max_len=128)\n",
        "\n",
        "bert = BertModel.from_pretrained(bert_name, config=bert_config)\n",
        "\n",
        "model = PolyEncoder(bert_config, bert=bert, poly_m=16)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.to(device)\n",
        "model.device\n",
        "\n",
        "context = ['This framework generates embeddings for each input sentence', \n",
        "            'Sentences are passed as a list of string.', \n",
        "            'The quick brown fox jumps over the lazy dog.']\n",
        "\n",
        "candidates = ['This framework generates embeddings for each input sentence', \n",
        "            'Sentences are passed as a list of string.', \n",
        "            'The quick brown fox jumps over the lazy dog.']\n",
        "\n",
        "def context_input(context):\n",
        "    context_input_ids, context_input_masks = context_transform(context)\n",
        "    contexts_token_ids_list_batch, contexts_input_masks_list_batch = [context_input_ids], [context_input_masks]\n",
        "\n",
        "    long_tensors = [contexts_token_ids_list_batch, contexts_input_masks_list_batch]\n",
        "\n",
        "    contexts_token_ids_list_batch, contexts_input_masks_list_batch = (torch.tensor(t, dtype=torch.long, device=device) for t in long_tensors)\n",
        "\n",
        "    return contexts_token_ids_list_batch, contexts_input_masks_list_batch\n",
        "contexts_token_ids_list_batch, contexts_input_masks_list_batch = context_input(context)\n",
        "\n",
        "def response_input(candidates):\n",
        "    responses_token_ids_list, responses_input_masks_list = response_transform(candidates)\n",
        "    responses_token_ids_list_batch, responses_input_masks_list_batch = [responses_token_ids_list], [responses_input_masks_list]\n",
        "\n",
        "    long_tensors = [responses_token_ids_list_batch, responses_input_masks_list_batch]\n",
        "\n",
        "    responses_token_ids_list_batch, responses_input_masks_list_batch = (torch.tensor(t, dtype=torch.long, device=device) for t in long_tensors)\n",
        "\n",
        "    return responses_token_ids_list_batch, responses_input_masks_list_batch\n",
        "responses_token_ids_list_batch, responses_input_masks_list_batch = response_input(candidates)\n",
        "\n",
        "def embs_gen(contexts_token_ids_list_batch, contexts_input_masks_list_batch):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        \n",
        "        ctx_out = model.bert(contexts_token_ids_list_batch, contexts_input_masks_list_batch)[0]  # [bs, length, dim]\n",
        "        poly_code_ids = torch.arange(model.poly_m, dtype=torch.long).to(contexts_token_ids_list_batch.device)\n",
        "        poly_code_ids = poly_code_ids.unsqueeze(0).expand(1, model.poly_m)\n",
        "        poly_codes = model.poly_code_embeddings(poly_code_ids) # [bs, poly_m, dim]\n",
        "        embs = model.dot_attention(poly_codes, ctx_out, ctx_out) # [bs, poly_m, dim]\n",
        "\n",
        "        return embs\n",
        "embs = embs_gen(contexts_token_ids_list_batch, contexts_input_masks_list_batch)\n",
        "\n",
        "def cand_emb_gen(responses_token_ids_list_batch, responses_input_masks_list_batch):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "                \n",
        "        batch_size, res_cnt, seq_length = responses_token_ids_list_batch.shape # res_cnt is 1 during training\n",
        "        responses_token_ids_list_batch = responses_token_ids_list_batch.view(-1, seq_length)\n",
        "        responses_input_masks_list_batch = responses_input_masks_list_batch.view(-1, seq_length)\n",
        "        cand_emb = model.bert(responses_token_ids_list_batch, responses_input_masks_list_batch)[0][:,0,:] # [bs, dim]\n",
        "        cand_emb = cand_emb.view(batch_size, res_cnt, -1) # [bs, res_cnt, dim]\n",
        "\n",
        "        return cand_emb\n",
        "cand_emb = cand_emb_gen(responses_token_ids_list_batch, responses_input_masks_list_batch)\n",
        "\n",
        "def loss(embs, cand_emb, contexts_token_ids_list_batch, responses_token_ids_list_batch):\n",
        "    batch_size, res_cnt, seq_length = responses_token_ids_list_batch.shape\n",
        "\n",
        "    ctx_emb = model.dot_attention(cand_emb, embs, embs) # [bs, bs, dim]\n",
        "    # print(ctx_emb)\n",
        "    ctx_emb = ctx_emb.squeeze()\n",
        "    # print(ctx_emb)\n",
        "    dot_product = (ctx_emb*cand_emb) # [bs, bs]\n",
        "    # print(dot_product)\n",
        "    dot_product = dot_product.sum(-1)\n",
        "    print(dot_product)\n",
        "    mask = torch.eye(batch_size).to(contexts_token_ids_list_batch.device) # [bs, bs]\n",
        "    print(mask)\n",
        "    loss = F.log_softmax(dot_product, dim=-1)\n",
        "    print(loss)\n",
        "    loss = loss * mask\n",
        "    print(loss)\n",
        "    loss = (-loss.sum(dim=1))\n",
        "    print(loss)\n",
        "    loss = loss.mean()\n",
        "    print(loss)\n",
        "    return loss\n",
        "# loss_ = loss(embs, cand_emb, contexts_token_ids_list_batch, responses_token_ids_list_batch)\n",
        "\n",
        "def score(embs, cand_emb):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        ctx_emb = model.dot_attention(cand_emb, embs, embs) # [bs, res_cnt, dim]\n",
        "        dot_product = (ctx_emb*cand_emb).sum(-1)\n",
        "        \n",
        "        return dot_product\n",
        "score_ = score(embs, cand_emb)\n",
        "\n",
        "# forward\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    \n",
        "    model_foward = model(contexts_token_ids_list_batch, contexts_input_masks_list_batch, responses_token_ids_list_batch, responses_input_masks_list_batch)\n",
        "\n",
        "### 데이터 검증\n",
        "import pickle\n",
        "\n",
        "with open('/content/Poly-Encoder/감성대화챗봇데이터/train_data_source.pickle', 'rb') as f:\n",
        "    train = pickle.load(f)\n",
        "with open('/content/Poly-Encoder/감성대화챗봇데이터/val_data_source.pickle', 'rb') as f:\n",
        "    dev = pickle.load(f)\n",
        "# index = 500\n",
        "# train[index]['context']\n",
        "# train[index]['responses']\n",
        "# dev[index]['context']\n",
        "# dev[index]['responses']\n",
        "\n",
        "### 챗봇 테이블 생성\n",
        "data = {\n",
        "    'context' : [],\n",
        "    'response': []\n",
        "}\n",
        "\n",
        "for sample in train:\n",
        "    data['context'].append(sample['context'])\n",
        "    data['response'].append([sample['responses'][0]])\n",
        "# len(data['context']), len(data['response'])\n",
        "# idx = 400\n",
        "# print(data['context'][idx])\n",
        "# print(data['response'][idx])\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "### generate cand_embs & create tensor table\n",
        "# response_input_srs = df['response'].apply(response_input)\n",
        "# response_input_lst = response_input_srs.to_list()\n",
        "\n",
        "# cand_embs_lst = []\n",
        "# for sample in response_input_lst:\n",
        "#     cand_embs_lst.append(cand_emb_gen(*sample).to('cpu'))\n",
        "# df['response embedding'] = cand_embs_lst\n",
        "# df[['response', 'response embedding']]\n",
        "# cand_embs = cand_embs_lst[0]\n",
        "# for idx in range(1, len(cand_embs_lst)):\n",
        "#     y = cand_embs_lst[idx]\n",
        "#     cand_embs = torch.cat((cand_embs, y), 1)\n",
        "# cand_embs = cand_embs.to(device)\n",
        "\n",
        "import pickle\n",
        "with open('/content/Poly-Encoder/감성대화챗봇데이터/cand_embs.pickle', 'rb') as f:\n",
        "    cand_embs = pickle.load(f)\n",
        "cand_embs.to(device)\n",
        "\n",
        "### generate context_embs\n",
        "query = ['너무 성급한 결정을 한 것 같아.']\n",
        "embs = embs_gen(*context_input(query))\n",
        "\n",
        "### Score & Retrieve\n",
        "import time\n",
        "start = time.time()\n",
        "s = score(embs, cand_embs)\n",
        "end = time.time()\n",
        "idx = s.argmax(1)\n",
        "idx = int(idx[0])\n",
        "# df['response'][idx]\n",
        "# df.iloc[idx]['context']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "### Chatbot UI\n",
        "consult_context = {\n",
        "    'num':[],\n",
        "    'name': [],\n",
        "    'customer':[],\n",
        "    'chatbot':[]\n",
        "}\n",
        "\n",
        "print('안녕하세요. 공감 만땅이~~⭐️ 공감이🍀 입니다.')\n",
        "print('세상에 완벽한 사람 없고, 완벽하지 않은 게 잘못이 아닌 것처럼❌')\n",
        "print('공감이도 부족한 면이 있지만 당신의 얘기에 집중할꺼에요~!😎')\n",
        "print('공감이가 당신을 이해할 수 있도록 당신에 대해 길게 말해주세요. (비밀인데 TMI 좋아해요💕)')\n",
        "print('공감이는 언제나 당신 편입니다. 🥰')\n",
        "\n",
        "print()\n",
        "print('-'*50)\n",
        "print()\n",
        "\n",
        "while True:\n",
        "\n",
        "  print('공감이 : ')\n",
        "  print('종료를 원한다면 \"종료\"를 입력해주세요.')\n",
        "  name = str(input('성함이 어떻게 되시나요?: '))\n",
        "  \n",
        "  if name == '종료':\n",
        "    break\n",
        "\n",
        "  while True:\n",
        "    print()\n",
        "    print('공감이🍀 : ')\n",
        "    confirm = input(f'{name}님 맞으신가요? (네/아니요): ')\n",
        "    print()\n",
        "\n",
        "    if confirm == '네':\n",
        "      break\n",
        "\n",
        "    else:\n",
        "      print('공감이🍀 : ')\n",
        "      name = str(input('성함을 다시 입력해주세요!: '))\n",
        "\n",
        "  print('-'*50)\n",
        "  print()\n",
        "\n",
        "  print(f'<<< {name}님 만약 상담을 그만두고 싶으시다면 \"끝\"를 입력해주세요. >>>')\n",
        "\n",
        "  print()\n",
        "  print('<<< 공감이는 아직 대화의 맥락을 추적하지는 못 합니다. >>>')\n",
        "  print('<<< 그저 두서 없이 마음을 털어놓아 보세요. >>>')\n",
        "  print('<<< 공감이는 따뜻한 말들만을 배웠답니다. >>>')\n",
        "  print()\n",
        "  print('<<< 다섯 단어 이상 입력 시 보다 정확한 답변이 가능합니다. >>>')\n",
        "  print()\n",
        "  print(f'{name}님의 고민은 무엇인가요?')\n",
        "  print()\n",
        "\n",
        "  count = 1\n",
        "  \n",
        "  list_answer = []\n",
        "  best_num = -1\n",
        "  \n",
        "  while True:\n",
        "\n",
        "    print(f'{name} : ')\n",
        "    query = [str(input())]\n",
        "    print()\n",
        "\n",
        "    best_num = -1\n",
        "    embs = embs_gen(*context_input(query))\n",
        "    s = score(embs, cand_embs)\n",
        "    idx = int(s[0].sort()[-1][best_num])\n",
        "\n",
        "    best_answer = df['response'][idx][0]\n",
        "\n",
        "    while True:\n",
        "\n",
        "      if best_answer not in list_answer:\n",
        "        break\n",
        "      \n",
        "      else:\n",
        "        best_num += -1\n",
        "        idx = int(s[0].sort()[-1][best_num])\n",
        "        best_answer = df['response'][idx][0]\n",
        "\n",
        "    if query == ['끝']:\n",
        "\n",
        "      print('-'*50)\n",
        "      print('-'*50)\n",
        "      print()\n",
        "\n",
        "      print(f'공감이🍀는 {name}님이 언제나 행복하시길 바랍니다. 감사합니다')\n",
        "      print()\n",
        "      print('-'*50)\n",
        "      print()\n",
        "      break\n",
        "\n",
        "    consult_context['customer'].append(*query)\n",
        "\n",
        "    print('공감이🍀 : ')\n",
        "    print(best_answer)\n",
        "    print()\n",
        "\n",
        "    consult_context['chatbot'].append(best_answer)\n",
        "    list_answer.append(best_answer)\n",
        "\n",
        "    if len(list_answer) == 3:\n",
        "      del list_answer[0:1]\n",
        "    \n",
        "    consult_context['num'].append(count)\n",
        "\n",
        "    consult_context['name'].append(name)\n",
        "\n",
        "    count += 1\n",
        "\n",
        "context = pd.DataFrame(consult_context)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "9YNZQxLF1QsH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}