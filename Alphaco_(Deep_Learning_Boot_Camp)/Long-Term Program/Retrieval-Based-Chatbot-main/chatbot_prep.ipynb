{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot-prep.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOKtHGLxCPwRQOdlEfK0E8Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dotsnangles/Retrieval-Based-Chatbot/blob/main/chatbot_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HXPA7-uDAIHP"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/Poly-Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnEJHV_6YR4k",
        "outputId": "268fb4e0-b640-4d77-db24-3f560a0dd417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jul 10 15:35:00 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZXufVK0iRN3Z"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n-ZyBZPWfX5m"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets folium==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4lrYnocYNH4",
        "outputId": "a24b35c0-84ab-469e-bcf8-553d78d36d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Poly-Encoder'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 129 (delta 41), reused 20 (delta 8), pack-reused 57\u001b[K\n",
            "Receiving objects: 100% (129/129), 40.70 KiB | 10.17 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n",
            "/content/Poly-Encoder\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dotsnangles/Poly-Encoder.git\n",
        "%cd /content/Poly-Encoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown -q --folder 1Ipr-aNF5ELMY0HTXAmeV26LlgktKUfmG\n",
        "!gdown -q --folder 1RH7laK4WlucCw68ZeExFvyg7vs-kB_x3"
      ],
      "metadata": {
        "id": "LVqpyud7T6Cg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWnJ129TxQSl",
        "outputId": "e4b23f09-285a-47ec-aba8-64dc714bc497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer, AutoModel\n",
        "from encoder import PolyEncoder\n",
        "from transform import SelectionJoinTransform, SelectionSequentialTransform\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)\n",
        "\n",
        "PATH = '/content/Poly-Encoder/chatbot_output/poly_16_pytorch_model.bin'\n",
        "\n",
        "bert_name = 'klue/bert-base'\n",
        "bert_config = BertConfig.from_pretrained(bert_name)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_name)\n",
        "tokenizer.add_tokens(['\\n'], special_tokens=True)\n",
        "\n",
        "context_transform = SelectionJoinTransform(tokenizer=tokenizer, max_len=256)\n",
        "response_transform = SelectionSequentialTransform(tokenizer=tokenizer, max_len=128)\n",
        "\n",
        "bert = BertModel.from_pretrained(bert_name, config=bert_config)\n",
        "\n",
        "model = PolyEncoder(bert_config, bert=bert, poly_m=16)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.to(device)\n",
        "model.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rapB1afogMhI"
      },
      "outputs": [],
      "source": [
        "context = ['This framework generates embeddings for each input sentence', \n",
        "            'Sentences are passed as a list of string.', \n",
        "            'The quick brown fox jumps over the lazy dog.']\n",
        "\n",
        "candidates = ['This framework generates embeddings for each input sentence', \n",
        "            'Sentences are passed as a list of string.', \n",
        "            'The quick brown fox jumps over the lazy dog.']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def context_input(context):\n",
        "    context_input_ids, context_input_masks = context_transform(context)\n",
        "    contexts_token_ids_list_batch, contexts_input_masks_list_batch = [context_input_ids], [context_input_masks]\n",
        "\n",
        "    long_tensors = [contexts_token_ids_list_batch, contexts_input_masks_list_batch]\n",
        "\n",
        "    contexts_token_ids_list_batch, contexts_input_masks_list_batch = (torch.tensor(t, dtype=torch.long, device=device) for t in long_tensors)\n",
        "\n",
        "    return contexts_token_ids_list_batch, contexts_input_masks_list_batch"
      ],
      "metadata": {
        "id": "YJ90g8I50Mkt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contexts_token_ids_list_batch, contexts_input_masks_list_batch = context_input(context)"
      ],
      "metadata": {
        "id": "oIDdtf9M1JD_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response_input(candidates):\n",
        "    responses_token_ids_list, responses_input_masks_list = response_transform(candidates)\n",
        "    responses_token_ids_list_batch, responses_input_masks_list_batch = [responses_token_ids_list], [responses_input_masks_list]\n",
        "\n",
        "    long_tensors = [responses_token_ids_list_batch, responses_input_masks_list_batch]\n",
        "\n",
        "    responses_token_ids_list_batch, responses_input_masks_list_batch = (torch.tensor(t, dtype=torch.long, device=device) for t in long_tensors)\n",
        "\n",
        "    return responses_token_ids_list_batch, responses_input_masks_list_batch"
      ],
      "metadata": {
        "id": "mfVz-wOI0m0J"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responses_token_ids_list_batch, responses_input_masks_list_batch = response_input(candidates)"
      ],
      "metadata": {
        "id": "cTG-RK8Z1Jd8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "e_ELL57RV6vb"
      },
      "outputs": [],
      "source": [
        "def embs_gen(contexts_token_ids_list_batch, contexts_input_masks_list_batch):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        \n",
        "        ctx_out = model.bert(contexts_token_ids_list_batch, contexts_input_masks_list_batch)[0]  # [bs, length, dim]\n",
        "        poly_code_ids = torch.arange(model.poly_m, dtype=torch.long).to(contexts_token_ids_list_batch.device)\n",
        "        poly_code_ids = poly_code_ids.unsqueeze(0).expand(1, model.poly_m)\n",
        "        poly_codes = model.poly_code_embeddings(poly_code_ids) # [bs, poly_m, dim]\n",
        "        embs = model.dot_attention(poly_codes, ctx_out, ctx_out) # [bs, poly_m, dim]\n",
        "\n",
        "        return embs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embs = embs_gen(contexts_token_ids_list_batch, contexts_input_masks_list_batch)"
      ],
      "metadata": {
        "id": "IG58NhFdYW8q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cand_emb_gen(responses_token_ids_list_batch, responses_input_masks_list_batch):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "                \n",
        "        batch_size, res_cnt, seq_length = responses_token_ids_list_batch.shape # res_cnt is 1 during training\n",
        "        responses_token_ids_list_batch = responses_token_ids_list_batch.view(-1, seq_length)\n",
        "        responses_input_masks_list_batch = responses_input_masks_list_batch.view(-1, seq_length)\n",
        "        cand_emb = model.bert(responses_token_ids_list_batch, responses_input_masks_list_batch)[0][:,0,:] # [bs, dim]\n",
        "        cand_emb = cand_emb.view(batch_size, res_cnt, -1) # [bs, res_cnt, dim]\n",
        "\n",
        "        return cand_emb"
      ],
      "metadata": {
        "id": "vwA8KNRm9_K4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cand_emb = cand_emb_gen(responses_token_ids_list_batch, responses_input_masks_list_batch)"
      ],
      "metadata": {
        "id": "TWCWgALsYZN0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(embs, cand_emb, contexts_token_ids_list_batch, responses_token_ids_list_batch):\n",
        "    batch_size, res_cnt, seq_length = responses_token_ids_list_batch.shape\n",
        "\n",
        "    ctx_emb = model.dot_attention(cand_emb, embs, embs) # [bs, bs, dim]\n",
        "    # print(ctx_emb)\n",
        "    ctx_emb = ctx_emb.squeeze()\n",
        "    # print(ctx_emb)\n",
        "    dot_product = (ctx_emb*cand_emb) # [bs, bs]\n",
        "    # print(dot_product)\n",
        "    dot_product = dot_product.sum(-1)\n",
        "    print(dot_product)\n",
        "    mask = torch.eye(batch_size).to(contexts_token_ids_list_batch.device) # [bs, bs]\n",
        "    print(mask)\n",
        "    loss = F.log_softmax(dot_product, dim=-1)\n",
        "    print(loss)\n",
        "    loss = loss * mask\n",
        "    print(loss)\n",
        "    loss = (-loss.sum(dim=1))\n",
        "    print(loss)\n",
        "    loss = loss.mean()\n",
        "    print(loss)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "y-VtzBuPfvwI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_ = loss(embs, cand_emb, contexts_token_ids_list_batch, responses_token_ids_list_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNptUQDOgYlh",
        "outputId": "5866fca6-21d9-4b6d-f777-913f08d2c712"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[498.3091, 500.8978, 504.1481]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[-5.8798, -3.2911, -0.0408]], device='cuda:0')\n",
            "tensor([[-5.8798, -3.2911, -0.0408]], device='cuda:0')\n",
            "tensor([9.2118], device='cuda:0')\n",
            "tensor(9.2118, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9.2118, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def score(embs, cand_emb):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        ctx_emb = model.dot_attention(cand_emb, embs, embs) # [bs, res_cnt, dim]\n",
        "        dot_product = (ctx_emb*cand_emb).sum(-1)\n",
        "        \n",
        "        return dot_product"
      ],
      "metadata": {
        "id": "B47Tw6vTKYr0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_ = score(embs, cand_emb)"
      ],
      "metadata": {
        "id": "JnKdzX6JYifc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a50adcc-d4ac-458b-e510-5d4663199ed6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[498.3091, 500.8978, 504.1481]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# forward\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    \n",
        "    model_foward = model(contexts_token_ids_list_batch, contexts_input_masks_list_batch, responses_token_ids_list_batch, responses_input_masks_list_batch)"
      ],
      "metadata": {
        "id": "IwPAL7K4ymBK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ddd6b6-a988-4b3c-f09d-13661644332e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[498.3091, 500.8978, 504.1481]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 검증"
      ],
      "metadata": {
        "id": "eI9iOlJMqnsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/Poly-Encoder/감성대화챗봇데이터/train_data_source.pickle', 'rb') as f:\n",
        "    train = pickle.load(f)\n",
        "with open('/content/Poly-Encoder/감성대화챗봇데이터/val_data_source.pickle', 'rb') as f:\n",
        "    dev = pickle.load(f)"
      ],
      "metadata": {
        "id": "vVRqi7D4Z9cO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index = 500"
      ],
      "metadata": {
        "id": "-Mss6BMYhtHR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train[index]['context']"
      ],
      "metadata": {
        "id": "EUXvHneNa2Y9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train[index]['responses']"
      ],
      "metadata": {
        "id": "qGzO7GhBhoZh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev[index]['context']"
      ],
      "metadata": {
        "id": "-HFg-Z4ca_RS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dev[index]['responses']"
      ],
      "metadata": {
        "id": "9qZBxsx2hhVz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 챗봇 테이블 생성"
      ],
      "metadata": {
        "id": "9xoeSM1zhMC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'context' : [],\n",
        "    'response': []\n",
        "}"
      ],
      "metadata": {
        "id": "V-Z7ng-sezA-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train:\n",
        "    data['context'].append(sample['context'])\n",
        "    data['response'].append([sample['responses'][0]])"
      ],
      "metadata": {
        "id": "QmyigaB7O3dD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(data['context']), len(data['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ooz_ZzGTed2r",
        "outputId": "8637a80c-ea9b-4874-e70b-91a722fb89a7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40879, 40879)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# idx = 400"
      ],
      "metadata": {
        "id": "I5wdEOXlfe00"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(data['context'][idx])\n",
        "# print(data['response'][idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aM5F5RPfQSL",
        "outputId": "83f14362-bad4-428f-d899-74b8e66385fb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['participant 1: 성적이 오르지 않아 고민이었는데 담임 선생님께서 도움을 많이 주셔서 매우 감사해.', 'participant 2: 담임 선생님께서 도움을 많이 주셨군요. 감사한 마음을 어떻게 표현하면 좋을까요?', 'participant 1: 글쎄. 어떻게 전해야 할지 몰라서 정말 고민이야.']\n",
            "['감사를 표할 방법에 대해 고민되시나 보네요.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "koG4IEVOflzu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "fMBcD90zf6e1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generate cand_embs & create tensor table"
      ],
      "metadata": {
        "id": "A8_V35VIbw8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# response_input_srs = df['response'].apply(response_input)\n",
        "# response_input_lst = response_input_srs.to_list()\n",
        "\n",
        "# cand_embs_lst = []\n",
        "# for sample in response_input_lst:\n",
        "#     cand_embs_lst.append(cand_emb_gen(*sample).to('cpu'))"
      ],
      "metadata": {
        "id": "Dn84_3gmhatF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['response embedding'] = cand_embs_lst\n",
        "# df[['response', 'response embedding']]"
      ],
      "metadata": {
        "id": "6EeK3Dk2cxMb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cand_embs = cand_embs_lst[0]\n",
        "# for idx in range(1, len(cand_embs_lst)):\n",
        "#     y = cand_embs_lst[idx]\n",
        "#     cand_embs = torch.cat((cand_embs, y), 1)"
      ],
      "metadata": {
        "id": "85FF9C7SytjY"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cand_embs = cand_embs.to(device)"
      ],
      "metadata": {
        "id": "K6AauDgXf1A_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/Poly-Encoder/감성대화챗봇데이터/cand_embs.pickle', 'rb') as f:\n",
        "    cand_embs = pickle.load(f)\n",
        "cand_embs.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4NPzhPkqLUM",
        "outputId": "832eca6f-f06f-4e53-c041-f5915c814b65"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.4922, -0.6527,  0.3834,  ..., -0.2355, -0.0816,  0.7831],\n",
              "         [ 0.4265, -0.7164,  0.4097,  ..., -0.4494,  0.0708,  1.1537],\n",
              "         [ 0.3295, -0.6451,  0.3668,  ..., -0.4229,  0.1024,  1.1591],\n",
              "         ...,\n",
              "         [ 0.7648, -0.7776,  0.2120,  ..., -0.2492,  0.2255,  0.7545],\n",
              "         [ 0.0553, -0.9839,  0.3826,  ..., -0.7569, -0.0208,  0.4652],\n",
              "         [ 0.4029, -0.7795,  0.4000,  ..., -0.3931,  0.0195,  0.8489]]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generate context_embs"
      ],
      "metadata": {
        "id": "KmQ0JqFmb4uM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = ['너무 성급한 결정을 한 것 같아.']"
      ],
      "metadata": {
        "id": "ck2UuttScy62"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embs = embs_gen(*context_input(query))"
      ],
      "metadata": {
        "id": "8swHCEtNb40O"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Score & Retrieve"
      ],
      "metadata": {
        "id": "n8iWvITPcoGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "Y6H5nF6klWHb"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "s = score(embs, cand_embs)\n",
        "end = time.time()"
      ],
      "metadata": {
        "id": "-pK9_nqIb46y"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = s.argmax(1)"
      ],
      "metadata": {
        "id": "DNoLrEhDb4-Z"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = int(idx[0])"
      ],
      "metadata": {
        "id": "5j89jHx4jQcY"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['response'][idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqz9nXgXjfqv",
        "outputId": "dc346be3-ac6a-47a1-f80d-fb95dc47fef4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['섣부른 결정에 후회하고 계시는군요. 어떻게 하고 싶으세요?']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[idx]['context']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw81DuOwkHrV",
        "outputId": "9871a340-f739-4112-ed61-4d0861756ac5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['participant 1: 가정 형편에 맞는 차를 계약하려 했는데 자동차 판매사원 판매 전략에 넘어가 버렸어.',\n",
              " 'participant 2: 예산을 초과하는 금액을 지출하게 되셨군요.',\n",
              " 'participant 1: 소나타 차량을 생각하고 갔는데 신형 그랜저에 앉아 보고서 마음이 변했지 뭐야.',\n",
              " 'participant 2: 좋은 걸 사고 싶은 마음과 재정 걱정에 혼란스러우셨겠어요.',\n",
              " 'participant 1: 집에 와서 다시 생각해보니 잘못 결정한 것 같아.']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chatbot UI"
      ],
      "metadata": {
        "id": "0HIG1epfTbmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "consult_context = {\n",
        "    'num':[],\n",
        "    'name': [],\n",
        "    'customer':[],\n",
        "    'chatbot':[]\n",
        "}\n",
        "\n",
        "print('안녕하세요. 공감 만땅이~~⭐️ 공감이🍀 입니다.')\n",
        "print('세상에 완벽한 사람 없고, 완벽하지 않은 게 잘못이 아닌 것처럼❌')\n",
        "print('공감이도 부족한 면이 있지만 당신의 얘기에 집중할꺼에요~!😎')\n",
        "print('공감이가 당신을 이해할 수 있도록 당신에 대해 길게 말해주세요. (비밀인데 TMI 좋아해요💕)')\n",
        "print('공감이는 언제나 당신 편입니다. 🥰')\n",
        "\n",
        "print()\n",
        "print('-'*50)\n",
        "print()\n",
        "\n",
        "while True:\n",
        "\n",
        "  print('공감이 : ')\n",
        "  print('종료를 원한다면 \"종료\"를 입력해주세요.')\n",
        "  name = str(input('성함이 어떻게 되시나요?: '))\n",
        "  \n",
        "  if name == '종료':\n",
        "    break\n",
        "\n",
        "  while True:\n",
        "    print()\n",
        "    print('공감이🍀 : ')\n",
        "    confirm = input(f'{name}님 맞으신가요? (네/아니요): ')\n",
        "    print()\n",
        "\n",
        "    if confirm == '네':\n",
        "      break\n",
        "\n",
        "    else:\n",
        "      print('공감이🍀 : ')\n",
        "      name = str(input('성함을 다시 입력해주세요!: '))\n",
        "\n",
        "  print('-'*50)\n",
        "  print()\n",
        "\n",
        "  print(f'{name}님 만약 상담을 그만두고 싶으시다면 \"끝\"를 입력해주세요.')\n",
        "\n",
        "  print()\n",
        "  print(f'{name}님의 고민은 무엇인가요?')\n",
        "  print()\n",
        "\n",
        "  count = 1\n",
        "  \n",
        "  list_answer = []\n",
        "  best_num = -1\n",
        "  \n",
        "  while True:\n",
        "\n",
        "    print(f'{name} : ')\n",
        "    query = [str(input())]\n",
        "    print()\n",
        "\n",
        "    best_num = -1\n",
        "    embs = embs_gen(*context_input(query))\n",
        "    s = score(embs, cand_embs)\n",
        "    idx = int(s[0].sort()[-1][best_num])\n",
        "\n",
        "    best_answer = df['response'][idx][0]\n",
        "\n",
        "    while True:\n",
        "\n",
        "      if best_answer not in list_answer:\n",
        "        break\n",
        "      \n",
        "      else:\n",
        "        best_num += -1\n",
        "        idx = int(s[0].sort()[-1][best_num])\n",
        "        best_answer = df['response'][idx][0]\n",
        "\n",
        "    if query == ['끝']:\n",
        "\n",
        "      print('-'*50)\n",
        "      print('-'*50)\n",
        "      print()\n",
        "\n",
        "      print(f'공감이🍀는 {name}님이 언제나 행복하시길 바랍니다. 감사합니다')\n",
        "      print()\n",
        "      print('-'*50)\n",
        "      print()\n",
        "      break\n",
        "\n",
        "    consult_context['customer'].append(*query)\n",
        "\n",
        "    print('공감이🍀 : ')\n",
        "    print(best_answer)\n",
        "    print()\n",
        "\n",
        "    consult_context['chatbot'].append(best_answer)\n",
        "    list_answer.append(best_answer)\n",
        "\n",
        "    if len(list_answer) == 3:\n",
        "      del list_answer[0:1]\n",
        "    \n",
        "    consult_context['num'].append(count)\n",
        "\n",
        "    consult_context['name'].append(name)\n",
        "\n",
        "    count += 1\n",
        "\n",
        "context = pd.DataFrame(consult_context)"
      ],
      "metadata": {
        "id": "RJANH_5mtOUw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}