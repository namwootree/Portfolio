{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namwootree/Portfolio/blob/main/Long-Term%20Program/news_topic/Reference_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Private_2nd] Huggingface를 사용한 베이스라인\n",
        "https://dacon.io/competitions/official/235747/codeshare/3069?page=1&dtype=recent"
      ],
      "metadata": {
        "id": "7TL3P_We6-Ak"
      },
      "id": "7TL3P_We6-Ak"
    },
    {
      "cell_type": "markdown",
      "id": "0343a98b",
      "metadata": {
        "id": "0343a98b"
      },
      "source": [
        "## 시작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9892bce4",
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "9892bce4"
      },
      "outputs": [],
      "source": [
        "# ------ LIBRARY -------#\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import pandas as pd\n",
        "import re\n",
        "import cv2\n",
        "# torch\n",
        "import torch\n",
        "import torch.cuda.amp as amp\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import *\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau, MultiStepLR, OneCycleLR\n",
        "#\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import torch_optimizer as optim\n",
        "from collections import defaultdict\n",
        "import itertools as it\n",
        "\n",
        "import tqdm\n",
        "import random\n",
        "#import time\n",
        "import matplotlib.pyplot as plt\n",
        "from timeit import default_timer as timer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "# transformer\n",
        "from transformers import XLMPreTrainedModel, XLMRobertaModel, XLMRobertaConfig, XLMRobertaTokenizer\n",
        "from transformers import XLMRobertaForSequenceClassification, BertForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import BertForSequenceClassification, DistilBertForSequenceClassification, XLNetForSequenceClassification,\\\n",
        "XLMRobertaForSequenceClassification, XLMForSequenceClassification, RobertaForSequenceClassification\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f87869f9",
      "metadata": {
        "code_folding": [],
        "id": "f87869f9"
      },
      "outputs": [],
      "source": [
        "# class args\n",
        "class args:\n",
        "    # ---- factor ---- #\n",
        "    debug=False\n",
        "    amp = True\n",
        "    gpu = '0'\n",
        "    \n",
        "    epochs=10\n",
        "    batch_size=1\n",
        "    weight_decay=1e-6\n",
        "    n_fold=5\n",
        "    fold=3 # [0, 1, 2, 3, 4] # 원래는 3\n",
        "    \n",
        "    exp_name = 'experiment_name_folder'\n",
        "    dir_ = f'./saved_models/'\n",
        "    pt = 'your_model_name'\n",
        "    max_len = 33\n",
        "    \n",
        "    start_lr = 1e-5#1e-3,5e-5\n",
        "    min_lr=1e-6\n",
        "    # ---- Dataset ---- #\n",
        "\n",
        "    # ---- Else ---- #\n",
        "    num_workers=8\n",
        "    seed=2021\n",
        "    scheduler = None#'get_linear_schedule_with_warmup'\n",
        "\n",
        "\n",
        "data_dir = './'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
        "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "##----------------\n",
        "def set_seeds(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False # for faster training, but not deterministic\n",
        "\n",
        "set_seeds(seed=args.seed)    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "299e9265",
      "metadata": {
        "code_folding": [
          1,
          11
        ],
        "id": "299e9265"
      },
      "outputs": [],
      "source": [
        "# - util - #\n",
        "def get_learning_rate(optimizer):\n",
        "    lr=[]\n",
        "    for param_group in optimizer.param_groups:\n",
        "        lr +=[ param_group['lr'] ]\n",
        "\n",
        "    assert(len(lr)==1) #we support only one param_group\n",
        "    lr = lr[0]\n",
        "\n",
        "    return lr\n",
        "\n",
        "def load_data():\n",
        "    train=pd.read_csv('./train_data.csv')\n",
        "    test=pd.read_csv('./test_data.csv')\n",
        "    \n",
        "    #\n",
        "    train=train[['title','topic_idx']]\n",
        "    test=test[['title']]\n",
        "    #\n",
        "    from sklearn.model_selection import StratifiedKFold\n",
        "    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
        "    train['fold'] = -1\n",
        "    for n_fold, (_,v_idx) in enumerate(skf.split(train, train['topic_idx'])):\n",
        "        train.loc[v_idx, 'fold']  = n_fold\n",
        "    train['id'] = [x for x in range(len(train))]\n",
        "    \n",
        "    return train, test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86ed46c9",
      "metadata": {
        "heading_collapsed": true,
        "id": "86ed46c9"
      },
      "source": [
        "# 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6feb9fe2",
      "metadata": {
        "code_folding": [
          0
        ],
        "hidden": true,
        "id": "6feb9fe2"
      },
      "outputs": [],
      "source": [
        "# make KoBertTokenizer\n",
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from shutil import copyfile\n",
        " \n",
        "from transformers import PreTrainedTokenizer\n",
        " \n",
        "logger = logging.getLogger(__name__)\n",
        " \n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\n",
        " \n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    \"vocab_file\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
        "    },\n",
        "    \"vocab_txt\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
        "    }\n",
        "}\n",
        " \n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    \"monologg/kobert\": 512,\n",
        "    \"monologg/kobert-lm\": 512,\n",
        "    \"monologg/distilkobert\": 512\n",
        "}\n",
        " \n",
        "PRETRAINED_INIT_CONFIGURATION = {\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
        "}\n",
        " \n",
        "SPIECE_UNDERLINE = u'▁'\n",
        " \n",
        "class KoBertTokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "        SentencePiece based tokenizer. Peculiarities:\n",
        "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
        "    \"\"\"\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        " \n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_file,\n",
        "            vocab_txt,\n",
        "            do_lower_case=False,\n",
        "            remove_space=True,\n",
        "            keep_accents=False,\n",
        "            unk_token=\"[UNK]\",\n",
        "            sep_token=\"[SEP]\",\n",
        "            pad_token=\"[PAD]\",\n",
        "            cls_token=\"[CLS]\",\n",
        "            mask_token=\"[MASK]\",\n",
        "            **kwargs):\n",
        "        super().__init__(\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs\n",
        "        )\n",
        " \n",
        "        # Build vocab\n",
        "        self.token2idx = dict()\n",
        "        self.idx2token = []\n",
        "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
        "            for idx, token in enumerate(f):\n",
        "                token = token.strip()\n",
        "                self.token2idx[token] = idx\n",
        "                self.idx2token.append(token)\n",
        " \n",
        "        #self.max_len_single_sentence = self.max_len - 2  # take into account special tokens\n",
        "        #self.max_len_sentences_pair = self.max_len - 3  # take into account special tokens\n",
        " \n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        " \n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.remove_space = remove_space\n",
        "        self.keep_accents = keep_accents\n",
        "        self.vocab_file = vocab_file\n",
        "        self.vocab_txt = vocab_txt\n",
        " \n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(vocab_file)\n",
        " \n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.idx2token)\n",
        " \n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        state[\"sp_model\"] = None\n",
        "        return state\n",
        " \n",
        "    def __setstate__(self, d):\n",
        "        self.__dict__ = d\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(self.vocab_file)\n",
        " \n",
        "    def preprocess_text(self, inputs):\n",
        "        if self.remove_space:\n",
        "            outputs = \" \".join(inputs.strip().split())\n",
        "        else:\n",
        "            outputs = inputs\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
        " \n",
        "        if not self.keep_accents:\n",
        "            outputs = unicodedata.normalize('NFKD', outputs)\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
        "        if self.do_lower_case:\n",
        "            outputs = outputs.lower()\n",
        " \n",
        "        return outputs\n",
        " \n",
        "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
        "        \"\"\" Tokenize a string. \"\"\"\n",
        "        text = self.preprocess_text(text)\n",
        " \n",
        "        if not sample:\n",
        "            pieces = self.sp_model.EncodeAsPieces(text)\n",
        "        else:\n",
        "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
        "        new_pieces = []\n",
        "        for piece in pieces:\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "                    if len(cur_pieces[0]) == 1:\n",
        "                        cur_pieces = cur_pieces[1:]\n",
        "                    else:\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\n",
        "                cur_pieces.append(piece[-1])\n",
        "                new_pieces.extend(cur_pieces)\n",
        "            else:\n",
        "                new_pieces.append(piece)\n",
        " \n",
        "        return new_pieces\n",
        " \n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
        " \n",
        "    def _convert_id_to_token(self, index, return_unicode=True):\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
        "        return self.idx2token[index]\n",
        " \n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
        "        return out_string\n",
        " \n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A RoBERTa sequence has the following format:\n",
        "            single sequence: [CLS] X [SEP]\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        " \n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
        "        Args:\n",
        "            token_ids_0: list of ids (must not contain special tokens)\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
        "                for sequence pairs\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
        "                special tokens for the model\n",
        "        Returns:\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
        "        \"\"\"\n",
        " \n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
        " \n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        " \n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A BERT sequence pair mask has the following format:\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
        "        | first sequence    | second sequence\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
        " \n",
        "    def save_vocabulary(self, save_directory):\n",
        "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
        "            to a directory.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
        "            return\n",
        " \n",
        "        # 1. Save sentencepiece model\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
        " \n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
        "            copyfile(self.vocab_file, out_vocab_model)\n",
        " \n",
        "        # 2. Save vocab.txt\n",
        "        index = 0\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(token + \"\\n\")\n",
        "                index += 1\n",
        " \n",
        "        return out_vocab_model, out_vocab_txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac556bf5",
      "metadata": {
        "code_folding": [
          0,
          17,
          69
        ],
        "hidden": true,
        "id": "ac556bf5"
      },
      "outputs": [],
      "source": [
        "def bert_tokenizer(sent, MAX_LEN, tokenizer):\n",
        "    \n",
        "    encoded_dict=tokenizer.encode_plus(\n",
        "    text = sent, \n",
        "    add_special_tokens=True, \n",
        "    max_length=MAX_LEN, \n",
        "    pad_to_max_length=True, \n",
        "    return_attention_mask=True,\n",
        "    truncation = True)\n",
        "    \n",
        "    input_id=encoded_dict['input_ids']\n",
        "    attention_mask=encoded_dict['attention_mask']\n",
        "    #token_type_id = encoded_dict['token_type_ids']\n",
        "    token_type_id = 0\n",
        "    \n",
        "    return input_id, attention_mask, token_type_id\n",
        "\n",
        "def preprocessing_train():\n",
        "    \n",
        "    pt = args.pt#'monologg/kobert'\n",
        "    \n",
        "    if 'kobert' in pt:\n",
        "        tokenizer = KoBertTokenizer.from_pretrained(pt,  cache_dir='bert_ckpt', do_lower_case=False)\n",
        "        print('load kobert')\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.pt)\n",
        "    \n",
        "    MAX_LEN = args.max_len\n",
        "    train = pd.read_csv('./train_data.csv')\n",
        "    train=train[['title','topic_idx']]\n",
        "\n",
        "    input_ids =[]\n",
        "    attention_masks =[]\n",
        "    token_type_ids =[]\n",
        "    train_data_labels = []\n",
        "\n",
        "    for train_sent, train_label in tqdm.tqdm(zip(train['title'], train['topic_idx'])):\n",
        "        try:\n",
        "            input_id, attention_mask,_ = bert_tokenizer(train_sent, MAX_LEN=MAX_LEN, tokenizer=tokenizer)\n",
        "\n",
        "            input_ids.append(input_id)\n",
        "            attention_masks.append(attention_mask)\n",
        "            token_type_ids.append(0)\n",
        "            #########################################\n",
        "            train_data_labels.append(train_label)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            pass\n",
        "\n",
        "    train_input_ids=np.array(input_ids, dtype=int)\n",
        "    train_attention_masks=np.array(attention_masks, dtype=int)\n",
        "    train_token_type_ids=np.array(token_type_ids, dtype=int)\n",
        "    ###########################################################\n",
        "    train_inputs=(train_input_ids, train_attention_masks, train_token_type_ids)\n",
        "    train_labels=np.asarray(train_data_labels, dtype=np.int32)\n",
        "\n",
        "    # save\n",
        "    train_data = {}\n",
        "\n",
        "    train_data['input_ids'] = train_input_ids\n",
        "    train_data['attention_mask'] = train_attention_masks\n",
        "    train_data['token_type_ids'] = train_token_type_ids\n",
        "    train_data['targets'] = np.asarray(train_data_labels, dtype=np.int32)\n",
        "    \n",
        "    os.makedirs(f'./data/{pt}/', exist_ok=True)\n",
        "    with open(f'./data/{pt}/train_data_{MAX_LEN}.pickle', 'wb') as f:\n",
        "        pickle.dump(train_data, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def preprocessing_test():\n",
        "    \n",
        "    pt = args.pt\n",
        "    if 'kobert' in pt:\n",
        "        tokenizer = KoBertTokenizer.from_pretrained(pt,  cache_dir='bert_ckpt', do_lower_case=False)\n",
        "        print('load kobert')\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.pt)\n",
        "    MAX_LEN = args.max_len\n",
        "    \n",
        "    test = pd.read_csv('./test_data.csv')\n",
        "    test=test[['title']]\n",
        "    \n",
        "    input_ids =[]\n",
        "    attention_masks =[]\n",
        "    token_type_ids =[]\n",
        "\n",
        "    for test_sent in tqdm.tqdm(test['title']):\n",
        "        try:\n",
        "            input_id, attention_mask,_ = bert_tokenizer(test_sent, MAX_LEN=MAX_LEN, tokenizer=tokenizer)\n",
        "\n",
        "            input_ids.append(input_id)\n",
        "            attention_masks.append(attention_mask)\n",
        "            token_type_ids.append(0)\n",
        "            #########################################\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            pass\n",
        "\n",
        "    test_input_ids=np.array(input_ids, dtype=int)\n",
        "    test_attention_masks=np.array(attention_masks, dtype=int)\n",
        "    test_token_type_ids=np.array(token_type_ids, dtype=int)\n",
        "    ###########################################################\n",
        "    test_inputs=(test_input_ids, test_attention_masks, test_token_type_ids)\n",
        "\n",
        "\n",
        "    # save\n",
        "    test_data = {}\n",
        "\n",
        "    test_data['input_ids'] = test_input_ids\n",
        "    test_data['attention_mask'] = test_attention_masks\n",
        "    test_data['token_type_ids'] = test_token_type_ids\n",
        "    \n",
        "    os.makedirs(f'./data/{pt}/', exist_ok=True)\n",
        "    with open(f'./data/{pt}/test_data_{MAX_LEN}.pickle', 'wb') as f:\n",
        "        pickle.dump(test_data, f, pickle.HIGHEST_PROTOCOL)\n",
        "           "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7db76a",
      "metadata": {
        "hidden": true,
        "id": "fd7db76a"
      },
      "outputs": [],
      "source": [
        "for pt, max_len in zip(['monologg/kobert','klue/roberta-base','klue/roberta-small','klue/roberta-large','xlm-roberta-large', \n",
        "           'bert-base-multilingual-uncased', 'klue/roberta-large'],[33,33,33,33,33,33,28]):\n",
        "    args.max_len = max_len\n",
        "    args.pt = pt\n",
        "    preprocessing_train()\n",
        "    preprocessing_test()\n",
        "        \n",
        "    print(f'{args.pt} 모델 전처리 완료')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbff676f",
      "metadata": {
        "heading_collapsed": true,
        "id": "fbff676f"
      },
      "source": [
        "# models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de6900c5",
      "metadata": {
        "code_folding": [
          3
        ],
        "hidden": true,
        "id": "de6900c5"
      },
      "outputs": [],
      "source": [
        "# ------------------------\n",
        "#  dataset\n",
        "# ------------------------\n",
        "class KobertDataSet(Dataset):\n",
        "    \n",
        "    def __init__(self, data, test=False):\n",
        "        \n",
        "        self.data = data\n",
        "        self.test = test\n",
        "        \n",
        "    def __len__(self):\n",
        "        \n",
        "        return self.data['input_ids'].shape[0]\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        \n",
        "        ids = torch.tensor(self.data['input_ids'][idx], dtype=torch.long)\n",
        "        mask = torch.tensor(self.data['attention_mask'][idx], dtype=torch.long)\n",
        "        token_type_ids = torch.tensor(self.data['token_type_ids'][idx], dtype=torch.long)\n",
        "         \n",
        "            \n",
        "        if self.test:\n",
        "            return {\n",
        "                'ids': ids,\n",
        "                'mask': mask,\n",
        "                'token_type_ids': token_type_ids\n",
        "            }\n",
        "        \n",
        "        else:\n",
        "            target = torch.tensor(self.data['targets'][idx],dtype=torch.long)\n",
        "\n",
        "            return {\n",
        "                    'ids': ids,\n",
        "                    'mask': mask,\n",
        "                    'token_type_ids': token_type_ids,\n",
        "                    'targets': target\n",
        "                }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2fd0f29",
      "metadata": {
        "id": "f2fd0f29"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89dee00d",
      "metadata": {
        "code_folding": [
          4,
          46,
          71,
          223
        ],
        "id": "89dee00d"
      },
      "outputs": [],
      "source": [
        "# ------------------------\n",
        "#  scheduler\n",
        "# ------------------------\n",
        "\n",
        "def do_valid(net, valid_loader):\n",
        "\n",
        "    val_loss = 0\n",
        "    target_lst = []\n",
        "    pred_lst = []\n",
        "    logit = []\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    net.eval()\n",
        "    start_timer = timer()\n",
        "    for t, data in enumerate(tqdm.tqdm(valid_loader)):\n",
        "        ids  = data['ids'].to(device)\n",
        "        mask  = data['mask'].to(device)\n",
        "        tokentype = data['token_type_ids'].to(device)\n",
        "        target = data['targets'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if args.amp:\n",
        "                with amp.autocast():\n",
        "                    # output\n",
        "                    output = net(ids, mask)\n",
        "                    output = output[0]\n",
        "\n",
        "                    # loss\n",
        "                    loss = loss_fn(output, target)\n",
        "\n",
        "            else:\n",
        "                output = net(ids, mask)#.squeeze(0)\n",
        "                loss = loss_fn(output, target)\n",
        "            \n",
        "            val_loss += loss\n",
        "            target_lst.extend(target.detach().cpu().numpy())\n",
        "            pred_lst.extend(output.argmax(dim=1).tolist())\n",
        "            logit.extend(output.tolist())\n",
        "            \n",
        "        val_mean_loss = val_loss / len(valid_loader)\n",
        "        validation_score = f1_score(y_true=target_lst, y_pred=pred_lst, average='macro')\n",
        "        validation_acc = accuracy_score(y_true=target_lst, y_pred=pred_lst)\n",
        "        \n",
        "\n",
        "    return val_mean_loss, validation_score, validation_acc, logit\n",
        "\n",
        "def do_predict(net, valid_loader):\n",
        "    \n",
        "    val_loss = 0\n",
        "    pred_lst = []\n",
        "    logit=[]\n",
        "    net.eval()\n",
        "    for t, data in enumerate(tqdm.tqdm(valid_loader)):\n",
        "        ids  = data['ids'].to(device)\n",
        "        mask  = data['mask'].to(device)\n",
        "        tokentype = data['token_type_ids'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if args.amp:\n",
        "                with amp.autocast():\n",
        "                    # output\n",
        "                    output = net(ids, mask)[0]\n",
        "\n",
        "            else:\n",
        "                output = net(ids, mask)\n",
        "             \n",
        "            pred_lst.extend(output.argmax(dim=1).tolist())\n",
        "            logit.extend(output.tolist())\n",
        "            \n",
        "    return pred_lst,logit\n",
        "\n",
        "def run_train(folds=3):\n",
        "    out_dir = args.dir_+ f'/fold{args.fold}/{args.exp_name}/'\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    \n",
        "    # load dataset\n",
        "    train, test = load_data()    \n",
        "    with open(f'./data/{args.pt}/train_data_{args.max_len}.pickle', 'rb') as f:\n",
        "        train_data = pickle.load(f)\n",
        "    with open(f'./data/{args.pt}/test_data_{args.max_len}.pickle', 'rb') as f:\n",
        "        test_data = pickle.load(f)    \n",
        "    \n",
        "    # split fold\n",
        "    for n_fold in range(5):\n",
        "        if n_fold != folds:\n",
        "            print(f'{n_fold} fold pass'+'\\n')\n",
        "            continue\n",
        "            \n",
        "        if args.debug:\n",
        "            train = train.sample(1000).copy()\n",
        "        \n",
        "        trn_idx = train[train['fold']!=n_fold]['id'].values\n",
        "        val_idx = train[train['fold']==n_fold]['id'].values\n",
        "    \n",
        "\n",
        "        train_dict = {'input_ids' : train_data['input_ids'][trn_idx] , 'attention_mask' : train_data['attention_mask'][trn_idx] , \n",
        "                      'token_type_ids' : train_data['token_type_ids'][trn_idx], 'targets' : train_data['targets'][trn_idx]}\n",
        "        val_dict = {'input_ids' : train_data['input_ids'][val_idx] , 'attention_mask' : train_data['attention_mask'][val_idx] , \n",
        "                      'token_type_ids' : train_data['token_type_ids'][val_idx], 'targets' : train_data['targets'][val_idx]}\n",
        "\n",
        "        ## dataset ------------------------------------\n",
        "        train_dataset = KobertDataSet(data = train_dict)\n",
        "        valid_dataset = KobertDataSet(data = val_dict)\n",
        "        trainloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size,\n",
        "                                 num_workers=8, shuffle=True, pin_memory=True)\n",
        "        validloader = DataLoader(dataset=valid_dataset, batch_size=args.batch_size, \n",
        "                                 num_workers=8, shuffle=False, pin_memory=True)\n",
        "\n",
        "        ## net ----------------------------------------\n",
        "        scaler = amp.GradScaler()\n",
        "        if 'xlm-roberta' in args.pt:\n",
        "            net = XLMRobertaForSequenceClassification.from_pretrained(args.pt, num_labels = 7) \n",
        "        \n",
        "        elif 'klue/roberta' in args.pt:\n",
        "            net = RobertaForSequenceClassification.from_pretrained(args.pt, num_labels = 7) \n",
        "        else:\n",
        "            net = BertForSequenceClassification.from_pretrained(args.pt, num_labels = 7) \n",
        "\n",
        "        net.to(device)\n",
        "        if len(args.gpu)>1:\n",
        "            net = nn.DataParallel(net)\n",
        "\n",
        "        # ------------------------\n",
        "        # loss\n",
        "        # ------------------------\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "        # ------------------------\n",
        "        #  Optimizer\n",
        "        # ------------------------\n",
        "        optimizer = optim.Lookahead(optim.RAdam(filter(lambda p: p.requires_grad,net.parameters()), lr=args.start_lr), alpha=0.5, k=5)\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = len(trainloader)*args.epochs)\n",
        "        \n",
        "        \n",
        "        # ----\n",
        "        start_timer = timer()\n",
        "        best_score = 0\n",
        "\n",
        "        for epoch in range(1, args.epochs+1):\n",
        "            train_loss = 0\n",
        "            valid_loss = 0\n",
        "\n",
        "            target_lst = []\n",
        "            pred_lst = []\n",
        "            lr = get_learning_rate(optimizer)\n",
        "            print(f'-------------------')\n",
        "            print(f'{epoch}epoch start')\n",
        "            print(f'-------------------'+'\\n')\n",
        "            print(f'learning rate : {lr : .6f}')\n",
        "            for t, data in enumerate(tqdm.tqdm(trainloader)):\n",
        "\n",
        "                # one iteration update  -------------\n",
        "                ids  = data['ids'].to(device)\n",
        "                mask  = data['mask'].to(device)\n",
        "                tokentype = data['token_type_ids'].to(device)\n",
        "                target = data['targets'].to(device)\n",
        "\n",
        "                # ------------\n",
        "                net.train()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "\n",
        "                if args.amp:\n",
        "                    with amp.autocast():\n",
        "                        # output\n",
        "                        output = net(ids, mask)\n",
        "                        output = output[0]\n",
        "\n",
        "                        # loss\n",
        "                        loss = loss_fn(output, target)\n",
        "                        train_loss += loss\n",
        "\n",
        "\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "\n",
        "                else:\n",
        "                    # output\n",
        "                    output = net(ids, mask)\n",
        "\n",
        "                    # loss\n",
        "                    loss = loss_fn(output, target)\n",
        "                    train_loss += loss\n",
        "\n",
        "                    # update\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "\n",
        "                # for calculate f1 score\n",
        "                target_lst.extend(target.detach().cpu().numpy())\n",
        "                pred_lst.extend(output.argmax(dim=1).tolist())\n",
        "\n",
        "\n",
        "                if scheduler is not None:\n",
        "                    scheduler.step() \n",
        "            train_loss = train_loss / len(trainloader)\n",
        "            train_score = f1_score(y_true=target_lst, y_pred=pred_lst, average='macro')\n",
        "            train_acc = accuracy_score(y_true=target_lst, y_pred=pred_lst)\n",
        "\n",
        "            # validation\n",
        "            valid_loss, valid_score, valid_acc, _ = do_valid(net, validloader)\n",
        "\n",
        "\n",
        "            if valid_acc > best_score:\n",
        "                best_score = valid_acc\n",
        "                best_epoch = epoch\n",
        "                best_loss = valid_loss\n",
        "\n",
        "                torch.save(net.state_dict(), out_dir + f'/{folds}f_{epoch}e_{best_score:.4f}_s.pth')\n",
        "                print('best model saved'+'\\n')\n",
        "\n",
        "\n",
        "            print(f'train loss : {train_loss:.4f}, train f1 score : {train_score : .4f}, train acc : {train_acc : .4f}'+'\\n')\n",
        "            print(f'valid loss : {valid_loss:.4f}, valid f1 score : {valid_score : .4f}, valid acc : {valid_acc : .4f}'+'\\n')\n",
        "\n",
        "\n",
        "        print(f'best valid loss : {best_loss : .4f}'+'\\n')\n",
        "        print(f'best epoch : {best_epoch }'+'\\n')\n",
        "        print(f'best accuracy : {best_score : .4f}'+'\\n')\n",
        "        \n",
        "def run_predict(model_path):\n",
        "    ## dataset ------------------------------------\n",
        "    # load\n",
        "    with open(f'./data/{args.pt}/test_data_{args.max_len}.pickle', 'rb') as f:\n",
        "        test_dict = pickle.load(f)\n",
        "        \n",
        "    print('test load')\n",
        "    test_dataset = KobertDataSet(data = test_dict, test=True)\n",
        "    testloader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, \n",
        "                             num_workers=8, shuffle=False, pin_memory=True)\n",
        "    print('set testloader')\n",
        "    ## net ----------------------------------------\n",
        "    scaler = amp.GradScaler()\n",
        "    if 'xlm-roberta' in args.pt:\n",
        "        net = XLMRobertaForSequenceClassification.from_pretrained(args.pt, num_labels = 7) \n",
        "        \n",
        "    elif 'klue/roberta' in args.pt:\n",
        "        net = RobertaForSequenceClassification.from_pretrained(args.pt, num_labels = 7) \n",
        "    else:\n",
        "        net = BertForSequenceClassification.from_pretrained(args.pt, num_labels = 7) \n",
        "        \n",
        "    net.to(device)\n",
        "    \n",
        "    if len(args.gpu)>1:\n",
        "        net = nn.DataParallel(net)\n",
        "\n",
        "    f = torch.load(model_path)\n",
        "    net.load_state_dict(f, strict=True)  # True\n",
        "    print('load saved models')\n",
        "    # ------------------------\n",
        "    # validation\n",
        "    preds, logit = do_predict(net, testloader) #outputs\n",
        "           \n",
        "    print('complete predict')\n",
        "    \n",
        "    return preds, np.array(logit)\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa4fb075",
      "metadata": {
        "id": "aa4fb075"
      },
      "outputs": [],
      "source": [
        "\"\"\"5fold 전용\"\"\"\n",
        "if __name__ == '__main__':\n",
        "    for pt, max_len in zip(['monologg/kobert','klue/roberta-base','klue/roberta-small','klue/roberta-large','xlm-roberta-large', \n",
        "           'bert-base-multilingual-uncased', 'klue/roberta-large'],[33,33,33,33,33,33,28]):\n",
        "        \n",
        "        args.max_len = max_len\n",
        "        args.pt = pt\n",
        "        args.exp_name = str(args.pt) + '_' + str(args.max_len)\n",
        "        \n",
        "        for i in [0,1,2,3,4]: # 5fold\n",
        "            run_train(folds=i)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63f1e8a2",
      "metadata": {
        "id": "63f1e8a2"
      },
      "source": [
        "# ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa34f47",
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "efa34f47"
      },
      "outputs": [],
      "source": [
        "def ensemble():\n",
        "    final_logit=0\n",
        "    args.max_len=33\n",
        "    args.pt = 'monologg/kobert'\n",
        "    _, logit1 = run_predict(\"./saved_models/fold3/kobert/0f_9e_0.8895_s.pth\")\n",
        "    _, logit2 = run_predict(\"./saved_models/fold3/kobert/1f_10e_0.8823_s.pth\")\n",
        "    _, logit3 = run_predict(\"./saved_models/fold3/kobert/2f_8e_0.8888_s.pth\")\n",
        "    _, logit4 = run_predict(\"./saved_models/fold3/kobert/3f_10e_0.8897_s.pth\")\n",
        "    _, logit5 = run_predict(\"./saved_models/fold3/kobert/4f_8e_0.8867_s.pth\")\n",
        "    final_logit += (logit1+logit2+logit3+logit4+logit5)/5\n",
        "    \n",
        "    #####################\n",
        "\n",
        "    args.pt = 'klue/roberta-base'\n",
        "    _, logit1 = run_predict(\"./saved_models/fold3/roberta-base/0f_5e_0.8920_s.pth\")\n",
        "    _, logit2 = run_predict(\"./saved_models/fold3/roberta-base/1f_4e_0.8879_s.pth\")\n",
        "    _, logit3 = run_predict(\"./saved_models/fold3/roberta-base/2f_5e_0.8889_s.pth\")\n",
        "    _, logit4 = run_predict(\"./saved_models/fold3/roberta-base/3f_4e_0.8951_s.pth\")\n",
        "    _, logit5 = run_predict(\"./saved_models/fold3/roberta-base/4f_4e_0.8887_s.pth\")\n",
        "\n",
        "    final_logit += (logit1+logit2+logit3+logit4+logit5)/5\n",
        "\n",
        "    #####################\n",
        "    args.pt = 'klue/roberta-small'\n",
        "    preds1, logit1 = run_predict(\"./saved_models/fold3/roberta-small/0f_8e_0.8900_s.pth\")\n",
        "    preds2, logit2 = run_predict(\"./saved_models/fold3/roberta-small/1f_9e_0.8813_s.pth\")\n",
        "    preds3, logit3 = run_predict(\"./saved_models/fold3/roberta-small/2f_7e_0.8884_s.pth\")\n",
        "    preds4, logit4 = run_predict(\"./saved_models/fold3/roberta-small/3f_3e_0.8958_s.pth\")\n",
        "    preds5, logit5 = run_predict(\"./saved_models/fold3/roberta-small/4f_4e_0.8881_s.pth\") # 8884 가능\n",
        "    final_logit += (logit1+logit2+logit3+logit4+logit5)/5\n",
        "    #####################\n",
        "\n",
        "    args.pt = 'bert-base-multilingual-uncased'\n",
        "    preds1, logit1 = run_predict(\"./saved_models/fold3/bert-base-multilingual-uncased/0f_5e_0.8624_s.pth\")\n",
        "    preds2, logit2 = run_predict(\"./saved_models/fold3/bert-base-multilingual-uncased/1f_8e_0.8573_s.pth\")\n",
        "    preds3, logit3 = run_predict(\"./saved_models/fold3/bert-base-multilingual-uncased/2f_9e_0.8674_s.pth\")\n",
        "    preds4, logit4 = run_predict(\"./saved_models/fold3/bert-base-multilingual-uncased/3f_8e_0.8649_s.pth\")\n",
        "    preds5, logit5 = run_predict(\"./saved_models/fold3/bert-base-multilingual-uncased/4f_9e_0.8673_s.pth\")\n",
        "    final_logit += (logit1+logit2+logit3+logit4+logit5)/5\n",
        "    #####################\n",
        "    args.pt = 'klue/roberta-large'\n",
        "    preds1, logit1 = run_predict(\"./saved_models/fold3/klue-roberta-large/0f_2e_0.8905_s.pth\")\n",
        "    preds2, logit2 = run_predict(\"./saved_models/fold3/klue-roberta-large/1f_4e_0.8897_s.pth\")\n",
        "    preds3, logit3 = run_predict(\"./saved_models/fold3/klue-roberta-large/2f_3e_0.8887_s.pth\")\n",
        "    preds4, logit4 = run_predict(\"./saved_models/fold3/klue-roberta-large/3f_3e_0.8949_s.pth\")\n",
        "    preds5, logit5 = run_predict(\"./saved_models/fold3/klue-roberta-large/4f_2e_0.8939_s.pth\")\n",
        "    final_logit += (logit1+logit2+logit3+logit4+logit5)/5\n",
        "    #####################\n",
        "    args.pt = 'xlm-roberta-large'\n",
        "    preds1, logit1 = run_predict(\"./saved_models/fold3/xlm-roberta-large_radam/0f_6e_0.8928_s.pth\")\n",
        "    preds2, logit2 = run_predict(\"./saved_models/fold3/xlm-roberta-large_radam/1f_5e_0.8850_s.pth\")\n",
        "    preds3, logit3 = run_predict(\"./saved_models/fold3/xlm-roberta-large_radam/2f_5e_0.8891_s.pth\")\n",
        "    preds4, logit4 = run_predict(\"./saved_models/fold3/xlm-roberta-large_radam/3f_8e_0.8938_s.pth\")\n",
        "    preds5, logit5 = run_predict(\"./saved_models/fold3/xlm-roberta-large_radam/4f_6e_0.8911_s.pth\")\n",
        "    final_logit += (logit1+logit2+logit3+logit4+logit5)/5\n",
        "    #####################\n",
        "    args.max_len=28\n",
        "    args.pt = 'klue/roberta-large'\n",
        "    preds1, logit1 = run_predict(\"./saved_models/fold3/klue-roberta-large_28/0f_6e_0.8912_s.pth\")\n",
        "    preds2, logit2 = run_predict(\"./saved_models/fold3//klue-roberta-large_28/1f_3e_0.8891_s.pth\")\n",
        "    preds3, logit3 = run_predict(\"./saved_models/fold3//klue-roberta-large_28/2f_5e_0.8891_s.pth\")\n",
        "    preds4, logit4 = run_predict(\"./saved_models/fold3//klue-roberta-large_28/3f_4e_0.8961_s.pth\")\n",
        "    preds5, logit5 = run_predict(\"./saved_models/fold3//klue-roberta-large_28/4f_2e_0.8938_s.pth\")\n",
        "    final_logit += (logit1+logit2+logit3+logit4+logit5)/5\n",
        "    \n",
        "    return final_logit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bb613ee",
      "metadata": {
        "scrolled": true,
        "id": "8bb613ee"
      },
      "outputs": [],
      "source": [
        "final_logit = ensemble()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b85117",
      "metadata": {
        "id": "80b85117"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cba9d8a",
      "metadata": {
        "heading_collapsed": true,
        "id": "3cba9d8a"
      },
      "source": [
        "# submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6b633c",
      "metadata": {
        "hidden": true,
        "id": "9a6b633c"
      },
      "outputs": [],
      "source": [
        "sub = pd.read_csv(\"./sample_submission.csv\")\n",
        "sub['topic_idx'] = final_logit.argmax(1)\n",
        "# preds\n",
        "sub.to_csv('./submission/final_submission.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01f833d1",
      "metadata": {
        "hidden": true,
        "id": "01f833d1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Reference_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}